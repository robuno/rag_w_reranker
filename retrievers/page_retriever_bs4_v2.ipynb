{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2qso1KqinWR"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GANGIHsnlxRX"
      },
      "outputs": [],
      "source": [
        "ROOT_SRC = \"https://denizyuret.github.io/Knet.jl/stable/\"\n",
        "\n",
        "response = requests.get(ROOT_SRC)\n",
        "bsoup = BeautifulSoup(response.text, \"html.parser\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12SAd5G6l84W",
        "outputId": "ef942eec-1740-4536-e66a-28aef66bc895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "#Manual\n",
            "#Textbook\n",
            "install/\n",
            "tutorial/\n",
            "reference/\n",
            "backprop/\n",
            "softmax/\n",
            "mlp/\n",
            "cnn/\n",
            "rnn/\n",
            "rl/\n",
            "opt/\n",
            "gen/\n",
            "nce/\n",
            "\n",
            "\n",
            "https://github.com/denizyuret/Knet.jl/blob/master/docs/src/index.md\n",
            "#\n",
            "#\n",
            "#Welcome-to-Knet.jl's-documentation!\n",
            "None\n",
            "#Welcome-to-Knet.jl's-documentation!\n",
            "#Manual\n",
            "None\n",
            "#Manual\n",
            "install/#Setting-up-Knet\n",
            "install/#Installation\n",
            "install/#Tips-for-developers\n",
            "install/#Using-Amazon-AWS\n",
            "install/#Using-Microsoft-Azure\n",
            "install/#Using-Ubuntu18.04\n",
            "tutorial/#Introduction-to-Knet\n",
            "tutorial/#Summary\n",
            "tutorial/#Philosophy\n",
            "tutorial/#Tutorial\n",
            "tutorial/#Benchmarks\n",
            "tutorial/#Under-the-hood\n",
            "reference/#Reference\n",
            "reference/#AutoGrad\n",
            "reference/#KnetArray\n",
            "reference/#File-I/O\n",
            "reference/#Parameter-initialization\n",
            "reference/#Activation-functions\n",
            "reference/#Loss-functions\n",
            "reference/#Convolution-and-Pooling\n",
            "reference/#Recurrent-neural-networks\n",
            "reference/#Batch-Normalization\n",
            "reference/#Model-optimization\n",
            "reference/#Hyperparameter-optimization\n",
            "reference/#Utilities\n",
            "reference/#AutoGrad-(advanced)\n",
            "reference/#Per-parameter-optimization-(advanced)\n",
            "reference/#Function-Index\n",
            "#Textbook\n",
            "None\n",
            "#Textbook\n",
            "backprop/#Backpropagation-and-SGD\n",
            "softmax/#Softmax-Classification\n",
            "mlp/#Multilayer-Perceptrons\n",
            "cnn/#Convolutional-Neural-Networks\n",
            "rnn/#Recurrent-Neural-Networks\n",
            "rl/#Reinforcement-Learning\n",
            "opt/#Optimization\n",
            "gen/#Generalization\n",
            "install/\n",
            "https://github.com/JuliaDocs/Documenter.jl\n",
            "https://julialang.org/\n",
            "https://github.com/JuliaDocs/Documenter.jl\n"
          ]
        }
      ],
      "source": [
        "all_links = bsoup.findAll(\"a\")\n",
        "\n",
        "links_v1 = []\n",
        "\n",
        "for link in all_links:\n",
        "  links_v1.append(link.get(\"href\"))\n",
        "  print(link.get(\"href\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qyg06XeroBu5",
        "outputId": "332db4c3-6146-4c42-e4af-cba6df8a5063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['#Manual', '#Textbook', 'install/', 'tutorial/', 'reference/', 'backprop/', 'softmax/', 'mlp/', 'cnn/', 'rnn/', 'rl/', 'opt/', 'gen/', 'nce/', 'https://github.com/denizyuret/Knet.jl/blob/master/docs/src/index.md', '#', '#', \"#Welcome-to-Knet.jl's-documentation!\", \"#Welcome-to-Knet.jl's-documentation!\", '#Manual', '#Manual', 'install/#Setting-up-Knet', 'install/#Installation', 'install/#Tips-for-developers', 'install/#Using-Amazon-AWS', 'install/#Using-Microsoft-Azure', 'install/#Using-Ubuntu18.04', 'tutorial/#Introduction-to-Knet', 'tutorial/#Summary', 'tutorial/#Philosophy', 'tutorial/#Tutorial', 'tutorial/#Benchmarks', 'tutorial/#Under-the-hood', 'reference/#Reference', 'reference/#AutoGrad', 'reference/#KnetArray', 'reference/#File-I/O', 'reference/#Parameter-initialization', 'reference/#Activation-functions', 'reference/#Loss-functions', 'reference/#Convolution-and-Pooling', 'reference/#Recurrent-neural-networks', 'reference/#Batch-Normalization', 'reference/#Model-optimization', 'reference/#Hyperparameter-optimization', 'reference/#Utilities', 'reference/#AutoGrad-(advanced)', 'reference/#Per-parameter-optimization-(advanced)', 'reference/#Function-Index', '#Textbook', '#Textbook', 'backprop/#Backpropagation-and-SGD', 'softmax/#Softmax-Classification', 'mlp/#Multilayer-Perceptrons', 'cnn/#Convolutional-Neural-Networks', 'rnn/#Recurrent-Neural-Networks', 'rl/#Reinforcement-Learning', 'opt/#Optimization', 'gen/#Generalization', 'install/', 'https://github.com/JuliaDocs/Documenter.jl', 'https://julialang.org/', 'https://github.com/JuliaDocs/Documenter.jl']\n",
            "['install/#Setting-up-Knet', 'install/#Installation', 'install/#Tips-for-developers', 'install/#Using-Amazon-AWS', 'install/#Using-Microsoft-Azure', 'install/#Using-Ubuntu18.04', 'tutorial/#Introduction-to-Knet', 'tutorial/#Summary', 'tutorial/#Philosophy', 'tutorial/#Tutorial', 'tutorial/#Benchmarks', 'tutorial/#Under-the-hood', 'reference/#Reference', 'reference/#AutoGrad', 'reference/#KnetArray', 'reference/#File-I/O', 'reference/#Parameter-initialization', 'reference/#Activation-functions', 'reference/#Loss-functions', 'reference/#Convolution-and-Pooling', 'reference/#Recurrent-neural-networks', 'reference/#Batch-Normalization', 'reference/#Model-optimization', 'reference/#Hyperparameter-optimization', 'reference/#Utilities', 'reference/#AutoGrad-(advanced)', 'reference/#Per-parameter-optimization-(advanced)', 'reference/#Function-Index', 'backprop/#Backpropagation-and-SGD', 'softmax/#Softmax-Classification', 'mlp/#Multilayer-Perceptrons', 'cnn/#Convolutional-Neural-Networks', 'rnn/#Recurrent-Neural-Networks', 'rl/#Reinforcement-Learning', 'opt/#Optimization', 'gen/#Generalization']\n"
          ]
        }
      ],
      "source": [
        "# remove empty strings in list\n",
        "links_v2 = [lin for lin in links_v1 if lin]\n",
        "print(links_v2)\n",
        "\n",
        "links_v3 = [i for i in links_v2 if ('#' in i and '/' in i)]\n",
        "print(links_v3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjYdAo5uOg-_",
        "outputId": "2425c4d9-1037-4a93-e4d9-6ffd81744095"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['install/#Setting-up-Knet',\n",
              " 'install/#Installation',\n",
              " 'install/#Tips-for-developers',\n",
              " 'install/#Using-Amazon-AWS',\n",
              " 'install/#Using-Microsoft-Azure',\n",
              " 'install/#Using-Ubuntu18.04',\n",
              " 'tutorial/#Introduction-to-Knet',\n",
              " 'tutorial/#Summary',\n",
              " 'tutorial/#Philosophy',\n",
              " 'tutorial/#Tutorial',\n",
              " 'tutorial/#Benchmarks',\n",
              " 'tutorial/#Under-the-hood',\n",
              " 'reference/#Reference',\n",
              " 'reference/#AutoGrad',\n",
              " 'reference/#KnetArray',\n",
              " 'reference/#File-I/O',\n",
              " 'reference/#Parameter-initialization',\n",
              " 'reference/#Activation-functions',\n",
              " 'reference/#Loss-functions',\n",
              " 'reference/#Convolution-and-Pooling',\n",
              " 'reference/#Recurrent-neural-networks',\n",
              " 'reference/#Batch-Normalization',\n",
              " 'reference/#Model-optimization',\n",
              " 'reference/#Hyperparameter-optimization',\n",
              " 'reference/#Utilities',\n",
              " 'reference/#AutoGrad-(advanced)',\n",
              " 'reference/#Per-parameter-optimization-(advanced)',\n",
              " 'reference/#Function-Index',\n",
              " 'backprop/#Backpropagation-and-SGD',\n",
              " 'softmax/#Softmax-Classification',\n",
              " 'mlp/#Multilayer-Perceptrons',\n",
              " 'cnn/#Convolutional-Neural-Networks',\n",
              " 'rnn/#Recurrent-Neural-Networks',\n",
              " 'rl/#Reinforcement-Learning',\n",
              " 'opt/#Optimization',\n",
              " 'gen/#Generalization']"
            ]
          },
          "execution_count": 259,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "links_v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Y93T8LK3PR1X",
        "outputId": "6c98a318-ed7b-48c5-f0db-f916f4791af1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'reference/#File-I/O'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "links_v3[15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAH8U7v2Sdqz"
      },
      "outputs": [],
      "source": [
        "# Function to retrieve all sub <li> elements within <ul> recursively\n",
        "def get_sub_list_items2(ul_element):\n",
        "    sub_list_items = []\n",
        "    for li in ul_element.find_all('li', recursive=False):\n",
        "        sub_list_items.append(li.get_text(strip=True))  # Add text content of <li> element to the list\n",
        "        # Recursively get sub list items if there are nested <ul> elements\n",
        "        nested_ul = li.find('ol')\n",
        "        if nested_ul:\n",
        "            sub_list_items.extend(get_sub_list_items(nested_ul))  # Extend the list with items from nested <ul>\n",
        "    return sub_list_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYX23bQenqVb"
      },
      "outputs": [],
      "source": [
        "# Function to retrieve all sub <li> elements within <ul> recursively\n",
        "def get_sub_list_items(ul_element):\n",
        "    sub_list_items = []\n",
        "    for li in ul_element.find_all('li', recursive=False):\n",
        "        sub_list_items.append(li.get_text(strip=True))  # Add text content of <li> element to the list\n",
        "        # Recursively get sub list items if there are nested <ul> elements\n",
        "        nested_ul = li.find('ul')\n",
        "        if nested_ul:\n",
        "            sub_list_items.extend(get_sub_list_items(nested_ul))  # Extend the list with items from nested <ul>\n",
        "    return sub_list_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMvZ9MTAU6_F",
        "outputId": "0386ef90-6aa9-4ad4-8892-b17f6f9333eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['install/#Setting-up-Knet',\n",
              " 'install/#Installation',\n",
              " 'install/#Tips-for-developers',\n",
              " 'install/#Using-Amazon-AWS',\n",
              " 'install/#Using-Microsoft-Azure',\n",
              " 'install/#Using-Ubuntu18.04',\n",
              " 'tutorial/#Introduction-to-Knet',\n",
              " 'tutorial/#Summary',\n",
              " 'tutorial/#Philosophy',\n",
              " 'tutorial/#Tutorial',\n",
              " 'tutorial/#Benchmarks',\n",
              " 'tutorial/#Under-the-hood',\n",
              " 'reference/#Reference',\n",
              " 'reference/#AutoGrad',\n",
              " 'reference/#KnetArray',\n",
              " 'reference/#File-I/O',\n",
              " 'reference/#Parameter-initialization',\n",
              " 'reference/#Activation-functions',\n",
              " 'reference/#Loss-functions',\n",
              " 'reference/#Convolution-and-Pooling',\n",
              " 'reference/#Recurrent-neural-networks',\n",
              " 'reference/#Batch-Normalization',\n",
              " 'reference/#Model-optimization',\n",
              " 'reference/#Hyperparameter-optimization',\n",
              " 'reference/#Utilities',\n",
              " 'reference/#AutoGrad-(advanced)',\n",
              " 'reference/#Per-parameter-optimization-(advanced)',\n",
              " 'reference/#Function-Index',\n",
              " 'backprop/#Backpropagation-and-SGD',\n",
              " 'softmax/#Softmax-Classification',\n",
              " 'mlp/#Multilayer-Perceptrons',\n",
              " 'cnn/#Convolutional-Neural-Networks',\n",
              " 'rnn/#Recurrent-Neural-Networks',\n",
              " 'rl/#Reinforcement-Learning',\n",
              " 'opt/#Optimization',\n",
              " 'gen/#Generalization']"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "links_v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0MaXZabWXDu",
        "outputId": "f9fb6fa2-4738-4e43-e370-b7b178757024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[<h2 id=\"AutoGrad\"><a class=\"docs-heading-anchor\" href=\"#AutoGrad\">AutoGrad</a><a id=\"AutoGrad-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#AutoGrad\" title=\"Permalink\"></a></h2>, <h2 id=\"KnetArray\"><a class=\"docs-heading-anchor\" href=\"#KnetArray\">KnetArray</a><a id=\"KnetArray-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#KnetArray\" title=\"Permalink\"></a></h2>, <h2 id=\"File-I/O\"><a class=\"docs-heading-anchor\" href=\"#File-I/O\">File I/O</a><a id=\"File-I/O-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#File-I/O\" title=\"Permalink\"></a></h2>, <h2 id=\"Parameter-initialization\"><a class=\"docs-heading-anchor\" href=\"#Parameter-initialization\">Parameter initialization</a><a id=\"Parameter-initialization-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Parameter-initialization\" title=\"Permalink\"></a></h2>, <h2 id=\"Activation-functions\"><a class=\"docs-heading-anchor\" href=\"#Activation-functions\">Activation functions</a><a id=\"Activation-functions-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Activation-functions\" title=\"Permalink\"></a></h2>, <h2 id=\"Loss-functions\"><a class=\"docs-heading-anchor\" href=\"#Loss-functions\">Loss functions</a><a id=\"Loss-functions-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Loss-functions\" title=\"Permalink\"></a></h2>, <h2 id=\"Convolution-and-Pooling\"><a class=\"docs-heading-anchor\" href=\"#Convolution-and-Pooling\">Convolution and Pooling</a><a id=\"Convolution-and-Pooling-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Convolution-and-Pooling\" title=\"Permalink\"></a></h2>, <h2 id=\"Recurrent-neural-networks\"><a class=\"docs-heading-anchor\" href=\"#Recurrent-neural-networks\">Recurrent neural networks</a><a id=\"Recurrent-neural-networks-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Recurrent-neural-networks\" title=\"Permalink\"></a></h2>, <h2 id=\"Batch-Normalization\"><a class=\"docs-heading-anchor\" href=\"#Batch-Normalization\">Batch Normalization</a><a id=\"Batch-Normalization-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Batch-Normalization\" title=\"Permalink\"></a></h2>, <h2 id=\"Model-optimization\"><a class=\"docs-heading-anchor\" href=\"#Model-optimization\">Model optimization</a><a id=\"Model-optimization-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Model-optimization\" title=\"Permalink\"></a></h2>, <h2 id=\"Hyperparameter-optimization\"><a class=\"docs-heading-anchor\" href=\"#Hyperparameter-optimization\">Hyperparameter optimization</a><a id=\"Hyperparameter-optimization-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Hyperparameter-optimization\" title=\"Permalink\"></a></h2>, <h2 id=\"Utilities\"><a class=\"docs-heading-anchor\" href=\"#Utilities\">Utilities</a><a id=\"Utilities-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Utilities\" title=\"Permalink\"></a></h2>, <h2 id=\"AutoGrad-(advanced)\"><a class=\"docs-heading-anchor\" href=\"#AutoGrad-(advanced)\">AutoGrad (advanced)</a><a id=\"AutoGrad-(advanced)-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#AutoGrad-(advanced)\" title=\"Permalink\"></a></h2>, <h2 id=\"Per-parameter-optimization-(advanced)\"><a class=\"docs-heading-anchor\" href=\"#Per-parameter-optimization-(advanced)\">Per-parameter optimization (advanced)</a><a id=\"Per-parameter-optimization-(advanced)-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Per-parameter-optimization-(advanced)\" title=\"Permalink\"></a></h2>, <h2 id=\"Function-Index\"><a class=\"docs-heading-anchor\" href=\"#Function-Index\">Function Index</a><a id=\"Function-Index-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Function-Index\" title=\"Permalink\"></a></h2>]\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#AutoGrad\n",
            "Heading: AutoGrad\n",
            "URL: #AutoGrad\n",
            "Reference:  Knet.AutoGrad — Module\n",
            "- Usage:\n",
            " - Code Block: \n",
            " [ x = Param([1,2,3])          # The user declares parameters with `Param`\n",
            "y = @diff sum(x .* x)       # computes gradients using `@diff`\n",
            "grad(y,x) => [2,4,6]        # looks up the gradient of a parameter with `grad` ]\n",
            "- Param(x) returns a struct that acts like x but marks it as a parameter you want to compute gradients with respect to.\n",
            "- @diff expr evaluates an expression and returns a struct that contains its value (which should be a scalar) and gradients with respect to the Params used in the computation.\n",
            "- grad(y, x) returns the gradient of a @diff result y with respect to any parameter x::Param. (nothing may be returned if the gradient is 0).\n",
            "- value(x) returns the value associated with x if x is a Param or the output of @diff, otherwise returns x.\n",
            "- params(x) returns an iterator of Params found by a recursive search of object x, which is typically a model or a @diff result.\n",
            "- Alternative usage:\n",
            " - Code Block: \n",
            " [ x = [1 2 3]\n",
            "f(x) = sum(x .* x)\n",
            "f(x) => 14\n",
            "grad(f)(x) => [2 4 6]\n",
            "gradloss(f)(x) => ([2 4 6], 14) ]\n",
            "- Given a scalar valued function f, grad(f,argnum=1) returns another function g which takes the same inputs as f and returns the gradient of the output with respect to the argnum'th argument. gradloss is similar except the resulting function also returns f's output.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#KnetArray\n",
            "Heading: KnetArray\n",
            "URL: #KnetArray\n",
            "Reference:  Knet.KnetArrays.KnetArray — Type\n",
            " - Code Block: \n",
            " [ KnetArray{T}(undef,dims)\n",
            "KnetArray(a::AbstractArray)\n",
            "Array(k::KnetArray) ]\n",
            "- Container for GPU arrays that supports most of the AbstractArray interface.  The constructor allocates a KnetArray in the currently active device, as specified by CUDA.device(). KnetArrays and Arrays can be converted to each other as shown above, which involves copying to and from the GPU memory.  Only Float32/64 KnetArrays are fully supported.\n",
            "- KnetArrays use the CUDA.jl package for allocation and some operations. Currently some of the custom CUDA kernels that implement elementwise, broadcasting, and reduction operations for KnetArrays work faster. Once these are improved in CUDA.jl, KnetArrays will be retired.\n",
            "- Supported functions:\n",
            "- Indexing: getindex, setindex! with the following index types:1-D: Real, Colon, OrdinalRange, AbstractArray{Real}, AbstractArray{Bool}, CartesianIndex, AbstractArray{CartesianIndex}, EmptyArray, KnetArray{Int32} (low level), KnetArray{0/1} (using float for BitArray) (1-D includes linear indexing of multidimensional arrays)2-D: (Colon,Union{Real,Colon,OrdinalRange,AbstractVector{Real},AbstractVector{Bool},KnetVector{Int32}}), (Union{Real,AbstractUnitRange,Colon}...) (in any order)N-D: (Real...)\n",
            "- 1-D: Real, Colon, OrdinalRange, AbstractArray{Real}, AbstractArray{Bool}, CartesianIndex, AbstractArray{CartesianIndex}, EmptyArray, KnetArray{Int32} (low level), KnetArray{0/1} (using float for BitArray) (1-D includes linear indexing of multidimensional arrays)\n",
            "- 2-D: (Colon,Union{Real,Colon,OrdinalRange,AbstractVector{Real},AbstractVector{Bool},KnetVector{Int32}}), (Union{Real,AbstractUnitRange,Colon}...) (in any order)\n",
            "- N-D: (Real...)\n",
            "- Array operations: ==, !=, adjoint, argmax, argmin, cat, convert, copy, copyto!, deepcopy, display, eachindex, eltype, endof, fill!, findmax, findmin, first, hcat, isapprox, isempty, length, ndims, one, ones, permutedims, pointer, rand!, randn!, reshape, similar, size, stride, strides, summary, transpose, vcat, vec, zero.  (Boolean operators generate outputs with same type as inputs; no support for KnetArray{Bool}.)\n",
            "- Unary functions with broadcasting: -, abs, abs2, acos, acosh, asin, asinh, atan, atanh, cbrt, ceil, cos, cosh, cospi, digamma, erf, erfc, erfcinv, erfcx, erfinv, exp, exp10, exp2, expm1, floor, gamma, lgamma, log, log10, log1p, log2, loggamma, one, round, sign, sin, sinh, sinpi, sqrt, tan, tanh, trigamma, trunc, zero\n",
            "- Binary functions with broadcasting: !=, *, +, -, /, <, <=, ==, >, >=, ^, max, min\n",
            "- Reduction operators: maximum, minimum, prod, sum\n",
            "- Statistics: mean, std, stdm, var, varm\n",
            "- Linear algebra: (*), axpy!, lmul!, norm, rmul!\n",
            "- Knet extras: batchnorm, bce, bmm, cat1d, conv4, cpucopy, deconv4, dropout, elu, gpucopy, logistic, logp, logsoftmax, logsumexp, mat, nll, pool, relu, RNN, selu, sigm, softmax, unpool (Only 4D/5D, Float32/64 KnetArrays support conv4, pool, deconv4, unpool)\n",
            "- 1-D: Real, Colon, OrdinalRange, AbstractArray{Real}, AbstractArray{Bool}, CartesianIndex, AbstractArray{CartesianIndex}, EmptyArray, KnetArray{Int32} (low level), KnetArray{0/1} (using float for BitArray) (1-D includes linear indexing of multidimensional arrays)\n",
            "- 2-D: (Colon,Union{Real,Colon,OrdinalRange,AbstractVector{Real},AbstractVector{Bool},KnetVector{Int32}}), (Union{Real,AbstractUnitRange,Colon}...) (in any order)\n",
            "- N-D: (Real...)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#File-I/O\n",
            "Heading: File I/O\n",
            "URL: #File-I/O\n",
            " - Missing docstring forKnet.save. Check Documenter's build log for details.\n",
            " - Missing docstring forKnet.load. Check Documenter's build log for details.\n",
            " - Missing docstring forKnet.@save. Check Documenter's build log for details.\n",
            " - Missing docstring forKnet.@load. Check Documenter's build log for details.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Parameter-initialization\n",
            "Heading: Parameter initialization\n",
            "URL: #Parameter-initialization\n",
            "Reference:  Knet.Train20.param — Function\n",
            " - Code Block: \n",
            " [ param(array; atype)\n",
            "param(dims...; init, atype)\n",
            "param0(dims...; atype) ]\n",
            "- The first form returns Param(atype(array)).\n",
            "- The second form Returns a randomly initialized Param(atype(init(dims...))).  \n",
            "- The third form param0 is an alias for param(dims...; init=zeros).\n",
            "- By default, init is xavier_uniform and atype is Knet.atype().\n",
            "Reference:  Knet.Train20.xavier — Function\n",
            " - Code Block: \n",
            " [ xavier_uniform(a...; gain=1)\n",
            "xavier(a...; gain=1) ]\n",
            "- Return uniform random weights in the range ± gain * sqrt(6 / (fanin + fanout)).  The a arguments are passed to rand to specify type and dimensions.  See (Glorot and Bengio 2010) or the PyTorch docs for a description.  The function implements equation (16) of the referenced paper. Also known as Glorot initialization. The function xavier is an alias for xavier_uniform. See also xavier_normal.\n",
            "Reference:  Knet.Train20.xavier_uniform — Function\n",
            " - Code Block: \n",
            " [ xavier_uniform(a...; gain=1)\n",
            "xavier(a...; gain=1) ]\n",
            "- Return uniform random weights in the range ± gain * sqrt(6 / (fanin + fanout)).  The a arguments are passed to rand to specify type and dimensions.  See (Glorot and Bengio 2010) or the PyTorch docs for a description.  The function implements equation (16) of the referenced paper. Also known as Glorot initialization. The function xavier is an alias for xavier_uniform. See also xavier_normal.\n",
            "Reference:  Knet.Train20.xavier_normal — Function\n",
            " - Code Block: \n",
            " [ xavier_normal(a...; gain=1) ]\n",
            "- Return normal distributed random weights with mean 0 and std gain * sqrt(2 / (fanin + fanout)).  The a arguments are passed to rand.  See (Glorot and Bengio 2010) and PyTorch docs for a description. Also known as Glorot initialization. See also xavier_uniform.\n",
            "Reference:  Knet.Train20.gaussian — Function\n",
            " - Code Block: \n",
            " [ gaussian(a...; mean=0.0, std=0.01) ]\n",
            "- Return a Gaussian array with a given mean and standard deviation.  The a arguments are passed to randn.\n",
            "Reference:  Knet.Train20.bilinear — Function\n",
            "- Bilinear interpolation filter weights; used for initializing deconvolution layers.\n",
            "- Adapted from https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/surgery.py#L33\n",
            "- Arguments:\n",
            "- T : Data Type\n",
            "- fw: Width upscale factor\n",
            "- fh: Height upscale factor\n",
            "- IN: Number of input filters\n",
            "- ON: Number of output filters\n",
            "- Example usage:\n",
            "- w = bilinear(Float32,2,2,128,128)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Activation-functions\n",
            "Heading: Activation functions\n",
            "URL: #Activation-functions\n",
            "Reference:  Knet.Ops20.elu — Function\n",
            " - Code Block: \n",
            " [ elu(x) ]\n",
            "- Return (x > 0 ? x : exp(x)-1).\n",
            "- Reference: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) (https://arxiv.org/abs/1511.07289).\n",
            "Reference:  Knet.Ops20.relu — Function\n",
            " - Code Block: \n",
            " [ relu(x) ]\n",
            "- Return max(0,x).\n",
            "- References: \n",
            "- Nair and Hinton, 2010. Rectified Linear Units Improve Restricted Boltzmann Machines. ICML.\n",
            "- Glorot, Bordes and Bengio, 2011. Deep Sparse Rectifier Neural Networks. AISTATS.\n",
            "Reference:  Knet.Ops20.selu — Function\n",
            " - Code Block: \n",
            " [ selu(x) ]\n",
            "- Return λ01 * (x > 0 ? x : α01 * (exp(x)-1)) where λ01=1.0507009873554805 and α01=1.6732632423543778.\n",
            "- Reference: Self-Normalizing Neural Networks (https://arxiv.org/abs/1706.02515).\n",
            "Reference:  Knet.Ops20.sigm — Function\n",
            " - Code Block: \n",
            " [ sigm(x) ]\n",
            "- Return 1/(1+exp(-x)).\n",
            "- Reference: Numerically stable sigm implementation from http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Loss-functions\n",
            "Heading: Loss functions\n",
            "URL: #Loss-functions\n",
            "Reference:  Knet.Ops20.accuracy — Function\n",
            " - Code Block: \n",
            " [ accuracy(scores, labels; dims=1, average=true) ]\n",
            "- Given an unnormalized scores matrix and an Integer array of correct labels, return the ratio of instances where the correct label has the maximum score. dims=1 means instances are in columns, dims=2 means instances are in rows. Use average=false to return the pair (ncorrect,count) instead of the ratio (ncorrect/count). The valid labels should be integers in the range 1:numclasses, if labels[i] == 0, instance i is skipped.\n",
            " - Code Block: \n",
            " [ accuracy(model; data, dims=1, average=true, o...) ]\n",
            "- Compute the number of correct predictions of a model over a dataset:\n",
            " - Code Block: \n",
            " [ accuracy(model(inputs; kwargs...), labels; dims) for (inputs,labels) in data ]\n",
            "- and return (ncorrect/count) if average=true or (ncorrect,count) if average=false where count is the number instances not skipped (instances with label==0 are skipped) and ncorrect is the number of them correctly labeled by the model.\n",
            "- The model should be a function returning scores given inputs, and data should be an iterable of (inputs,labels) pairs. The valid labels should be integers in the range 1:numclasses, if labels[i] == 0, instance i is skipped.\n",
            "Reference:  Knet.Ops20.bce — Function\n",
            " - Code Block: \n",
            " [ bce(scores, labels; average=true) ]\n",
            "- Computes binary cross entropy loss given predicted unnormalized scores and answer labels for a binary prediction task. Label values should be in {0,1}. Scores are unrestricted and will be converted to probabilities using\n",
            " - Code Block: \n",
            " [ probs = 1 ./ (1 .+ exp.(-scores)) ]\n",
            "- The loss calculated is\n",
            " - Code Block: \n",
            " [ -(labels .* log.(probs) .+ (1 .- labels) .* log.(1 .- probs)) ]\n",
            "- The return value is (total/count) if average=true and (total,count) if average=false where count is the number of instances and total is their total loss.\n",
            "- See also logistic which computes the same loss with {-1,1} labels.\n",
            "- Reference: https://towardsdatascience.com/nothing-but-numpy-understanding-creating-binary-classification-neural-networks-with-e746423c8d5c\n",
            "Reference:  Knet.Ops20.logistic — Function\n",
            " - Code Block: \n",
            " [ logistic(scores, labels; average=true) ]\n",
            "- Computes logistic loss given predicted unnormalized scores and answer labels for a binary prediction task.\n",
            " - Code Block: \n",
            " [ log.(1 .+ exp.(-labels .* scores)) ]\n",
            "- Label values should be {-1,1}. Scores are unrestricted.  The return value is (total/count) if average=true and (total,count) if average=false where count is the number of instances and total is their total loss.\n",
            "- See also bce which computes the same loss with {0,1} labels.\n",
            "- Reference: https://towardsdatascience.com/nothing-but-numpy-understanding-creating-binary-classification-neural-networks-with-e746423c8d5c\n",
            "Reference:  Knet.Ops20.logp — Function\n",
            " - Code Block: \n",
            " [ softmax(x; dims=:)\n",
            "logsoftmax(x; dims=:) ]\n",
            "- Treat entries in x as as unnormalized log probabilities and return normalized (log) probabilities, i.e. \n",
            " - Code Block: \n",
            " [ softmax(x; dims) = exp.(x) ./ sum(exp.(x); dims=dims)\n",
            "logsoftmax(x; dims) = x .- log.(sum(exp.(x); dims=dims)) ]\n",
            "- For numerical stability x = x .- maximum(x,dims=dims) is performed before exponentiation.\n",
            "- dims is an optional argument, if not specified the normalization is over the whole x, otherwise the normalization is performed over the given dimensions.  In particular, if x is a matrix, dims=1 normalizes columns of x and dims=2 normalizes rows of x.\n",
            "Reference:  Knet.Ops20.logsoftmax — Function\n",
            " - Code Block: \n",
            " [ softmax(x; dims=:)\n",
            "logsoftmax(x; dims=:) ]\n",
            "- Treat entries in x as as unnormalized log probabilities and return normalized (log) probabilities, i.e. \n",
            " - Code Block: \n",
            " [ softmax(x; dims) = exp.(x) ./ sum(exp.(x); dims=dims)\n",
            "logsoftmax(x; dims) = x .- log.(sum(exp.(x); dims=dims)) ]\n",
            "- For numerical stability x = x .- maximum(x,dims=dims) is performed before exponentiation.\n",
            "- dims is an optional argument, if not specified the normalization is over the whole x, otherwise the normalization is performed over the given dimensions.  In particular, if x is a matrix, dims=1 normalizes columns of x and dims=2 normalizes rows of x.\n",
            "Reference:  Knet.Ops20.logsumexp — Function\n",
            " - Code Block: \n",
            " [ logsumexp(x;dims=:) ]\n",
            "- Compute log(sum(exp(x);dims)) in a numerically stable manner.\n",
            "- dims is an optional argument, if not specified the summation is over the whole x, otherwise the summation is performed over the given dimensions.  In particular if x is a matrix, dims=1 sums columns of x and dims=2 sums rows of x.\n",
            "Reference:  Knet.Ops20.nll — Function\n",
            " - Code Block: \n",
            " [ nll(scores, labels; dims=1, average=true) ]\n",
            "- Return the negative log likelihood for a single batch of data given an unnormalized scores matrix and an Integer array of correct labels. The scores matrix should have size (classes,instances) if dims=1 or (instances,classes) if dims=2. labels[i] should be in 1:classes to indicate the correct class for instance i, or 0 to skip instance i.\n",
            "- The return value is (total/count) if average=true and (total,count) if average=false where count is the number of instances not skipped (i.e. label != 0) and total is their total negative log likelihood.\n",
            "- Example\n",
            "- Let's assume that there are three classes (cat, dog, ostrich) and just 2 instances with the unnormalized score scores[:,1] and scores[:,2] respectively. The first instance is actually a cat and the second instance a dog:\n",
            " - Code Block: \n",
            " [ scores = [12.2    0.3;\n",
            "           2.0   21.5;\n",
            "           0.0  -21.0]\n",
            "labels = [1, 2]\n",
            "nll(scores,labels)\n",
            "# returns 2.1657e-5 ]\n",
            "- The probabilites are derived from the scores and the negative log-probabilities corresponding to the labels are averaged:\n",
            " - Code Block: \n",
            " [ probabilites = exp.(scores) ./ sum(exp.(scores),dims=1)\n",
            "-(log(probabilites[labels[1],1]) + log(probabilites[labels[2],2]))/2\n",
            "# returns 2.1657e-5 ]\n",
            " - Code Block: \n",
            " [ nll(model; data, dims=1, average=true, o...) ]\n",
            "- Compute the negative log likelihood for a model over a dataset:\n",
            " - Code Block: \n",
            " [ nll(model(inputs; kwargs...), labels; dims) for (inputs,labels) in data ]\n",
            "- and return (total/count) if average=true or (total,count) if average=false where count is the number of instances not skipped (instances with label==0 are skipped) and total is their total negative log likelihood.\n",
            "- The model should be a function returning scores given inputs, and data should be an iterable of (inputs,labels) pairs. The valid labels should be integers in the range 1:numclasses, if labels[i] == 0, instance i is skipped.\n",
            "Reference:  Knet.Ops20.softmax — Function\n",
            " - Code Block: \n",
            " [ softmax(x; dims=:)\n",
            "logsoftmax(x; dims=:) ]\n",
            "- Treat entries in x as as unnormalized log probabilities and return normalized (log) probabilities, i.e. \n",
            " - Code Block: \n",
            " [ softmax(x; dims) = exp.(x) ./ sum(exp.(x); dims=dims)\n",
            "logsoftmax(x; dims) = x .- log.(sum(exp.(x); dims=dims)) ]\n",
            "- For numerical stability x = x .- maximum(x,dims=dims) is performed before exponentiation.\n",
            "- dims is an optional argument, if not specified the normalization is over the whole x, otherwise the normalization is performed over the given dimensions.  In particular, if x is a matrix, dims=1 normalizes columns of x and dims=2 normalizes rows of x.\n",
            "Reference:  Knet.Ops20.zeroone — Function\n",
            "- zeroone loss is equal to 1 - accuracy\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Convolution-and-Pooling\n",
            "Heading: Convolution and Pooling\n",
            "URL: #Convolution-and-Pooling\n",
            "Reference:  Knet.Ops20.conv4 — Function\n",
            " - Code Block: \n",
            " [ conv4(w, x; kwargs...) ]\n",
            "- Execute convolutions or cross-correlations using filters specified with w over tensor x.\n",
            "- If w has dimensions (W1,W2,...,Cx,Cy) and x has dimensions (X1,X2,...,Cx,N), the result y will have dimensions (Y1,Y2,...,Cy,N) where Cx is the number of input channels, Cy is the number of output channels, N is the number of instances, and Wi,Xi,Yi are spatial dimensions with Yi determined by:\n",
            " - Code Block: \n",
            " [ Yi = 1 + floor((Xi + 2*padding[i] - ((Wi-1)*dilation[i] + 1)) / stride[i]) ]\n",
            "- padding, stride and dilation are keyword arguments that can be specified as a single number (in which case they apply to all dimensions), or an array/tuple with entries for each spatial dimension.\n",
            "- Keywords\n",
            "- padding=0: the number of extra zeros implicitly concatenated at the start and end of each dimension.\n",
            "- stride=1: the number of elements to slide to reach the next filtering window.\n",
            "- dilation=1: dilation factor for each dimension.\n",
            "- mode=0: 0 for convolution and 1 for cross-correlation (which flips the filter).\n",
            "- alpha=1: can be used to scale the result.\n",
            "- group=1: can be used to perform grouped convolutions.\n",
            "Reference:  Knet.Ops20.deconv4 — Function\n",
            " - Code Block: \n",
            " [ deconv4(w, x; kwargs...) ]\n",
            "- Simulate 4-D deconvolution by using transposed convolution operation. Its forward pass is equivalent to backward pass of a convolution (gradients with respect to input tensor). Likewise, its backward pass (gradients with respect to input tensor) is equivalent to forward pass of a convolution. Since it swaps forward and backward passes of convolution operation, padding and stride options belong to output tensor. See this report for further explanation.\n",
            "- If w has dimensions (W1,W2,...,Cy,Cx) and x has dimensions (X1,X2,...,Cx,N), the result y=deconv4(w,x) will have dimensions (Y1,Y2,...,Cy,N) where\n",
            " - Code Block: \n",
            " [ Yi = (Xi - 1)*stride[i] + ((Wi-1)*dilation[i] + 1) - 2*padding[i] ]\n",
            "- Here Cx is the number of x channels, Cy is the number of y channels, N is the number of instances, and Wi,Xi,Yi are spatial dimensions. Padding and stride are keyword arguments that can be specified as a single number (in which case they apply to all dimensions), or an array/tuple with entries for each spatial dimension.\n",
            "- Keywords\n",
            "- padding=0: the number of extra zeros implicitly concatenated at the start and at the end of each dimension.\n",
            "- stride=1: the number of elements to slide to reach the next filtering window.\n",
            "- mode=0: 0 for convolution and 1 for cross-correlation.\n",
            "- alpha=1: can be used to scale the result.\n",
            "- handle: handle to a previously created cuDNN context. Defaults to a Knet allocated handle.\n",
            "- group=1: can be used to perform grouped convolutions.\n",
            "Reference:  Knet.Ops20.pool — Function\n",
            " - Code Block: \n",
            " [ pool(x; kwargs...) ]\n",
            "- Compute pooling of input values (i.e., the maximum or average of several adjacent values) to produce an output with smaller height and/or width.\n",
            "- If x has dimensions (X1,X2,...,Cx,N), the result y will have dimensions (Y1,Y2,...,Cx,N) where\n",
            " - Code Block: \n",
            " [ Yi=1+floor((Xi+2*padding[i]-window[i])/stride[i]) ]\n",
            "- Here Cx is the number of input channels, N is the number of instances, and Xi,Yi are spatial dimensions.  window, padding and stride are keyword arguments that can be specified as a single number (in which case they apply to all dimensions), or an array/tuple with entries for each spatial dimension.\n",
            "- Keywords:\n",
            "- window=2: the pooling window size for each dimension.\n",
            "- padding=0: the number of extra zeros implicitly concatenated at the start and at the end of each dimension.\n",
            "- stride=window: the number of elements to slide to reach the next pooling window.\n",
            "- mode=0: 0 for max, 1 for average including padded values, 2 for average excluding padded values, 3 for deterministic max.\n",
            "- maxpoolingNanOpt=1: Nan numbers are not propagated if 0, they are propagated if 1.\n",
            "- alpha=1: can be used to scale the result.\n",
            "Reference:  Knet.Ops20.unpool — Function\n",
            " - Code Block: \n",
            " [ unpool(x; o...) ]\n",
            "- Perform the reverse of pooling: x == pool(unpool(x;o...); o...)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Recurrent-neural-networks\n",
            "Heading: Recurrent neural networks\n",
            "URL: #Recurrent-neural-networks\n",
            "Reference:  Knet.Ops20.RNN — Type\n",
            " - Code Block: \n",
            " [ rnn = RNN(inputSize, hiddenSize; opts...)\n",
            "rnn(x; batchSizes) => y\n",
            "rnn.h, rnn.c  # hidden and cell states ]\n",
            "- RNN returns a callable RNN object rnn. Given a minibatch of sequences x, rnn(x) returns y, the hidden states of the final layer for each time step. rnn.h and rnn.c fields can be used to set the initial hidden states and read the final hidden states of all layers.  Note that the final time step of y always contains the final hidden state of the last layer, equivalent to rnn.h for a single layer network.\n",
            "- Dimensions: The input x can be 1, 2, or 3 dimensional and y will have the same number of dimensions as x. size(x)=(X,[B,T]) and size(y)=(H/2H,[B,T]) where X is inputSize, B is batchSize, T is seqLength, H is hiddenSize, 2H is for bidirectional RNNs. By default a 1-D x represents a single instance for a single time step, a 2-D x represents a single minibatch for a single time step, and a 3-D x represents a sequence of identically sized minibatches for multiple time steps. The output y gives the hidden state (of the final layer for multi-layer RNNs) for each time step. The fields rnn.h and rnn.c represent the hidden states of all layers in a single time step and have size (H,B,L/2L) where L is numLayers and 2L is for bidirectional RNNs.\n",
            "- batchSizes: If batchSizes=nothing (default), all sequences in a minibatch are assumed to be the same length. If batchSizes is an array of (non-increasing) integers, it gives us the batch size for each time step (allowing different sequences in the minibatch to have different lengths). In this case x will typically be 2-D with the second dimension representing variable size batches for time steps. If batchSizes is used, sum(batchSizes) should equal length(x) ÷ size(x,1). When the batch size is different in every time step, hidden states will have size (H,B,L/2L) where B is always the size of the first (largest) minibatch.\n",
            "- Hidden states: The hidden and cell states are kept in rnn.h and rnn.c fields (the cell state is only used by LSTM). They can be initialized during construction using the h and c keyword arguments, or modified later by direct assignment. Valid values are nothing (default), 0, or an array of the right type and size possibly wrapped in a Param. If the value is nothing the initial state is assumed to be zero and the final state is discarded keeping the value nothing. If the value is 0 the initial state is assumed to be zero and 0 is replaced by the final state on return. If the value is a valid state, it is used as the initial state and is replaced by the final state on return.\n",
            "- In a differentiation context the returned final hidden states will be wrapped in Result types. This is necessary if the same RNN object is to be called multiple times in a single iteration. Between iterations (i.e. after diff/update) the hidden states need to be unboxed with e.g. rnn.h = value(rnn.h) to prevent spurious dependencies. This happens automatically during the backward pass for GPU RNNs but needs to be done manually for CPU RNNs. See the CharLM Tutorial for an example.\n",
            "- Keyword arguments for RNN:\n",
            "- h=nothing: Initial hidden state.\n",
            "- c=nothing: Initial cell state.\n",
            "- rnnType=:lstmType of RNN: One of :relu, :tanh, :lstm, :gru.\n",
            "- numLayers=1: Number of RNN layers.\n",
            "- bidirectional=false: Create a bidirectional RNN iftrue.\n",
            "- dropout=0: Dropout probability. Applied to input and between layers.\n",
            "- skipInput=false: Do not multiply the input with a matrix iftrue.\n",
            "- algo=0: Algorithm to use, see CUDNN docs for details.\n",
            "- seed=0: Random number seed for dropout. Usestime()if 0.\n",
            "- winit=xavier: Weight initialization method for matrices.\n",
            "- binit=zeros: Weight initialization method for bias vectors.\n",
            "- finit=ones: Weight initialization method for the bias of forget gates.\n",
            "- atype=Knet.atype(): array type for model weights.\n",
            "- Formulas: RNNs compute the output h[t] for a given iteration from the recurrent input h[t-1] and the previous layer input x[t] given matrices W, R and biases bW, bR from the following equations:\n",
            "- :relu and :tanh: Single gate RNN with activation function f:\n",
            " - Code Block: \n",
            " [ h[t] = f(W * x[t] .+ R * h[t-1] .+ bW .+ bR) ]\n",
            "- :gru: Gated recurrent unit:\n",
            " - Code Block: \n",
            " [ i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
            "r[t] = sigm(Wr * x[t] .+ Rr * h[t-1] .+ bWr .+ bRr) # reset gate\n",
            "n[t] = tanh(Wn * x[t] .+ r[t] .* (Rn * h[t-1] .+ bRn) .+ bWn) # new gate\n",
            "h[t] = (1 - i[t]) .* n[t] .+ i[t] .* h[t-1] ]\n",
            "- :lstm: Long short term memory unit with no peephole connections:\n",
            " - Code Block: \n",
            " [ i[t] = sigm(Wi * x[t] .+ Ri * h[t-1] .+ bWi .+ bRi) # input gate\n",
            "f[t] = sigm(Wf * x[t] .+ Rf * h[t-1] .+ bWf .+ bRf) # forget gate\n",
            "o[t] = sigm(Wo * x[t] .+ Ro * h[t-1] .+ bWo .+ bRo) # output gate\n",
            "n[t] = tanh(Wn * x[t] .+ Rn * h[t-1] .+ bWn .+ bRn) # new gate\n",
            "c[t] = f[t] .* c[t-1] .+ i[t] .* n[t]               # cell output\n",
            "h[t] = o[t] .* tanh(c[t]) ]\n",
            "Reference:  Knet.Ops20.rnnparam — Function\n",
            " - Code Block: \n",
            " [ rnnparam(r::RNN, layer, id, param) ]\n",
            "- Return a single weight matrix or bias vector as a slice of RNN weights.\n",
            "- Valid layer values:\n",
            "- For unidirectional RNNs 1:numLayers\n",
            "- For bidirectional RNNs 1:2*numLayers, forw and back layers alternate.\n",
            "- Valid id values:\n",
            "- For RELU and TANH RNNs, input = 1, hidden = 2.\n",
            "- For GRU reset = 1,4; update = 2,5; newmem = 3,6; 1:3 for input, 4:6 for hidden\n",
            "- For LSTM inputgate = 1,5; forget = 2,6; newmem = 3,7; output = 4,8; 1:4 for input, 5:8 for hidden\n",
            "- Valid param values:\n",
            "- Return the weight matrix (transposed!) ifparam==1.\n",
            "- Return the bias vector ifparam==2.\n",
            "- The effect of skipInput: Let I=1 for RELU/TANH, 1:3 for GRU, 1:4 for LSTM\n",
            "- For skipInput=false (default), rnnparam(r,1,I,1) is a (inputSize,hiddenSize) matrix.\n",
            "- For skipInput=true, rnnparam(r,1,I,1) isnothing.\n",
            "- For bidirectional, the same applies to rnnparam(r,2,I,1): the first back layer.\n",
            "- The input biases (par=2) are returned even if skipInput=true.\n",
            "Reference:  Knet.Ops20.rnnparams — Function\n",
            " - Code Block: \n",
            " [ rnnparams(r::RNN) ]\n",
            "- Return the RNN parameters as an Array{Any}.\n",
            "- The order of params returned (subject to change):\n",
            "- All weight matrices come before all bias vectors.\n",
            "- Matrices and biases are sorted lexically based on (layer,id).\n",
            "- See @doc rnnparam for valid layer and id values.\n",
            "- Input multiplying matrices arenothingif r.inputMode = 1.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Batch-Normalization\n",
            "Heading: Batch Normalization\n",
            "URL: #Batch-Normalization\n",
            "Reference:  Knet.Ops20.batchnorm — Function\n",
            " - Code Block: \n",
            " [ batchnorm(x[, moments, params]; kwargs...) ]\n",
            "- perform batch normalization on x with optional mean and variance in moments and scaling factor and bias in params. See https://arxiv.org/abs/1502.03167 for reference.\n",
            "- 2d, 4d and 5d inputs are supported. Mean and variance are computed over dimensions (2,), (1,2,4) and (1,2,3,5) for 2d, 4d and 5d arrays, respectively.\n",
            "- moments stores running mean and variance to be used at inference time.  It is optional in training mode, but mandatory in test mode.  Training and test modes can be controlled by the training keyword argument which defaults to Knet.training().\n",
            "- params stores the optional affine parameters gamma and beta.  bnparams function can be used to initialize params.\n",
            "- Example\n",
            " - Code Block: \n",
            " [ # Inilization, C is an integer\n",
            "moments = bnmoments()\n",
            "params = bnparams(C)\n",
            "...\n",
            "# size(x) -> (H, W, C, N)\n",
            "y = batchnorm(x, moments, params)\n",
            "# size(y) -> (H, W, C, N) ]\n",
            "- Keywords\n",
            "- eps=1e-5: The epsilon parameter added to the variance to avoid division by 0.\n",
            "- training=Knet.training(): When training is true, the mean and variance of x are used  and moments argument is modified if it is provided. When training is false, mean and  variance stored in the moments argument are used.\n",
            "Reference:  Knet.Ops20.bnmoments — Function\n",
            " - Code Block: \n",
            " [ bnmoments(;momentum=0.1, mean=nothing, var=nothing, meaninit=zeros, varinit=ones) ]\n",
            "- Return a BNMoments object, a data structure used to store running mean and running variance of batch normalization with the following fields:\n",
            "- momentum=0.1: A real number between 0 and 1 to be used as the scale of last\n",
            "- mean and variance. The existing running mean or variance is multiplied by (1-momentum).\n",
            "- mean=nothing: The running mean.\n",
            "- var=nothing: The running variance.\n",
            "- meaninit=zeros: The function used for initialize the running mean. Should either be\n",
            "- nothing or of the form ([eltype], dims...)->data. zeros is a good option.\n",
            "- varinit=ones: The function used for initialize the running variance. Should either be\n",
            "- nothing or ([eltype], dims...)->data. ones is a good option.\n",
            "- This constructor can be used directly load moments from data. meaninit and varinit are called if mean and var are nothing. Type and size of the mean and var are determined automatically from the inputs in the batchnorm calls.\n",
            "Reference:  Knet.Ops20.bnparams — Function\n",
            " - Code Block: \n",
            " [ bnparams(etype, channels::Integer) ]\n",
            "- Return a single 1d array that contains both scale and bias of batchnorm, where the first half is scale and the second half is bias.\n",
            "- bnparams(channels) calls bnparams(Float64, channels), following Julia convention.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Model-optimization\n",
            "Heading: Model optimization\n",
            "URL: #Model-optimization\n",
            "Reference:  Knet.Train20.minimize — Function\n",
            " - Code Block: \n",
            " [ minimize(func, data, optimizer=Adam(); params)\n",
            "sgd     (func, data; lr=0.1,  gclip, params)\n",
            "momentum(func, data; lr=0.05, gamma=0.95, gclip, params)\n",
            "nesterov(func, data; lr=0.05, gamma=0.95, gclip, params)\n",
            "adagrad (func, data; lr=0.05, eps=1e-6, gclip, params)\n",
            "rmsprop (func, data; lr=0.01, rho=0.9, eps=1e-6, gclip, params)\n",
            "adadelta(func, data; lr=1.0,  rho=0.9, eps=1e-6, gclip, params)\n",
            "adam    (func, data; lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, gclip, params) ]\n",
            "- Return an iterator which applies func to arguments in data, i.e.  (func(args...) for args in data), and updates the parameters every iteration to minimize func.  func should return a scalar value.\n",
            "- The common keyword argument params can be used to list the Params to be optimized.  If not specified, any Param that takes part in the computation of func(args...) will be updated.\n",
            "- The common keyword argument gclip can be used to implement per-parameter gradient clipping. For a parameter gradient g, if norm(g) > gclip > 0, g is scaled so that its norm is equal to gclip. If not specified no gradient clipping is performed.\n",
            "- These functions do not perform optimization, but return an iterator that can. Any function that produces values from an iterator can be used with such an object, e.g. progress!(sgd(f,d)) iterates the sgd optimizer and displays a progress bar. For convenience, appending ! to the name of the function iterates and returns nothing, i.e. sgd!(...) is equivalent to (for x in sgd(...) end).\n",
            "- We define optimizers as lazy iterators to have explicit control over them:\n",
            "- To report progress useprogress(sgd(f,d)).\n",
            "- To run until convergence useconverge(sgd(f,cycle(d))).\n",
            "- To run multiple epochs usesgd(f,repeat(d,n)).\n",
            "- To run a given number of iterations usesgd(f,take(cycle(d),n)).\n",
            "- To do a task every n iterations use(task() for (i,j) in enumerate(sgd(f,d)) if i%n == 1).\n",
            "- These functions apply the same algorithm with the same configuration to every parameter by default. minimize takes an explicit optimizer argument, all others call minimize with an appropriate optimizer argument (see @doc update! for a list of possible optimizers). Before calling update! on a Param, minimize sets its opt field to a copy of this default optimizer if it is not already set. The opt field is used by the update! function to determine the type of update performed on that parameter.  If you need finer grained control, you can set the optimizer of an individual Param by setting its opt field before calling one of these functions. They will not override the opt field if it is already set, e.g. sgd(model,data) will perform an Adam update for a parameter whose opt field is an Adam object. This also means you can stop and start the training without losing optimization state, the first call will set the opt fields and the subsequent calls will not override them.\n",
            "- Given a parameter w and its gradient g here are the updates applied by each optimizer:\n",
            " - Code Block: \n",
            " [ # sgd (http://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
            "w .= w - lr * g\n",
            "\n",
            "# momentum (http://jlmelville.github.io/mize/nesterov.html)\n",
            "v .= gamma * v - lr * g\n",
            "w .= w + v\n",
            "\n",
            "# nesterov (http://jlmelville.github.io/mize/nesterov.html)\n",
            "w .= w - gamma * v\n",
            "v .= gamma * v - lr * g\n",
            "w .= w + (1 + gamma) * v\n",
            "\n",
            "# adagrad (http://www.jmlr.org/papers/v12/duchi11a.html)\n",
            "G .= G + g .^ 2\n",
            "w .= w - lr * g ./ sqrt(G + eps)\n",
            "\n",
            "# rmsprop (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
            "G .= rho * G + (1-rho) * g .^ 2 \n",
            "w .= w - lr * g ./ sqrt(G + eps)\n",
            "\n",
            "# adadelta (http://arxiv.org/abs/1212.5701)\n",
            "G .= rho * G + (1-rho) * g .^ 2\n",
            "update = sqrt(delta + eps) .* g ./ sqrt(G + eps)\n",
            "w = w - lr * update\n",
            "delta = rho * delta + (1-rho) * update .^ 2\n",
            "\n",
            "# adam (http://arxiv.org/abs/1412.6980)\n",
            "v = beta1 * v + (1 - beta1) * g\n",
            "G = beta2 * G + (1 - beta2) * g .^ 2\n",
            "vhat = v ./ (1 - beta1 ^ t)\n",
            "Ghat = G ./ (1 - beta2 ^ t)\n",
            "w = w - (lr / (sqrt(Ghat) + eps)) * vhat ]\n",
            "Reference:  Knet.Train20.converge — Function\n",
            " - Code Block: \n",
            " [ converge(itr; alpha=0.1) ]\n",
            "- Return an iterator which acts exactly like itr, but quits when values from itr stop decreasing. itr should produce numeric values.\n",
            "- It can be used to train a model with the data cycled:\n",
            " - Code Block: \n",
            " [ progress!(converge(minimize(model,cycle(data)))) ]\n",
            "- alpha controls the exponential average of values to detect convergence. Here is how convergence is decided:\n",
            " - Code Block: \n",
            " [ p = x - avgx\n",
            "avgx = c.alpha * x + (1-c.alpha) * avgx\n",
            "avgp = c.alpha * p + (1-c.alpha) * avgp\n",
            "avgp > 0.0 && return nothing ]\n",
            "- converge!(...) is equivalent to (for x in converge(...) end), i.e.  iterates over the object created by converge(...) and returns nothing.\n",
            "Reference:  Knet.Train20.minibatch — Function\n",
            " - Code Block: \n",
            " [ minibatch(x, [y], batchsize; shuffle, partial, xtype, ytype, xsize, ysize) ]\n",
            "- Return an iterator of minibatches [(xi,yi)...] given data tensors x, y and batchsize.  \n",
            "- The last dimension of x and y give the number of instances and should be equal. y is optional, if omitted a sequence of xi will be generated rather than (xi,yi) tuples.  Use repeat(d,n) for multiple epochs, Iterators.take(d,n) for a partial epoch, and Iterators.cycle(d) to cycle through the data forever (this can be used with converge). If you need the iterator to continue from its last position when stopped early (e.g. by a break in a for loop), use Iterators.Stateful(d) (by default the iterator would restart from the beginning).\n",
            "- Keyword arguments:\n",
            "- shuffle=false: Shuffle the instances every epoch.\n",
            "- partial=false: If true include the last partial minibatch < batchsize.\n",
            "- xtype=typeof(x): Convert xi in minibatches to this type.\n",
            "- ytype=typeof(y): Convert yi in minibatches to this type.\n",
            "- xsize=size(x): Convert xi in minibatches to this shape (with last dimension adjusted for batchsize).\n",
            "- ysize=size(y): Convert yi in minibatches to this shape (with last dimension adjusted for batchsize).\n",
            "Reference:  Knet.Train20.progress — Function\n",
            " - Code Block: \n",
            " [ progress(msg, itr; steps, seconds, io)\n",
            "progress(itr; o...) do p; [body of the msg function]; end\n",
            "progress(itr; o...)\n",
            "progress!(...) ]\n",
            "- Return a Progress iterator which acts exactly like itr, but prints a progressbar:\n",
            " - Code Block: \n",
            " [ ┣█████████████████▎  ┫ [86.83%, 903/1040, 01:36/01:50, 9.42i/s] 3.87835 ]\n",
            "- Here 86.83% is the percentage completed, 903 is the number of iterations completed, 1040 is the total number of iterations. 01:36 is elapsed time, 01:50 is the estimated total time, 9.42i/s is the average number of iterations completed per second. If the speed is less than 1, the average number of seconds per iteration (s/i) is reported instead.  The bar, percent, total iterations, and estimated total time are omitted for iterators whose size is unknown.\n",
            "- The 3.87835 at the end is the output of the msg function applied to the Progress iterator. The message can be customized by the first two forms above, if not specified (the third form) nothing gets printed at the end of the line. The message function can use the following fields of its p::Progress argument: p.currval is the current iterator value and p.curriter is the current iteration count.\n",
            "- The progress bar is updated and msg is called with the Progress iterator every steps iterations or every seconds seconds in addition to the first and the last iteration. If neither steps nor seconds is specified the default is to update every second. The keyword argument io determines where the progress bar is printed, the default is stderr.\n",
            "- The last form, progress!(...), is equivalent to (for x in progress(...) end), i.e. iterates over the object created by progress(...) and returns nothing.\n",
            " - Missing docstring forKnet.training. Check Documenter's build log for details.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Hyperparameter-optimization\n",
            "Heading: Hyperparameter optimization\n",
            "URL: #Hyperparameter-optimization\n",
            "Reference:  Knet.Train20.goldensection — Function\n",
            " - Code Block: \n",
            " [ goldensection(f,n;kwargs) => (fmin,xmin) ]\n",
            "- Find the minimum of f using concurrent golden section search in n dimensions. See Knet.goldensection_demo() for an example.\n",
            "- f is a function from a Vector{Float64} of length n to a Number.  It can return NaN for out of range inputs.  Goldensection will always start with a zero vector as the initial input to f, and the initial step size will be 1 in each dimension.  The user should define f to scale and shift this input range into a vector meaningful for their application. For positive inputs like learning rate or hidden size, you can use a transformation such as x0*exp(x) where x is a value goldensection passes to f and x0 is your initial guess for this value. This will effectively start the search at x0, then move with multiplicative steps.\n",
            "- I designed this algorithm combining ideas from Golden Section Search and Hill Climbing Search. It essentially runs golden section search concurrently in each dimension, picking the next step based on estimated gain.\n",
            "- Keyword arguments\n",
            "- dxmin=0.1: smallest step size.\n",
            "- accel=φ: acceleration rate. Golden ratioφ=1.618...is best.\n",
            "- verbose=false: usetrueto print individual steps.\n",
            "- history=[]: cache of[(x,f(x)),...]function evaluations.\n",
            "Reference:  Knet.Train20.hyperband — Function\n",
            " - Code Block: \n",
            " [ hyperband(getconfig, getloss, maxresource=27, reduction=3) ]\n",
            "- Hyperparameter optimization using the hyperband algorithm from (Lisha et al. 2016).  You can try a simple MNIST example using Knet.hyperband_demo(). \n",
            "- Arguments\n",
            "- getconfig()returns random configurations with a user defined type and distribution.\n",
            "- getloss(c,n)returns loss for configurationcand number of resources (e.g. epochs)n.\n",
            "- maxresourceis the maximum number of resources any one configuration should be given.\n",
            "- reductionis an algorithm parameter (see paper), 3 is a good value.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Utilities\n",
            "Heading: Utilities\n",
            "URL: #Utilities\n",
            "Reference:  Knet.Ops20.bmm — Function\n",
            " - Code Block: \n",
            " [ bmm(A, B ; transA=false, transB=false) ]\n",
            "- Perform a batch matrix-matrix product of matrices stored in A and B. size(A,2) == size(B,1) and size(A)[3:end] and size(B)[3:end] must match.  If A is a (m,n,b...) tensor, B is a (n,k,b...) tensor, and the output is a (m,k,b...)  tensor.\n",
            "Reference:  AutoGrad.cat1d — Function\n",
            " - Code Block: \n",
            " [ cat1d(args...) ]\n",
            "- Return vcat(vec.(args)...) but possibly more efficiently. Can be used to concatenate the contents of arrays with different shapes and sizes.\n",
            " - Missing docstring forKnet.cpucopy. Check Documenter's build log for details.\n",
            "Reference:  Knet.dir — Function\n",
            "- Construct a path relative to Knet root, e.g. Knet.dir(\"examples\") => \"~/.julia/dev/Knet/examples\"\n",
            "Reference:  Knet.Ops20.dropout — Function\n",
            " - Code Block: \n",
            " [ dropout(x, p; drop, seed) ]\n",
            "- Given an array x and probability 0<=p<=1 return an array y in which each element is 0 with probability p or x[i]/(1-p) with probability 1-p. Just return x if p==0, or drop=false. By default drop=true in a @diff context, drop=false otherwise.  Specify a non-zero seed::Number to set the random number seed for reproducible results. See (Srivastava et al. 2014) for a reference.\n",
            "Reference:  Knet.KnetArrays.gc — Function\n",
            " - Code Block: \n",
            " [ Knet.gc(dev=CUDA.device().handle) ]\n",
            "- cudaFree all pointers allocated on device dev that were previously allocated and garbage collected. Normally Knet holds on to all garbage collected pointers for reuse. Try this if you run out of GPU memory.\n",
            " - Missing docstring forKnet.gpu. Check Documenter's build log for details.\n",
            " - Missing docstring forKnet.gpucopy. Check Documenter's build log for details.\n",
            " - Missing docstring forKnet.invx. Check Documenter's build log for details.\n",
            "Reference:  Knet.Ops20.mat — Function\n",
            " - Code Block: \n",
            " [ mat(x; dims = ndims(x) - 1) ]\n",
            "- Reshape x into a two-dimensional matrix by joining the first dims dimensions, i.e.  reshape(x, prod(size(x,i) for i in 1:dims), :)\n",
            "- dims=ndims(x)-1 (default) is typically used when turning the output of a 4-D convolution result into a 2-D input for a fully connected layer.\n",
            "- dims=1 is typically used when turning the 3-D output of an RNN layer into a 2-D input for a fully connected layer.\n",
            "- dims=0 will turn the input into a row vector, dims=ndims(x) will turn it into a column vector.\n",
            "Reference:  Knet.KnetArrays.seed! — Function\n",
            "- Call both CUDA.seed! (if available) and Random.seed!\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#AutoGrad-(advanced)\n",
            "Heading: AutoGrad (advanced)\n",
            "URL: #AutoGrad-(advanced)\n",
            "Reference:  AutoGrad.@gcheck — Macro\n",
            " - Code Block: \n",
            " [ gcheck(f, x...; kw, o...)\n",
            "@gcheck f(x...; kw...) (opt1=val1,opt2=val2,...) ]\n",
            "- Numerically check the gradient of f(x...; kw...) and return a boolean result.\n",
            "- Example call: gcheck(nll,model,x,y) or @gcheck nll(model,x,y). The parameters should be marked as Param arrays in f, x, and/or kw.  Only 10 random entries in each large numeric array are checked by default.  If the output of f is not a number, we check the gradient of sum(f(x...; kw...)). Keyword arguments:\n",
            "- kw=(): keyword arguments to be passed tof, i.e.f(x...; kw...)\n",
            "- nsample=10: number of random entries from each param to check\n",
            "- atol=0.01,rtol=0.05: tolerance parameters.  Seeisapproxfor their meaning.\n",
            "- delta=0.0001: step size for numerical gradient calculation.\n",
            "- verbose=1: 0 prints nothing, 1 shows failing tests, 2 shows all tests.\n",
            "Reference:  AutoGrad.@primitive — Macro\n",
            " - Code Block: \n",
            " [ @primitive  fx g1 g2... ]\n",
            "- Define a new primitive operation for AutoGrad and (optionally) specify its gradients. Non-differentiable functions such as sign, and non-numeric functions such as size should be defined using the @zerograd macro instead.\n",
            "- Examples\n",
            " - Code Block: \n",
            " [ @primitive sin(x::Number)\n",
            "@primitive hypot(x1,x2),dy,y\n",
            "\n",
            "@primitive sin(x::Number),dy  (dy.*cos(x))\n",
            "@primitive hypot(x1,x2),dy,y  (dy.*x1./y)  (dy.*x2./y) ]\n",
            "- The first example shows that fx is a typed method declaration.  Julia supports multiple dispatch, i.e. a single function can have multiple methods with different arg types. AutoGrad takes advantage of this and supports multiple dispatch for primitives and gradients.\n",
            "- The second example specifies variable names for the output gradient dy and the output y after the method declaration which can be used in gradient expressions.  Untyped, ellipsis and keyword arguments are ok as in f(a::Int,b,c...;d=1).  Parametric methods such as f(x::T) where {T<:Number} cannot be used.\n",
            "- The method declaration can optionally be followed by gradient expressions.  The third and fourth examples show how gradients can be specified.  Note that the parameters, the return variable and the output gradient of the original function can be used in the gradient expressions.\n",
            "- Under the hood\n",
            "- The @primitive macro turns the first example into:\n",
            " - Code Block: \n",
            " [ sin(x::Value{T}) where {T<:Number} = forw(sin, x) ]\n",
            "- This will cause calls to sin with a boxed argument (Value{T<:Number}) to be recorded. The recorded operations are used by AutoGrad to construct a dynamic computational graph. With multiple arguments things are a bit more complicated.  Here is what happens with the second example:\n",
            " - Code Block: \n",
            " [ hypot(x1::Value{S}, x2::Value{T}) where {S,T} = forw(hypot, x1, x2)\n",
            "hypot(x1::S, x2::Value{T})        where {S,T} = forw(hypot, x1, x2)\n",
            "hypot(x1::Value{S}, x2::T)        where {S,T} = forw(hypot, x1, x2) ]\n",
            "- We want the forw method to be called if any one of the arguments is a boxed Value.  There is no easy way to specify this in Julia, so the macro generates all 2^N-1 boxed/unboxed argument combinations.\n",
            "- In AutoGrad, gradients are defined using gradient methods that have the following pattern:\n",
            " - Code Block: \n",
            " [ back(f,Arg{i},dy,y,x...) => dx[i] ]\n",
            "- For the third example here is the generated gradient method:\n",
            " - Code Block: \n",
            " [ back(::typeof(sin), ::Type{Arg{1}}, dy, y, x::Value{T}) where {T<:Number} = dy .* cos(x) ]\n",
            "- For the last example a different gradient method is generated for each argument:\n",
            " - Code Block: \n",
            " [ back(::typeof(hypot), ::Type{Arg{1}}, dy, y, x1::Value{S}, x2::Value{T}) where {S,T} = (dy .* x1) ./ y\n",
            "back(::typeof(hypot), ::Type{Arg{2}}, dy, y, x1::Value{S}, x2::Value{T}) where {S,T} = (dy .* x2) ./ y ]\n",
            "- In fact @primitive generates four more definitions for the other boxed/unboxed argument combinations.\n",
            "- Broadcasting\n",
            "- Broadcasting is handled by extra forw and back methods. @primitive defines the following  so that broadcasting of a primitive function with a boxed value triggers forw and back.\n",
            " - Code Block: \n",
            " [ broadcasted(::typeof(sin), x::Value{T}) where {T<:Number} = forw(broadcasted,sin,x)\n",
            "back(::typeof(broadcasted), ::Type{Arg{2}}, dy, y, ::typeof(sin), x::Value{T}) where {T<:Number} = dy .* cos(x) ]\n",
            "- If you do not want the broadcasting methods, you can use the @primitive1 macro. If you only want the broadcasting methods use @primitive2. As a motivating example, here is how * is defined for non-scalars:\n",
            " - Code Block: \n",
            " [ @primitive1 *(x1,x2),dy  (dy*x2')  (x1'*dy)\n",
            "@primitive2 *(x1,x2),dy  unbroadcast(x1,dy.*x2)  unbroadcast(x2,x1.*dy) ]\n",
            "- Regular * is matrix multiplication, broadcasted * is elementwise multiplication and the two have different gradients as defined above. unbroadcast(a,b) reduces b to the same shape as a by performing the necessary summations.\n",
            "Reference:  AutoGrad.@zerograd — Macro\n",
            " - Code Block: \n",
            " [ @zerograd f(args...; kwargs...) ]\n",
            "- Define f as an AutoGrad primitive operation with zero gradient.\n",
            "- Example:\n",
            " - Code Block: \n",
            " [ @zerograd  floor(x::Float32) ]\n",
            "- @zerograd allows f to handle boxed Value inputs by unboxing them like a @primitive, but unlike @primitive it does not record its actions or return a boxed Value result. Some functions, like sign(), have zero gradient.  Others, like length() have discrete or constant outputs.  These need to handle Value inputs, but do not need to record anything and can return regular values.  Their output can be treated like a constant in the program. Use the @zerograd macro for those.  Use the @zerograd1 variant if you don't want to define the broadcasting version and @zerograd2 if you only want to define the broadcasting version. Note that kwargs are NOT unboxed.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Per-parameter-optimization-(advanced)\n",
            "Heading: Per-parameter optimization (advanced)\n",
            "URL: #Per-parameter-optimization-(advanced)\n",
            "- The model optimization methods apply the same algorithm with the same configuration to every parameter. If you need finer grained control, you can set the optimization algorithm and configuration of an individual Param by setting its opt field to one of the optimization objects like Adam listed below. The opt field is used as an argument to update! and controls the type of update performed on that parameter. Model optimization methods like sgd will not override the opt field if it is already set, e.g. sgd(model,data) will perform an Adam update for a parameter whose opt field is an Adam object. This also means you can stop and start the training without losing optimization state, the first call will set the opt fields and the subsequent calls will not override them.\n",
            "Reference:  Knet.Train20.update! — Function\n",
            " - Code Block: \n",
            " [ update!(weights::Param, gradients)\n",
            "update!(weights, gradients; lr=0.1, gclip=0)\n",
            "update!(weights, gradients, optimizers) ]\n",
            "- Update the weights using their gradients and the optimization algorithms specified using (1) the opt field of a Param, (2) keyword arguments, (3) the third argument.\n",
            "- weights can be an individual Param, numeric array, or a collection of arrays/Params represented by an iterator or dictionary. gradients should be a matching individual array or collection. In the first form, the optimizer should be specified in weights.opt. In the second form the optimizer defaults to SGD with learning rate lr and gradient clip gclip. In the third form optimizers should be a matching individual optimizer or collection of optimizers.  The weights and possibly gradients and optimizers are modified in-place.\n",
            "- Individual optimization parameters can be one of the following types. The keyword arguments for each constructor and their default values are listed as well.\n",
            "- SGD(;lr=0.1, gclip=0)\n",
            "- Momentum(;lr=0.05, gamma=0.95, gclip=0)\n",
            "- Nesterov(;lr=0.05, gamma=0.95, gclip=0)\n",
            "- Adagrad(;lr=0.05, eps=1e-6, gclip=0)\n",
            "- Rmsprop(;lr=0.01, rho=0.9, eps=1e-6, gclip=0)\n",
            "- Adadelta(;lr=1.0, rho=0.9, eps=1e-6, gclip=0)\n",
            "- Adam(;lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, gclip=0)\n",
            "- Example:\n",
            " - Code Block: \n",
            " [ w = Param(rand(d), Adam())  # a Param with a specified optimizer\n",
            "g = lossgradient0(w)        # gradient g has the same shape as w\n",
            "update!(w, g)               # update w in-place with Adam()\n",
            "\n",
            "w = rand(d)                 # an individual weight array\n",
            "g = lossgradient1(w)        # gradient g has the same shape as w\n",
            "update!(w, g)               # update w in-place with SGD()\n",
            "update!(w, g; lr=0.1)       # update w in-place with SGD(lr=0.1)\n",
            "update!(w, g, SGD(lr=0.1))  # update w in-place with SGD(lr=0.1)\n",
            "\n",
            "w = (rand(d1), rand(d2))    # a tuple of weight arrays\n",
            "g = lossgradient2(w)        # g will also be a tuple\n",
            "p = (Adam(), SGD())         # p has optimizers for each w[i]\n",
            "update!(w, g, p)            # update each w[i] in-place with g[i],p[i]\n",
            "\n",
            "w = Any[rand(d1), rand(d2)] # any iterator can be used\n",
            "g = lossgradient3(w)        # g will be similar to w\n",
            "p = Any[Adam(), SGD()]      # p should be an iterator of same length\n",
            "update!(w, g, p)            # update each w[i] in-place with g[i],p[i]\n",
            "\n",
            "w = Dict(:a => rand(d1), :b => rand(d2)) # dictionaries can be used\n",
            "g = lossgradient4(w)\n",
            "p = Dict(:a => Adam(), :b => SGD())\n",
            "update!(w, g, p) ]\n",
            "Reference:  Knet.Train20.SGD — Type\n",
            " - Code Block: \n",
            " [ SGD(;lr=0.1,gclip=0)\n",
            "update!(w,g,p::SGD)\n",
            "update!(w,g;lr=0.1) ]\n",
            "- Container for parameters of the Stochastic gradient descent (SGD) optimization algorithm used by update!.\n",
            "- SGD is an optimization technique to minimize an objective function by updating its weights in the opposite direction of their gradient. The learning rate (lr) determines the size of the step.  SGD updates the weights with the following formula:\n",
            " - Code Block: \n",
            " [ w = w - lr * g ]\n",
            "- where w is a weight array, g is the gradient of the loss function w.r.t w and lr is the learning rate.\n",
            "- If norm(g) > gclip > 0, g is scaled so that its norm is equal to gclip.  If gclip==0 no scaling takes place.\n",
            "- SGD is used by default if no algorithm is specified in the two argument version of update![@ref].\n",
            "Reference:  Knet.Train20.Momentum — Type\n",
            " - Code Block: \n",
            " [ Momentum(;lr=0.05, gclip=0, gamma=0.95)\n",
            "update!(w,g,p::Momentum) ]\n",
            "- Container for parameters of the Momentum optimization algorithm used by update!.\n",
            "- The Momentum method tries to accelerate SGD by adding a velocity term to the update.  This also decreases the oscillation between successive steps. It updates the weights with the following formulas:\n",
            " - Code Block: \n",
            " [ velocity = gamma * velocity + lr * g\n",
            "w = w - velocity ]\n",
            "- where w is a weight array, g is the gradient of the objective function w.r.t w, lr is the learning rate, gamma is the momentum parameter, velocity is an array with the same size and type of w and holds the accelerated gradients.\n",
            "- If norm(g) > gclip > 0, g is scaled so that its norm is equal to gclip.  If gclip==0 no scaling takes place.\n",
            "- Reference: Qian, N. (1999). On the momentum term in gradient descent learning algorithms.  Neural Networks : The Official Journal of the International Neural Network Society, 12(1), 145–151.\n",
            "Reference:  Knet.Train20.Nesterov — Type\n",
            " - Code Block: \n",
            " [ Nesterov(; lr=0.05, gclip=0, gamma=0.95)\n",
            "update!(w,g,p::Momentum) ]\n",
            "- Container for parameters of Nesterov's momentum optimization algorithm used by update!.\n",
            "- It is similar to standard Momentum but with a slightly different update rule:\n",
            " - Code Block: \n",
            " [ velocity = gamma * velocity_old - lr * g\n",
            "w = w_old - velocity_old + (1+gamma) * velocity ]\n",
            "- where w is a weight array, g is the gradient of the objective function w.r.t w, lr is the learning rate, gamma is the momentum parameter, velocity is an array with the same size and type of w and holds the accelerated gradients.\n",
            "- If norm(g) > gclip > 0, g is scaled so that its norm is equal to gclip.  If gclip == 0 no scaling takes place.\n",
            "- Reference Implementation : Yoshua Bengio, Nicolas Boulanger-Lewandowski and Razvan P ascanu\n",
            "Reference:  Knet.Train20.Adagrad — Type\n",
            " - Code Block: \n",
            " [ Adagrad(;lr=0.05, gclip=0, eps=1e-6)\n",
            "update!(w,g,p::Adagrad) ]\n",
            "- Container for parameters of the Adagrad optimization algorithm used by update!.\n",
            "- Adagrad is one of the methods that adapts the learning rate to each of the weights.  It stores the sum of the squares of the gradients to scale the learning rate.  The learning rate is adapted for each weight by the value of current gradient divided by the accumulated gradients. Hence, the learning rate is greater for the parameters where the accumulated gradients are small and the learning rate is small if the accumulated gradients are large. It updates the weights with the following formulas:\n",
            " - Code Block: \n",
            " [ G = G + g .^ 2\n",
            "w = w - g .* lr ./ sqrt(G + eps) ]\n",
            "- where w is the weight, g is the gradient of the objective function w.r.t w, lr is the learning rate, G is an array with the same size and type of w and holds the sum of the squares of the gradients. eps is a small constant to prevent a zero value in the denominator.\n",
            "- If norm(g) > gclip > 0, g is scaled so that its norm is equal to gclip.  If gclip==0 no scaling takes place.\n",
            "- Reference: Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121–2159.\n",
            "Reference:  Knet.Train20.Rmsprop — Type\n",
            " - Code Block: \n",
            " [ Rmsprop(;lr=0.01, gclip=0, rho=0.9, eps=1e-6)\n",
            "update!(w,g,p::Rmsprop) ]\n",
            "- Container for parameters of the Rmsprop optimization algorithm used by update!.\n",
            "- Rmsprop scales the learning rates by dividing the root mean squared of the gradients. It updates the weights with the following formula:\n",
            " - Code Block: \n",
            " [ G = (1-rho) * g .^ 2 + rho * G\n",
            "w = w - lr * g ./ sqrt(G + eps) ]\n",
            "- where w is the weight, g is the gradient of the objective function w.r.t w, lr is the learning rate, G is an array with the same size and type of w and holds the sum of the squares of the gradients. eps is a small constant to prevent a zero value in the denominator.  rho is the momentum parameter and delta is an array with the same size and type of w and holds the sum of the squared updates.\n",
            "- If norm(g) > gclip > 0, g is scaled so that its norm is equal to gclip.  If gclip==0 no scaling takes place.\n",
            "- Reference: Tijmen Tieleman and Geoffrey Hinton (2012). \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\"  COURSERA: Neural Networks for Machine Learning 4.2.\n",
            "Reference:  Knet.Train20.Adadelta — Type\n",
            " - Code Block: \n",
            " [ Adadelta(;lr=1.0, gclip=0, rho=0.9, eps=1e-6)\n",
            "update!(w,g,p::Adadelta) ]\n",
            "- Container for parameters of the Adadelta optimization algorithm used by update!.\n",
            "- Adadelta is an extension of Adagrad that tries to prevent the decrease of the learning rates to zero as training progresses. It scales the learning rate based on the accumulated gradients like Adagrad and holds the acceleration term like Momentum. It updates the weights with the following formulas:\n",
            " - Code Block: \n",
            " [ G = (1-rho) * g .^ 2 + rho * G\n",
            "update = g .* sqrt(delta + eps) ./ sqrt(G + eps)\n",
            "w = w - lr * update\n",
            "delta = rho * delta + (1-rho) * update .^ 2 ]\n",
            "- where w is the weight, g is the gradient of the objective function w.r.t w, lr is the learning rate, G is an array with the same size and type of w and holds the sum of the squares of the gradients. eps is a small constant to prevent a zero value in the denominator.  rho is the momentum parameter and delta is an array with the same size and type of w and holds the sum of the squared updates.\n",
            "- If norm(g) > gclip > 0, g is scaled so that its norm is equal to gclip.  If gclip==0 no scaling takes place.\n",
            "- Reference: Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method.\n",
            "Reference:  Knet.Train20.Adam — Type\n",
            " - Code Block: \n",
            " [ Adam(;lr=0.001, gclip=0, beta1=0.9, beta2=0.999, eps=1e-8)\n",
            "update!(w,g,p::Adam) ]\n",
            "- Container for parameters of the Adam optimization algorithm used by update!.\n",
            "- Adam is one of the methods that compute the adaptive learning rate. It stores accumulated gradients (first moment) and the sum of the squared of gradients (second).  It scales the first and second moment as a function of time. Here is the update formulas:\n",
            " - Code Block: \n",
            " [ m = beta1 * m + (1 - beta1) * g\n",
            "v = beta2 * v + (1 - beta2) * g .* g\n",
            "mhat = m ./ (1 - beta1 ^ t)\n",
            "vhat = v ./ (1 - beta2 ^ t)\n",
            "w = w - (lr / (sqrt(vhat) + eps)) * mhat ]\n",
            "- where w is the weight, g is the gradient of the objective function w.r.t w, lr is the learning rate, m is an array with the same size and type of w and holds the accumulated gradients. v is an array with the same size and type of w and holds the sum of the squares of the gradients. eps is a small constant to prevent a zero denominator. beta1 and beta2 are the parameters to calculate bias corrected first and second moments. t is the update count.\n",
            "- If norm(g) > gclip > 0, g is scaled so that its norm is equal to gclip.  If gclip==0 no scaling takes place.\n",
            "- Reference: Kingma, D. P., & Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1–13.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Function-Index\n",
            "Heading: Function Index\n",
            "URL: #Function-Index\n",
            "- Knet.AutoGrad\n",
            "- Knet.KnetArrays.KnetArray\n",
            "- Knet.Ops20.RNN\n",
            "- Knet.Train20.Adadelta\n",
            "- Knet.Train20.Adagrad\n",
            "- Knet.Train20.Adam\n",
            "- Knet.Train20.Momentum\n",
            "- Knet.Train20.Nesterov\n",
            "- Knet.Train20.Rmsprop\n",
            "- Knet.Train20.SGD\n",
            "- AutoGrad.cat1d\n",
            "- Knet.KnetArrays.gc\n",
            "- Knet.KnetArrays.seed!\n",
            "- Knet.Ops20.accuracy\n",
            "- Knet.Ops20.batchnorm\n",
            "- Knet.Ops20.bce\n",
            "- Knet.Ops20.bmm\n",
            "- Knet.Ops20.bnmoments\n",
            "- Knet.Ops20.bnparams\n",
            "- Knet.Ops20.conv4\n",
            "- Knet.Ops20.deconv4\n",
            "- Knet.Ops20.dropout\n",
            "- Knet.Ops20.elu\n",
            "- Knet.Ops20.logistic\n",
            "- Knet.Ops20.logp\n",
            "- Knet.Ops20.logsoftmax\n",
            "- Knet.Ops20.logsumexp\n",
            "- Knet.Ops20.mat\n",
            "- Knet.Ops20.nll\n",
            "- Knet.Ops20.pool\n",
            "- Knet.Ops20.relu\n",
            "- Knet.Ops20.rnnparam\n",
            "- Knet.Ops20.rnnparams\n",
            "- Knet.Ops20.selu\n",
            "- Knet.Ops20.sigm\n",
            "- Knet.Ops20.softmax\n",
            "- Knet.Ops20.unpool\n",
            "- Knet.Ops20.zeroone\n",
            "- Knet.Train20.bilinear\n",
            "- Knet.Train20.converge\n",
            "- Knet.Train20.gaussian\n",
            "- Knet.Train20.goldensection\n",
            "- Knet.Train20.hyperband\n",
            "- Knet.Train20.minibatch\n",
            "- Knet.Train20.minimize\n",
            "- Knet.Train20.param\n",
            "- Knet.Train20.progress\n",
            "- Knet.Train20.update!\n",
            "- Knet.Train20.xavier\n",
            "- Knet.Train20.xavier_normal\n",
            "- Knet.Train20.xavier_uniform\n",
            "- Knet.dir\n",
            "- AutoGrad.@gcheck\n",
            "- AutoGrad.@primitive\n",
            "- AutoGrad.@zerograd\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[<h2 id=\"Installation\"><a class=\"docs-heading-anchor\" href=\"#Installation\">Installation</a><a id=\"Installation-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Installation\" title=\"Permalink\"></a></h2>, <h2 id=\"Tips-for-developers\"><a class=\"docs-heading-anchor\" href=\"#Tips-for-developers\">Tips for developers</a><a id=\"Tips-for-developers-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Tips-for-developers\" title=\"Permalink\"></a></h2>, <h2 id=\"Using-Amazon-AWS\"><a class=\"docs-heading-anchor\" href=\"#Using-Amazon-AWS\">Using Amazon AWS</a><a id=\"Using-Amazon-AWS-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Using-Amazon-AWS\" title=\"Permalink\"></a></h2>, <h2 id=\"Using-Microsoft-Azure\"><a class=\"docs-heading-anchor\" href=\"#Using-Microsoft-Azure\">Using Microsoft Azure</a><a id=\"Using-Microsoft-Azure-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Using-Microsoft-Azure\" title=\"Permalink\"></a></h2>, <h2 id=\"Using-Ubuntu18.04\"><a class=\"docs-heading-anchor\" href=\"#Using-Ubuntu18.04\">Using Ubuntu18.04</a><a id=\"Using-Ubuntu18.04-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Using-Ubuntu18.04\" title=\"Permalink\"></a></h2>]\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Installation\n",
            "Heading: Installation\n",
            "URL: #Installation\n",
            "- For best results install (1) Julia, (2) CUDA.jl, (3) Knet.jl in that order. Step (2) can be skipped if you do not need GPU support. An optional step (4) below describes how to interact with the Knet tutorial notebooks.\n",
            "- Julia:Download and install the latest version of Julia fromjulialang.org. As of this writing the latest version is 1.4.2 and I have tested Knet using 64-bit binaries for Generic Linux on x86, macOS, and Windows.\n",
            "- CUDA.jl:If you are going to use an NVIDIA GPU, start Julia and install CUDA.jl withusing Pkg; Pkg.add(\"CUDA\"). The Julia CUDA stack requires users to have a functional NVIDIA driver, seeCUDA.jl installation instructionsfor detailed requirements. You can test your installation withusing CUDA; CUDA.functional(), if CUDA is not functional, Knet will not be able to use the GPU.\n",
            "- Knet:to install Knet start Julia and runusing Pkg; Pkg.add(\"Knet\"). If you have problems with the installation, you can get support fromknet-users.\n",
            "- Tutorial:The best way to learn Knet is through the includedJupyter notebooks. You need the IJulia package to run the notebooks which can be installed with:using Pkg; Pkg.add(\"IJulia\"). You can then interact with the tutorial notebooks with:using IJulia, Knet; notebook(dir=Knet.dir(\"tutorial\")). This should open a browser with a list of tutorial notebooks. If you have not used Jupyter before, please take a look at Jupyter notebook tutorials online. Note that the first timenotebook()is run, there may be a long startup time for installations.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Tips-for-developers\n",
            "Heading: Tips for developers\n",
            "URL: #Tips-for-developers\n",
            "- Knet is an open-source project and we are always open to new contributions: bug fixes, new machine learning models and operators, inspiring examples, benchmarking results are all welcome. If you'd like to contribute to the code base, please sign up at the knet-dev mailing list and follow these tips:\n",
            "- Please get an account atgithub.com.\n",
            "- ForktheKnet   repository.\n",
            "- Point Julia to your fork withusing Pkg; pkg\"dev git@github.com:your-username/Knet.jl.git\".\n",
            "- Make sure yourfork is   up-to-date.\n",
            "- Retrieve the latest version of the master branch usinggit pullin the Knet directory.\n",
            "- Implement your contribution.  This typically involves:Creating a git branch.Writing your code.Adding documentation under doc/src and a summary in NEWS.md.Adding unit tests in the test directory and usingPkg.test(\"Knet\").\n",
            "- Creating a git branch.\n",
            "- Writing your code.\n",
            "- Adding documentation under doc/src and a summary in NEWS.md.\n",
            "- Adding unit tests in the test directory and usingPkg.test(\"Knet\").\n",
            "- Please submit your contribution using apull   request.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Using-Amazon-AWS\n",
            "Heading: Using Amazon AWS\n",
            "URL: #Using-Amazon-AWS\n",
            "- If you don't have access to a GPU machine, but would like to experiment with one, Amazon Web Services is a possible solution. I have prepared a machine image (AMI) with everything you need to run Knet. Here are step by step instructions for launching a GPU instance with a Knet image (the screens may have changed slightly since this writing):\n",
            "- 1. First, you need to sign up and create an account following the instructions on Setting Up with Amazon EC2. Once you have an account, open the Amazon EC2 console and login. You should see the following screen:\n",
            "- \n",
            "- 2. Make sure you select the \"Ohio\" region in the upper right corner, then click on AMIs on the lower left menu. At the search box, choose \"Public images\" and search for \"Knet\". Click on the latest Knet image (Knet-1.0.0 as of this writing). You should see the following screen with information about the Knet AMI. Click on the \"Launch\" button on the upper left.\n",
            "- \n",
            "- Note: Instead of \"Launch\", you may want to experiment with \"Spot Request\" under \"Actions\" to get a lower price. You may also qualify for an educational grant if you are a student or researcher.\n",
            "- 3. You should see the \"Step 2: Choose an Instance Type\" page. Pick one of the GPU instances (I have tested with the g2 series and the p2 series). Click on \"Review and Launch\".\n",
            "- \n",
            "- 4. This should take you to the \"Step 7: Review Instance Launch\" page. You can just click \"Launch\" here:\n",
            "- \n",
            "- 5. You should see the \"key pair\" pop up menu. In order to login to your instance, you need an ssh key pair. If you have created a pair during the initial setup you can use it with \"Choose an existing key pair\". Otherwise pick \"Create a new key pair\" from the pull down menu, enter a name for it, and click \"Download Key Pair\". Make sure you keep the downloaded file, we will use it to login. After making sure you have the key file (it has a .pem extension), click \"Launch Instances\" on the lower right.\n",
            "- \n",
            "- 6. We have completed the request. You should see the \"Launch Status\" page. Click on your instance id under \"Your instances are launching\":\n",
            "- \n",
            "- 7. You should be taken to the \"Instances\" screen and see the address of your instance where it says something like \"Public DNS: ec2-54-153-5-184.us-west-1.compute.amazonaws.com\".\n",
            "- \n",
            "- 8.  Open up a terminal (or Putty if you are on Windows) and type:\n",
            " - Code Block: \n",
            " [     ssh -i knetkey.pem ec2-user@ec2-54-153-5-184.us-west-1.compute.amazonaws.com ]\n",
            "- Replacing knetkey.pem with the path to your key file and ec2-54-153-5-184 with the address of your machine. If all goes well you should get a shell prompt on your machine instance.\n",
            "- 9. There you can type julia, and at the julia prompt using Pkg, Pkg.update() and Pkg.build(\"Knet\") to get the latest versions of the packages, as the versions in the AMI may be out of date:\n",
            " - Code Block: \n",
            " [ [ec2-user@ip-172-31-24-60 deps]$ julia\n",
            "               _\n",
            "   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n",
            "  (_)     | (_) (_)    |\n",
            "   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n",
            "  | | | | | | |/ _` |  |\n",
            "  | | |_| | | | (_| |  |  Version 1.0.0 (2018-08-08)\n",
            " _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n",
            "|__/                   |\n",
            "\n",
            "julia> using Pkg\n",
            "julia> Pkg.update()\n",
            "julia> Pkg.build(\"Knet\") ]\n",
            "- Finally you can run Pkg.test(\"Knet\") to make sure all is good. This should take about 10-15 minutes. If all tests pass, you are ready to work with Knet:\n",
            " - Code Block: \n",
            " [ julia> Pkg.test(\"Knet\")\n",
            "INFO: Testing Knet\n",
            "...\n",
            "INFO: Knet tests passed\n",
            "\n",
            "julia> ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Using-Microsoft-Azure\n",
            "Heading: Using Microsoft Azure\n",
            "URL: #Using-Microsoft-Azure\n",
            "- Knet can be used with Azure. For GPU support, you need to create a virtual machine with GPU, for instance Standard_NC6 with Ubuntu18.04 as operating system. Then follow Using Ubuntu18.04.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Using-Ubuntu18.04\n",
            "Heading: Using Ubuntu18.04\n",
            "URL: #Using-Ubuntu18.04\n",
            "- The CUDA stack can be installed using the following instructions:\n",
            " - Code Block: \n",
            " [ ################################################################################\n",
            "##### Prerequisites\n",
            "################################################################################\n",
            "\n",
            "sudo apt install make gcc g++ wget\n",
            "\n",
            "################################################################################\n",
            "##### driver\n",
            "################################################################################\n",
            "# The appropriate driver version can be selected here:\n",
            "# http://www.nvidia.com/Download/index.aspx\n",
            "# The following code is for Azure Standard_NC6 machines (K80 GPU)\n",
            "\n",
            "wget http://us.download.nvidia.com/tesla/440.64.00/NVIDIA-Linux-x86_64-440.64.00.run\n",
            "sudo sh NVIDIA-Linux-x86_64-440.64.00.run\n",
            "\n",
            "################################################################################\n",
            "##### toolkit\n",
            "################################################################################\n",
            "# the appropriate toolkit version can be selected here:\n",
            "# https://developer.nvidia.com/cuda-downloads\n",
            "# The following code is for Azure Standard_NC6 machines (K80 GPU)\n",
            "\n",
            "wget http://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux.run\n",
            "sudo sh cuda_10.2.89_440.33.01_linux.run\n",
            "\n",
            "# add the following two lines to ~/.bashrc\n",
            "PATH=$PATH:/usr/local/cuda-10.2/bin\n",
            "LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.2/lib64\n",
            "\n",
            "sudo reboot now\n",
            "\n",
            "################################################################################\n",
            "##### cudnnn\n",
            "################################################################################\n",
            "\n",
            "# download cudnn using the browser\n",
            "# https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html\n",
            "# The following code is for Azure Standard_NC6 machines (K80 GPU)\n",
            "\n",
            "tar -xzvf cudnn-10.2-linux-x64-v7.6.5.32.tgzsudo cp cuda/include/cudnn.h /usr/local/cuda/include\n",
            "sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\n",
            "sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* ]\n",
            "- Afterwards Knet can be installed as usual:\n",
            " - Code Block: \n",
            " [ julia> using Pkg\n",
            "julia> Pkg.update()\n",
            "julia> Pkg.add(\"Knet\")\n",
            "julia> Pkg.build(\"Knet\")\n",
            "julia> using Knet; include(Knet.dir(\"test/gpu.jl\")) ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[<h2 id=\"Summary\"><a class=\"docs-heading-anchor\" href=\"#Summary\">Summary</a><a id=\"Summary-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Summary\" title=\"Permalink\"></a></h2>, <h2 id=\"Philosophy\"><a class=\"docs-heading-anchor\" href=\"#Philosophy\">Philosophy</a><a id=\"Philosophy-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Philosophy\" title=\"Permalink\"></a></h2>, <h2 id=\"Tutorial\"><a class=\"docs-heading-anchor\" href=\"#Tutorial\">Tutorial</a><a id=\"Tutorial-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Tutorial\" title=\"Permalink\"></a></h2>, <h2 id=\"Benchmarks\"><a class=\"docs-heading-anchor\" href=\"#Benchmarks\">Benchmarks</a><a id=\"Benchmarks-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Benchmarks\" title=\"Permalink\"></a></h2>, <h2 id=\"Under-the-hood\"><a class=\"docs-heading-anchor\" href=\"#Under-the-hood\">Under the hood</a><a id=\"Under-the-hood-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Under-the-hood\" title=\"Permalink\"></a></h2>]\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Summary\n",
            "Heading: Summary\n",
            "URL: #Summary\n",
            "- Knet (pronounced \"kay-net\") is the Koç University deep learning framework implemented in Julia by Deniz Yuret and collaborators.  It supports GPU operation and automatic differentiation using dynamic computational graphs for models defined in plain Julia. You can install Knet with the following at the julia prompt: using Pkg; Pkg.add(\"Knet\"). Some useful links:\n",
            "- Tutorial:introduces Julia and Knet via examples.\n",
            "- Documentation:installation, introduction, design, implementation, full reference and deep learning chapters.\n",
            "- Examples:more tutorials and example models.\n",
            "- Benchmarks:comparison of Knet's speed with TensorFlow, PyTorch, DyNet etc.\n",
            "- Paper:Yuret, D. \"Knet: beginning deep learning with 100 lines of julia.\" InMachine Learning Systems Workshopat NIPS 2016.\n",
            "- KnetML:github organization with Knet repos of models, tutorials, layer collections and other resources.\n",
            "- Images:Knet machine images are available forAWS,SingularityandDocker.\n",
            "- Issues:if you find a bug, please open a github issue.\n",
            "- knet-users:if you need help or would like to request a feature, please join this mailing list.\n",
            "- knet-dev:if you would like to contribute to Knet development, please join this mailing list and check out thesetips.\n",
            "- knet-slack:Slack channel for Knet.\n",
            "- Related work: Please check outFlux,Mocha,JuliaML,JuliaDiff,JuliaGPU,JuliaOptfor related packages.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Philosophy\n",
            "Heading: Philosophy\n",
            "URL: #Philosophy\n",
            "- Knet uses dynamic computational graphs generated at runtime for automatic differentiation of (almost) any Julia code.  This allows machine learning models to be implemented by defining just the forward calculation (i.e. the computation from parameters and data to loss) using the full power and expressivity of Julia. The implementation can use helper functions, loops, conditionals, recursion, closures, tuples and dictionaries, array indexing, concatenation and other high level language features, some of which are often missing in the restricted modeling languages of static computational graph systems like Theano, Torch, Caffe and Tensorflow.  GPU operation is supported by simply using the KnetArray type instead of regular Array for parameters and data.\n",
            "- Knet builds a dynamic computational graph by recording primitive operations during forward calculation.  Only pointers to inputs and outputs are recorded for efficiency.  Therefore array overwriting is not supported during forward and backward passes.  This encourages a clean functional programming style.  High performance is achieved using custom memory management and efficient GPU kernels.  See Under the hood for more details.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Tutorial\n",
            "Heading: Tutorial\n",
            "URL: #Tutorial\n",
            "- The Knet tutorial consists of Jupyter notebooks that introduce the programming language Julia and the Knet deep learning framework. By the end, the reader should be able to define, train, evaluate, and visualize basic MLP, CNN, and RNN models.  Each notebook is written to work stand-alone but they rely on concepts introduced in earlier notebooks, so I recommend reading them in order. Every Knet function outside of the standard Julia library is defined or explained before use. You can view the notebooks using the following links, or interact with them using a Jupyter server. Instructions for running a server locally or in the cloud can be found in the tutorial README.\n",
            "- Julia is fast:comparison of Julia's speed to C, Python and numpy.\n",
            "- Getting to know Julia:basic Julia tutorial fromJuliaBox.\n",
            "- Quick start:if you are familiar with other deep learning frameworks and want to see a quick Julia example.\n",
            "- The MNIST dataset:introduction to the MNIST handwritten digit recognition dataset.\n",
            "- Julia iterators:iterators are useful for generating and training with data.\n",
            "- Creating a model:define, train, visualize simple linear models, introduce gradients, SGD, using the GPU.\n",
            "- Multilayer perceptrons:multi layer perceptrons, nonlinearities, model capacity, overfitting, regularization, dropout.\n",
            "- Convolutional networks:convolutional neural networks, sparse and shared weights using conv4 and pool operations.\n",
            "- Recurrent networks:introduction to recurrent neural networks.\n",
            "- IMDB sentiment analysis:a simple RNN sequence classification model for sentiment analysis of IMDB movie reviews.\n",
            "- Language modeling:a character based RNN language model that can write Shakespeare sonnets and Julia programs.\n",
            "- Sequence to sequence:a sequence to sequence RNN model typically used for machine translation.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Benchmarks\n",
            "Heading: Benchmarks\n",
            "URL: #Benchmarks\n",
            "- Each of the examples above was used as a benchmark to compare Knet with other frameworks. The table below shows the number of seconds it takes to train a given model for a particular dataset, number of epochs and minibatch size for Knet, Theano, Torch, Caffe and TensorFlow. Knet had comparable performance to other commonly used frameworks.\n",
            "- The benchmarking was done on g2.2xlarge GPU instances on Amazon AWS. The code is available at github and as machine image deep_AMI_v6 at AWS N.California. See the section on Using Amazon AWS for more information. The datasets are available online using the following links: Housing, MNIST, Hiawatha. The MLP uses a single hidden layer of 64 units. CharLM uses a single layer LSTM language model with embedding and hidden layer sizes set to 256 and trained using BPTT with a sequence length of 100. Each dataset was minibatched and transferred to GPU prior to benchmarking when possible.\n",
            "- We implemented dynamic neural network examples from the dynet-benchmark repo to compare Knet with DyNet and Chainer. See DyNet technical report for the architectural details of the implemented examples and the github repo for the source code.\n",
            "- rnnlm-batch: A recurrent neural network language model onPTBcorpus.\n",
            "- bilstm-tagger: A bidirectional LSTM network that predicts a tag for each word. It is trained onWikiNERdataset.\n",
            "- bilstm-tagger-withchar: Similar to bilstm-tagger, but uses characer-based embeddings for unknown words.\n",
            "- treenn: A tree-structured LSTM sentiment classifier trained onStanford Sentiment Treebankdataset.\n",
            "- Benchmarks were run on a server with Intel(R) Xeon(R) CPU E5-2695 v4 @ 2.10GHz and Tesla K80.\n",
            "- More recently, @ilkarman has published CNN and RNN benchmarks on Nvidia K80 GPUs, using the Microsoft Azure Data Science Virtual Machine for Linux (Ubuntu). The results are copied below.  You can find versions of the Knet notebooks used for these benchmarks in the Knet/examples/DeepLearningFrameworks directory.\n",
            "- Training CNN (VGG-style) on CIFAR-10 - Image Recognition\n",
            "- Training RNN (GRU) on IMDB - Natural Language Processing (Sentiment Analysis)\n",
            "- Inference ResNet-50 (Feature Extraction)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Under-the-hood\n",
            "Heading: Under the hood\n",
            "URL: #Under-the-hood\n",
            "- Knet relies on the AutoGrad package and the KnetArray data type for its functionality and performance. AutoGrad computes the gradient of Julia functions and KnetArray implements high performance GPU arrays with custom memory management. This section briefly describes them.\n",
            "- GPUs have become indispensable for training large deep learning models.  Even the small examples implemented here run up to 17x faster on the GPU compared to the 8 core CPU architecture we use for benchmarking. However GPU implementations have a few potential pitfalls: (i) GPU memory allocation is slow, (ii) GPU-RAM memory transfer is slow, (iii) reduction operations (like sum) can be very slow unless implemented properly (See Optimizing Parallel Reduction in CUDA).\n",
            "- Knet implements KnetArray as a Julia data type that wraps GPU array pointers. KnetArray is based on the more standard CudaArray with a few important differences: (i) KnetArrays have a custom memory manager, similar to ArrayFire, which reuse pointers garbage collected by Julia to reduce the number of GPU memory allocations, (ii) contiguous array ranges (e.g. a[:,3:5]) are handled as views with shared pointers instead of copies when possible, and (iii) a number of custom CUDA kernels written for KnetArrays implement element-wise, broadcasting, and scalar and vector reduction operations efficiently. As a result Knet allows users to implement their models using high-level code, yet be competitive in performance with other frameworks as demonstrated in the benchmarks section. Other GPU related Julia packages can be found in JuliaGPU.\n",
            "- As we have seen, many common machine learning models can be expressed as differentiable programs that input parameters and data and output a scalar loss value. The loss value measures how close the model predictions are to desired values with the given parameters. Training a model can then be seen as an optimization problem: find the parameters that minimize the loss. Typically, a gradient based optimization algorithm is used for computational efficiency: the direction in the parameter space in which the loss reduction is maximum is given by the negative gradient of the loss with respect to the parameters. Thus gradient computations take a central stage in software frameworks for machine learning. In this section I will briefly outline existing gradient computation techniques and motivate the particular approach taken by Knet.\n",
            "- Computation of gradients in computer models is performed by four main methods (Baydin et al. 2015):\n",
            "- manual differentiation (programming the derivatives)\n",
            "- numerical differentiation (using finite difference approximations)\n",
            "- symbolic differentiation (using expression manipulation)\n",
            "- automatic differentiation (detailed below)\n",
            "- Manually taking derivatives and coding the result is labor intensive, error-prone, and all but impossible with complex deep learning models.  Numerical differentiation is simple: $f'(x)=(f(x+\\epsilon)-f(x-\\epsilon))/(2\\epsilon)$ but impractical: the finite difference equation needs to be evaluated for each individual parameter, of which there are typically many. Pure symbolic differentiation using expression manipulation, as implemented in software such as Maxima, Maple, and Mathematica is impractical for different reasons: (i) it may not be feasible to express a machine learning model as a closed form mathematical expression, and (ii) the symbolic derivative can be exponentially larger than the model itself leading to inefficient run-time calculation. This leaves us with automatic differentiation.\n",
            "- Automatic differentiation is the idea of using symbolic derivatives only at the level of elementary operations, and computing the gradient of a compound function by applying the chain rule to intermediate numerical results. For example, pure symbolic differentiation of $\\sin^2(x)$ could give us $2\\sin(x)\\cos(x)$ directly. Automatic differentiation would use the intermediate numerical values $x_1=\\sin(x)$, $x_2=x_1^2$ and the elementary derivatives $dx_2/dx_1=2x_1$, $dx_1/dx=\\cos(x)$ to compute the same answer without ever building a full gradient expression.\n",
            "- To implement automatic differentiation the target function needs to be decomposed into its elementary operations, a process similar to compilation. Most older machine learning frameworks (such as Theano, Torch, Caffe, Tensorflow and older versions of Knet prior to v0.8) compile models expressed in a restricted mini-language into a static computational graph of elementary operations that have pre-defined derivatives. There are two drawbacks with this approach: (i) the restricted mini-languages tend to have limited support for high-level language features such as conditionals, loops, helper functions, array indexing, etc. (e.g. the infamous scan operation in Theano) (ii) the sequence of elementary operations that unfold at run-time needs to be known in advance, and they are difficult to handle when the sequence is data dependent.\n",
            "- There is an alternative: high-level languages, like Julia and Python, already know how to decompose functions into their elementary operations. If we let the users define their models directly in a high-level language, then record the elementary operations during loss calculation at run-time, a dynamic computational graph can be constructed from the recorded operations. The cost of recording is not prohibitive: The table below gives cumulative times for elementary operations of an MLP with quadratic loss. Recording only adds 15% to the raw cost of the forward computation. Backpropagation roughly doubles the total time as expected.\n",
            "- This is the approach taken by the popular autograd Python package and its Julia port AutoGrad.jl used by Knet. Recently, other machine learning frameworks have been adapting dynamic computational graphs: Chainer, DyNet, PyTorch, TensorFlow Fold. Related Julia projects include Flux and JuliaDiff.\n",
            "- In AutoGrad, parameters of interest are boxed by the Param type. y = @diff f(x) returns a struct such that value(y) gives f(x) (which should be a scalar), params(y) gives the list of parameters that took place in the computation of f(x), and grad(y,p) gives the gradient of f(x) with respect to parameter p.  In a @diff context, the elementary operations in f are overloaded to record their actions and output boxed answers when their inputs are boxed. The sequence of recorded operations is then used to compute gradients. Derivatives can be defined independently for each method of a function (determined by argument types) making full use of Julia's multiple dispatch. New elementary operations and derivatives can be defined concisely using Julia's macro and meta-programming facilities. See AutoGrad.jl for details.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[<h2 id=\"Supervised-learning\"><a class=\"docs-heading-anchor\" href=\"#Supervised-learning\">Supervised learning</a><a id=\"Supervised-learning-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Supervised-learning\" title=\"Permalink\"></a></h2>, <h2 id=\"Partial-derivatives\"><a class=\"docs-heading-anchor\" href=\"#Partial-derivatives\">Partial derivatives</a><a id=\"Partial-derivatives-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Partial-derivatives\" title=\"Permalink\"></a></h2>, <h2 id=\"Chain-rule-and-backpropagation\"><a class=\"docs-heading-anchor\" href=\"#Chain-rule-and-backpropagation\">Chain rule and backpropagation</a><a id=\"Chain-rule-and-backpropagation-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Chain-rule-and-backpropagation\" title=\"Permalink\"></a></h2>, <h2 id=\"Multiple-dimensions\"><a class=\"docs-heading-anchor\" href=\"#Multiple-dimensions\">Multiple dimensions</a><a id=\"Multiple-dimensions-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Multiple-dimensions\" title=\"Permalink\"></a></h2>, <h2 id=\"Multiple-instances\"><a class=\"docs-heading-anchor\" href=\"#Multiple-instances\">Multiple instances</a><a id=\"Multiple-instances-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Multiple-instances\" title=\"Permalink\"></a></h2>, <h2 id=\"Stochastic-Gradient-Descent\"><a class=\"docs-heading-anchor\" href=\"#Stochastic-Gradient-Descent\">Stochastic Gradient Descent</a><a id=\"Stochastic-Gradient-Descent-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Stochastic-Gradient-Descent\" title=\"Permalink\"></a></h2>, <h2 id=\"Housing-Example\"><a class=\"docs-heading-anchor\" href=\"#Housing-Example\">Housing Example</a><a id=\"Housing-Example-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Housing-Example\" title=\"Permalink\"></a></h2>, <h2 id=\"Problems-with-SGD\"><a class=\"docs-heading-anchor\" href=\"#Problems-with-SGD\">Problems with SGD</a><a id=\"Problems-with-SGD-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Problems-with-SGD\" title=\"Permalink\"></a></h2>, <h2 id=\"References\"><a class=\"docs-heading-anchor\" href=\"#References\">References</a><a id=\"References-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#References\" title=\"Permalink\"></a></h2>, <h2 id=\"Notes\"><a class=\"docs-heading-anchor\" href=\"#Notes\">Notes</a><a id=\"Notes-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Notes\" title=\"Permalink\"></a></h2>]\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Supervised-learning\n",
            "Heading: Supervised learning\n",
            "URL: #Supervised-learning\n",
            "- Arthur Samuel, the author of the first self-learning checkers program, defined machine learning as a \"field of study that gives computers the ability to learn without being explicitly programmed\". This leaves the definition of learning a bit circular. Tom M. Mitchell provided a more formal definition: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E,\" where the task, the experience, and the performance measure are to be specified based on the problem.\n",
            "- We will start with supervised learning, where the task is to predict the output of an unknown process given its input, the experience consists of training data, a set of example input-output pairs, and the performance measure is given by a loss function which tells us how far the predictions are from actual outputs.  \n",
            "- We model the unknown process using a prediction function, a parametric function that predicts the output of the process given its input.  Here is an example:\n",
            "- \\[\\hat{y} = W x + b\\]\n",
            "- Here $x$ is the model input, $\\hat{y}$ is the model output, $W$ is a matrix of weights, and $b$ is a vector of biases. By adjusting the parameters of this model, i.e. the weights and the biases, we can make it compute any linear function of $x$.\n",
            "- \"All models are wrong, but some models are useful.\" George Box famously said. We do not necessarily know that the system whose output we are trying to predict is governed by a linear relationship. All we know is a finite number of input-output examples in the training data:\n",
            "- \\[\\mathcal{D}=\\{(x_1,y_1),\\ldots,(x_N,y_N)\\}\\]\n",
            "- It is just that we have to start model building somewhere and the set of all linear functions is a good place to start for now.\n",
            "- A commonly used loss function in problems with numeric outputs is the squared error, i.e. the average squared difference between the actual output values and the ones predicted by the model. So our goal is to find model parameters that minimize the squared error:\n",
            "- \\[\\arg\\min_{W,b} \\frac{1}{N} \\sum_{n=1}^N \\| \\hat{y}_n - y_n \\|^2\\]\n",
            "- Where $\\hat{y}_n = W x_n + b$ denotes the output predicted by the model for the $n$ th example.\n",
            "- There are several methods to find the solution to the problem of minimizing squared error. Here we will present the stochastic gradient descent (SGD) method because it generalizes well to more complex models. In SGD, we take the training examples (individually or in groups), compute the gradient of the error for the current example(s) with respect to the parameters using the backpropagation algorithm, and move the parameters a small step in the direction that will decrease the error. First some notes on the math.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Partial-derivatives\n",
            "Heading: Partial derivatives\n",
            "URL: #Partial-derivatives\n",
            "- When we have a function with a scalar output, we can look at how its value changes in response to a small change in one of its inputs or parameters, holding the rest fixed. This is called a partial derivative. Let us consider the squared error for the $n$ th input as an example:\n",
            "- \\[J = \\| W x_n + b - y_n \\|^2\\]\n",
            "- So the partial derivative $\\partial J / \\partial w_{ij}$ would tell us how many units $J$ would move if we moved $w_{ij}$ in $W$ one unit (at least for small enough units). Here is a more graphical representation:\n",
            "- In this figure, it is easier to see that the machinery that generates $J$ has many \"inputs\". In particular we can talk about how $J$ is effected by changing parameters $W$ and $b$, as well as changing the input $x$, the model output $\\hat{y}$, the desired output $y$, or intermediate values like $z$ or $r$. So partial derivatives like $\\partial J / \\partial x_i$ or $\\partial J / \\partial \\hat{y}_j$ are fair game and tell us how $J$ would react in response to small changes in those quantities.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Chain-rule-and-backpropagation\n",
            "Heading: Chain rule and backpropagation\n",
            "URL: #Chain-rule-and-backpropagation\n",
            "- The chain rule allows us to calculate partial derivatives in terms of other partial derivatives, simplifying the overall computation. We will go over it in some detail as it forms the basis of the backpropagation algorithm. For now let us assume that each of the variables in the above example are scalars. We will start by looking at the effect of $r$ on $J$ and move backward from there. Basic calculus tells us that:\n",
            "- \\[\\begin{aligned}\n",
            "J &= r^2 \\\\\n",
            "{\\partial J}/{\\partial r} &= 2r\n",
            "\\end{aligned}\\]\n",
            "- Thus, if $r=5$ and we decrease $r$ by a small $\\epsilon$, the squared error $J$ will go down by $10\\epsilon$. Now let's move back a step and look at $\\hat{y}$:\n",
            "- \\[\\begin{aligned}\n",
            "r &= \\hat{y} - y \\\\\n",
            "{\\partial r}/{\\partial \\hat{y}} &= 1\n",
            "\\end{aligned}\\]\n",
            "- So how much effect will a small $\\epsilon$ decrease in $\\hat{y}$ have on $J$ when $r=5$? Well, when $\\hat{y}$ goes down by $\\epsilon$, so will $r$, which means $J$ will go down by $10\\epsilon$ again. The chain rule expresses this idea:\n",
            "- \\[\\frac{\\partial J}{\\partial\\hat{y}} = \n",
            "\\frac{\\partial J}{\\partial r}\n",
            "\\frac{\\partial r}{\\partial\\hat{y}}\n",
            "= 2r\\]\n",
            "- Going back further, we have:\n",
            "- \\[\\begin{aligned}\n",
            "\\hat{y} &= z + b \\\\\n",
            "{\\partial \\hat{y}}/{\\partial b} &= 1 \\\\\n",
            "{\\partial \\hat{y}}/{\\partial z} &= 1\n",
            "\\end{aligned}\\]\n",
            "- Which means $b$ and $z$ have the same effect on $J$ as $\\hat{y}$ and $r$, i.e. decreasing them by $\\epsilon$ will decrease $J$ by $2r\\epsilon$ as well. Finally:\n",
            "- \\[\\begin{aligned}\n",
            "z &= w x \\\\\n",
            "{\\partial z}/{\\partial x} &= w \\\\\n",
            "{\\partial z}/{\\partial w} &= x\n",
            "\\end{aligned}\\]\n",
            "- This allows us to compute the effect of $w$ on $J$ in several steps: moving $w$ by $\\epsilon$ will move $z$ by $x\\epsilon$, $\\hat{y}$ and $r$ will move exactly the same amount because their partials with $z$ are 1, and finally since $r$ moves by $x\\epsilon$, $J$ will move by $2rx\\epsilon$.\n",
            "- \\[\\frac{\\partial J}{\\partial w} =\n",
            "\\frac{\\partial J}{\\partial r}\n",
            "\\frac{\\partial r}{\\partial \\hat{y}}\n",
            "\\frac{\\partial \\hat{y}}{\\partial z}\n",
            "\\frac{\\partial z}{\\partial w}\n",
            "= 2rx\\]\n",
            "- We can represent this process of computing partial derivatives as follows:\n",
            "- Note that we have the same number of boxes and operations, but all the arrows are reversed. Let us call this the backward pass, and the original computation in the previous picture the forward pass. Each box in this backward-pass picture represents the partial derivative for the corresponding box in the previous forward-pass picture. Most importantly, each computation is local: each operation takes the partial derivative of its output, and multiplies it with a factor that only depends on the original input/output values to compute the partial derivative of its input(s). In fact we can implement the forward and backward passes for the linear regression model using the following local operations:\n",
            "- This is basically the backpropagation algorithm in a nutshell, i.e.  backpropagation can be viewed as the application of Leibniz' chain rule from 1660s to machine learning in 1980s.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Multiple-dimensions\n",
            "Heading: Multiple dimensions\n",
            "URL: #Multiple-dimensions\n",
            "- Let's look at the case where the input and output are not scalars but vectors. In particular assume that $x \\in \\mathbb{R}^D$ and $y \\in \\mathbb{R}^C$. This makes $W \\in \\mathbb{R}^{C\\times D}$ a matrix and $z,b,\\hat{y},r$ vectors in $\\mathbb{R}^C$. During the forward pass, $z=Wx$ operation is now a matrix-vector product, the additions and subtractions are elementwise operations. The squared error $J=\\|r\\|^2=\\sum r_i^2$ is still a scalar. For the backward pass we ask how much each element of these vectors or matrices effect $J$. Starting with $r$:\n",
            "- \\[\\begin{aligned}\n",
            "J &= \\sum r_i^2 \\\\\n",
            "{\\partial J}/{\\partial r_i} &= 2r_i\n",
            "\\end{aligned}\\]\n",
            "- We see that when $r$ is a vector, the partial derivative of each component is equal to twice that component. If we put these partial derivatives together in a vector, we obtain a gradient vector:\n",
            "- \\[\\nabla_r J\n",
            "\\equiv \\langle \\frac{\\partial J}{\\partial r_1}, \\cdots, \\frac{\\partial J}{\\partial r_C} \\rangle\n",
            "= \\langle 2 r_1, \\ldots, 2 r_C \\rangle \n",
            "= 2\\vec{r}\\]\n",
            "- The addition, subtraction, and square norm operations work the same way as before except they act on each element. Moving back through the elementwise operations we see that:\n",
            "- \\[\\nabla_r J = \\nabla_{\\hat{y}} J = \\nabla_b J = \\nabla_z J = 2\\vec{r}\\]\n",
            "- For the operation $z=Wx$, a little algebra will show you that:\n",
            "- \\[\\begin{aligned}\n",
            "\\nabla_W J &= \\nabla_z J \\cdot x^T \\\\\n",
            "\\nabla_x J &= W^T \\cdot \\nabla_z J\n",
            "\\end{aligned}\\]\n",
            "- Note that the gradient of a variable has the same shape as the variable itself. In particular $\\nabla_W J$ is a $C\\times D$ matrix. Here is the graphical representation for matrix multiplication:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Multiple-instances\n",
            "Heading: Multiple instances\n",
            "URL: #Multiple-instances\n",
            "- We will typically process data multiple instances at a time for efficiency. Thus, the input $x$ will be a $D\\times N$ matrix, and the output $y$ will be a $C\\times N$ matrix, the $N$ columns representing $N$ different instances. Please verify to yourself that the forward and backward operations as described above handle this case without much change: the elementwise operations act on the elements of the matrices just like vectors, and the matrix multiplication and its gradient remains the same. Here is a picture of the forward and backward passes:\n",
            "- The only complication is at the addition of the bias vector. In the batch setting, we are adding $b\\in\\mathbb{R}^{C\\times 1}$ to $z\\in\\mathbb{R}^{C\\times N}$. This will be a broadcasting operation, i.e. the vector $b$ will be added to each column of the matrix $z$ to get $\\hat{y}$. In the backward pass, we'll need to add the columns of $\\nabla_{\\hat{y}} J$ to get the gradient $\\nabla_b J$.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Stochastic-Gradient-Descent\n",
            "Heading: Stochastic Gradient Descent\n",
            "URL: #Stochastic-Gradient-Descent\n",
            "- The gradients calculated by backprop, $\\nabla_w J$ and $\\nabla_b J$, tell us how much small changes in corresponding entries in $w$ and $b$ will effect the error (for the current example(s)). Small steps in the gradient direction will increase the error, steps in the opposite direction will decrease the error.\n",
            "- In fact, we can show that the gradient is the direction of steepest ascent. Consider a unit vector $v$ pointing in some arbitrary direction.  The rate of change in this direction, $\\nabla_v J$ (directional derivative), is given by the projection of $v$ onto the gradient, $\\nabla J$, i.e. their dot product $\\nabla J \\cdot v$:\n",
            "- \\[\\nabla_v J = \\frac{\\partial J}{\\partial x_1} v_1 + \\frac{\\partial J}{\\partial x_2} v_2 + \\cdots = \\nabla J \\cdot v\\]\n",
            "- What direction maximizes this dot product? Recall that:\n",
            "- \\[\\nabla J \\cdot v = | \\nabla J |\\,\\, | v | \\cos(\\theta)\\]\n",
            "- where $\\theta$ is the angle between $v$ and the gradient vector.  $\\cos(\\theta)$ is maximized when the two vectors point in the same direction. So if you are going to move a fixed (small) size step, the gradient direction gives you the biggest bang for the buck.\n",
            "- This suggests the following update rule:\n",
            "- \\[w \\leftarrow w - \\nabla_w J\\]\n",
            "- This is the basic idea behind Stochastic Gradient Descent (SGD): Go over the training set instance by instance (or minibatch by minibatch). Run the backpropagation algorithm to calculate the error gradients. Update the weights and biases in the opposite direction of these gradients.  Rinse and repeat...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Housing-Example\n",
            "Heading: Housing Example\n",
            "URL: #Housing-Example\n",
            "- We will use the Boston Housing dataset from the UCI Machine Learning Repository to train a linear regression model using backprop and SGD. The dataset has housing related information for 506 neighborhoods in Boston from 1978. Each neighborhood has 14 attributes, the goal is to use the first 13, such as average number of rooms per house, or distance to employment centers, to predict the 14’th attribute: median dollar value of the houses.\n",
            "- First, we download and convert the data into Julia arrays. The Knet package provides some utilities for this:\n",
            " - Code Block: \n",
            " [ using Knet\n",
            "include(Knet.dir(\"data\",\"housing.jl\"))\n",
            "x,y = housing()  # x is (13,506); y is (1,506) ]\n",
            "- Second, we implement our loss calculation in Julia. Personally I think callable objects are the most natural way to represent parametric functions. But you can use any coding style you wish as long as you can calculate a scalar loss from parameters and data.\n",
            " - Code Block: \n",
            " [ struct Linear; w; b; end                  # new type that can be used as a function\n",
            "(f::Linear)(x) = f.w * x .+ f.b           # prediction function if one argument\n",
            "(f::Linear)(x,y) = mean(abs2, f(x) - y)   # loss function if two arguments ]\n",
            "- Now we can initialize a model, make some predictions, and calculate loss:\n",
            " - Code Block: \n",
            " [ julia> model = Linear(zeros(1,13), zeros(1))\n",
            "Linear([0.0 0.0 … 0.0 0.0], [0.0])\n",
            "\n",
            "julia> pred = model(x)          # predictions for 506 instances\n",
            "1×506 Array{Float64,2}:\n",
            " 0.0   0.0   0.0   …  0.0   0.0   0.0\n",
            "\n",
            "julia> y                        # not too close to real outputs\n",
            "1×506 Array{Float64,2}:\n",
            " 24.0  21.6  34.7  …  23.9  22.0  11.9\n",
            "\n",
            "julia> loss = model(x,y)        # average loss for 506 instances\n",
            "592.1469169960474 ]\n",
            "- The loss gradients with respect to the model parameters can be computed manually as described above:\n",
            " - Code Block: \n",
            " [ julia> r = (model(x) - y) / length(y)\n",
            "1×506 Array{Float64,2}:\n",
            " -0.0474308  -0.0426877  -0.0685771  …  -0.0472332  -0.0434783  -0.0235178\n",
            "\n",
            "julia> ∇w = 2r * x'\n",
            "1×13 Array{Float64,2}:\n",
            " 7.12844  -6.617  8.88016  -3.2174  …  8.60132  9.32187  -6.12163  13.5419\n",
            "\n",
            "julia> ∇b = sum(2r)\n",
            "-45.06561264822134 ]\n",
            "- For larger models manual gradient calculation becomes impractical.  The Knet package can calculate gradients automatically for us: (1) mark the parameters with the Param type, (2) apply the @diff macro to the loss calculation, (3) the grad function calculates the gradients:\n",
            " - Code Block: \n",
            " [ julia> model = Linear(Param(zeros(1,13)), Param(zeros(1)))\n",
            "Linear(P(Array{Float64,2}(1,13)), P(Array{Float64,1}(1)))\n",
            "\n",
            "julia> loss = @diff model(x,y)\n",
            "T(592.1469169960474)\n",
            "\n",
            "julia> grad(loss, model.w)\n",
            "1×13 Array{Float64,2}:\n",
            " 7.12844  -6.617  8.88016  -3.2174  …  8.60132  9.32187  -6.12163  13.5419\n",
            "\n",
            "julia> grad(loss, model.b)\n",
            "1-element Array{Float64,1}:\n",
            " -45.06561264822134 ]\n",
            "- We can use the gradients to train our model:\n",
            " - Code Block: \n",
            " [ function sgdupdate(model, x, y)\n",
            "    loss = @diff model(x, y)\n",
            "    for p in params(model)\n",
            "        p .-= 0.1 * grad(loss, p)\n",
            "    end\n",
            "    return value(loss)\n",
            "end ]\n",
            "- Here is a plot of the loss value vs the number of updates:\n",
            " - Code Block: \n",
            " [ julia> using Plots\n",
            "julia> plot([sgdupdate(model,x,y) for i in 1:20]) ]\n",
            "- \n",
            "- The new predictions are a lot closer to the actual outputs:\n",
            " - Code Block: \n",
            " [ julia> [ model(x); y ]\n",
            "2×506 Array{Float64,2}:\n",
            " 30.4126  24.8121  30.7946  29.2931  …  27.6193  26.1515  21.9643\n",
            " 24.0     21.6     34.7     33.4        23.9     22.0     11.9    ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Problems-with-SGD\n",
            "Heading: Problems with SGD\n",
            "URL: #Problems-with-SGD\n",
            "- Over the years, people have noted many subtle problems with the SGD algorithm and suggested improvements:\n",
            "- Step size: If the step sizes are too small, the SGD algorithm will take too long to converge. If they are too big it will overshoot the optimum and start to oscillate. So we scale the gradients with an adjustable parameter called the learning rate $\\eta$:\n",
            "- $w \\leftarrow w - \\eta \\nabla_w J$\n",
            "- Step direction: More importantly, it turns out the gradient (or its opposite) is often NOT the direction you want to go in order to minimize error. Let us illustrate with a simple picture:\n",
            "- The figure on the left shows what would happen if you stood on one side of the long narrow valley and took the direction of steepest descent: this would point to the other side of the valley and you would end up moving back and forth between the two sides, instead of taking the gentle incline down as in the figure on the right. The direction across the valley has a high gradient but also a high curvature (second derivative) which means the descent will be sharp but short lived. On the other hand the direction following the bottom of the valley has a smaller gradient and low curvature, the descent will be slow but it will continue for a longer distance. Newton's method adjusts the direction taking into account the second derivative:\n",
            "- In this figure, the two axes are w1 and w2, two parameters of our network, and the contour plot represents the error with a minimum at x.  If we start at x0, the Newton direction (in red) points almost towards the minimum, whereas the gradient (in green), perpendicular to the contours, points to the right.\n",
            "- Unfortunately Newton's direction is expensive to compute. However, it is also probably unnecessary for several reasons: (1) Newton gives us the ideal direction for second degree objective functions, which our objective function almost certainly is not, (2) The error function whose gradient backprop calculated is the error for the last minibatch/instance only, which at best is a very noisy approximation of the real error function, thus we shouldn't spend too much effort trying to get the direction exactly right.\n",
            "- So people have come up with various approximate methods to improve the step direction. Instead of multiplying each component of the gradient with the same learning rate, these methods scale them separately using their running average (momentum, Nesterov), or RMS (Adagrad, Rmsprop).  Some even cap the gradients at an arbitrary upper limit (gradient clipping) to prevent instabilities.\n",
            "- You may wonder whether these methods still give us directions that consistently increase/decrease the objective function. If we do not insist on the maximum increase, any direction whose components have the same signs as the gradient vector is guaranteed to increase the function (for short enough steps). The reason is again given by the dot product $\\nabla J \\cdot v$. As long as these two vectors carry the same signs in the same components, the dot product, i.e. the rate of change along $v$, is guaranteed to be positive.\n",
            "- Minimize what? The final problem with gradient descent, other than not telling us the ideal step size or direction, is that it is not even minimizing the right objective! We want small error on never before seen test data, not just on the training data. The truth is, a sufficiently large model with a good optimization algorithm can get arbitrarily low error (down to the noise limit) on any finite training data (e.g. by just memorizing the answers). And it can typically do so in many different ways (typically many different local minima for training error in weight space exist). Some of those ways will generalize well to unseen data, some won't. And unseen data is (by definition) not seen, so how will we ever know which weight settings will do well on it?\n",
            "- There are at least three ways people deal with this problem: (1) Bayes tells us that we should use all possible models and weigh their answers by how well they do on training data (see Radford Neal's fbm), (2) New methods like dropout that add distortions and noise to inputs, activations, or weights during training seem to help generalization, (3) Pressuring the optimization to stay in one corner of the weight space (e.g. L1, L2, maxnorm regularization) helps generalization.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#References\n",
            "Heading: References\n",
            "URL: #References\n",
            "- UFLDL Tutorial, Linear Regression\n",
            "- cs231n Optimization Notes\n",
            "- cs229 Convex optimization overview,Part 2\n",
            "- cs229 Linear algebra review and reference\n",
            "- cs229 Review of probability theory\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Notes\n",
            "Heading: Notes\n",
            "URL: #Notes\n",
            "- Supervised learning is also known asregressionif the outputs are numeric andclassificationif they are discrete.\n",
            "- Linear regressionis a regression model with a linear prediction function.  Linear regression with a scalar input and output is calledsimple linear regression, if the input is a vector we havemultiple linear regression, and if the output is a vector we havemultivariate linear regression.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[<h2 id=\"Classification\"><a class=\"docs-heading-anchor\" href=\"#Classification\">Classification</a><a id=\"Classification-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Classification\" title=\"Permalink\"></a></h2>, <h2 id=\"Likelihood\"><a class=\"docs-heading-anchor\" href=\"#Likelihood\">Likelihood</a><a id=\"Likelihood-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Likelihood\" title=\"Permalink\"></a></h2>, <h2 id=\"Softmax\"><a class=\"docs-heading-anchor\" href=\"#Softmax\">Softmax</a><a id=\"Softmax-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Softmax\" title=\"Permalink\"></a></h2>, <h2 id=\"One-hot-vectors\"><a class=\"docs-heading-anchor\" href=\"#One-hot-vectors\">One-hot vectors</a><a id=\"One-hot-vectors-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#One-hot-vectors\" title=\"Permalink\"></a></h2>, <h2 id=\"Gradient-of-log-likelihood\"><a class=\"docs-heading-anchor\" href=\"#Gradient-of-log-likelihood\">Gradient of log likelihood</a><a id=\"Gradient-of-log-likelihood-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Gradient-of-log-likelihood\" title=\"Permalink\"></a></h2>, <h2 id=\"MNIST-example\"><a class=\"docs-heading-anchor\" href=\"#MNIST-example\">MNIST example</a><a id=\"MNIST-example-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#MNIST-example\" title=\"Permalink\"></a></h2>, <h2 id=\"Representational-power\"><a class=\"docs-heading-anchor\" href=\"#Representational-power\">Representational power</a><a id=\"Representational-power-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Representational-power\" title=\"Permalink\"></a></h2>, <h2 id=\"References\"><a class=\"docs-heading-anchor\" href=\"#References\">References</a><a id=\"References-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#References\" title=\"Permalink\"></a></h2>]\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Classification\n",
            "Heading: Classification\n",
            "URL: #Classification\n",
            "- Classification problems are supervised machine learning problems where the task is to predict a discrete class for a given input (unlike regression where the output was numeric). A typical example is handwritten digit recognition where the input is an image of a handwritten digit, and the output is one of the discrete categories $\\{0, \\ldots, 9\\}$. As in all supervised learning problems the training data consists of a set of example input-output pairs.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Likelihood\n",
            "Heading: Likelihood\n",
            "URL: #Likelihood\n",
            "- A natural objective in classification could be to minimize the number of misclassified examples in the training data. This number is known as the zero-one loss. However the zero-one loss has some undesirable properties for training: in particular it is discontinuous. A small change in one of the parameters either has no effect on the loss, or can turn one or more of the predictions from false to true or true to false, causing a discontinuous jump in the objective. This means the gradient of the zero-one loss with respect to the parameters is either undefined or zero, thus not helpful.\n",
            "- A more commonly used objective for classification is conditional likelihood: the probability of the observed data given our model and the inputs. Instead of predicting a single class for each instance, we let our model predict a probability distribution over all classes. Then we adjust the weights of the model to increase the probabilities for the correct classes and decrease it for others. This is also known as the maximum likelihood estimation (MLE).\n",
            "- Let $\\mathcal{X}=\\{x_1,\\ldots,x_N\\}$ be the inputs in the training data, $\\mathcal{Y}=\\{y_1,\\ldots,y_N\\}$ be the correct classes and $\\theta$ be the parameters of our model. Conditional likelihood is:\n",
            "- \\[L(\\theta) = P(\\mathcal{Y}|\\mathcal{X},\\theta) \n",
            "= \\prod_{n=1}^N P(y_n|x_n,\\theta)\\]\n",
            "- The second equation assumes that the data instances were generated independently. \n",
            "- We usually work with log likelihood for mathematical convenience: log is a monotonically increasing function, so maximizing likelihood is the same as maximizing log likelihood:\n",
            "- \\[\\ell(\\theta) = \\log P(\\mathcal{Y}|\\mathcal{X},\\theta) \n",
            "= \\sum_{n=1}^N \\log P(y_n|x_n,\\theta)\\]\n",
            "- We will typically use the negative of $\\ell$ (machine learning people like to minimize), which is known as negative log likelihood (NLL), or cross-entropy loss.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Softmax\n",
            "Heading: Softmax\n",
            "URL: #Softmax\n",
            "- A classification model for a problem with $C$ classes typically generates $y\\in\\mathbb{R}^C$, a vector of $C$ scores (e.g. we might use multivariate linear regression with a vector output as seen in the last chapter).  In general these scores will be arbitrary real numbers.  To go from arbitrary scores $y\\in\\mathbb{R}^C$ to normalized probability estimates $p\\in\\mathbb{R}^C$ for a single instance, we use exponentiation and normalization:\n",
            "- \\[p_i = \\frac{\\exp y_i}{\\sum_{c=1}^C \\exp y_c}\\]\n",
            "- where $i,c\\in\\{1,\\ldots,C\\}$ range over classes, and $p_i, y_i, y_c$ refer to class probabilities and scores for a single instance. This is called the softmax function. A model that converts the unnormalized values at the end of a linear regression to normalized probabilities for classification is called the softmax classifier.\n",
            "- We need to figure out the backward pass for the softmax function. In other words if someone gives us the gradient of some objective $J$ with respect to the class probabilities $p$ for a single training instance, what is the gradient with respect to the input of the softmax $y$? First we'll find the partial derivative of one component of $p$ with respect to one component of $y$:\n",
            "- \\[\\frac{\\partial p_i}{\\partial y_j} \n",
            "= \\frac{[i=j] \\exp y_i (\\sum_c \\exp y_c) - \\exp y_i \\exp y_j}{(\\sum_c \\exp y_c)^2}\n",
            "= \\,[i=j]\\, p_i - p_i p_j\\]\n",
            "- The square brackets are the Iverson bracket notation, i.e. $[A]$ is 1 if $A$ is true, and 0 if $A$ is false.\n",
            "- Note that a single entry in $y$ effects $J$ through multiple paths ($y_j$ contributes to the denominator of every $p_i$), and these effects need to be added for $\\partial J/\\partial y_j$:\n",
            "- \\[\\frac{\\partial J}{\\partial y_j}\n",
            "= \\sum_{i=1}^C \\frac{\\partial J}{\\partial p_i}\n",
            "\\frac{\\partial p_i}{\\partial y_j}\\]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#One-hot-vectors\n",
            "Heading: One-hot vectors\n",
            "URL: #One-hot-vectors\n",
            "- When using a probabilistic classifier, it is convenient to represent the desired output as a one-hot vector, i.e. a vector in which all entries are '0' except a single '1'. If the correct class is $c\\in\\{1,\\ldots,C\\}$, we represent this with a one-hot vector $p\\in\\mathbb{R}^C$ where $p_c = 1$ and $p_{i\\neq c} = 0$. Note that $p$ can be viewed as a probability vector where all the probability mass is concentrated at class c. This representation also allows us to have probabilistic targets where there is not a single answer but target probabilities associated with each answer. Given a one-hot (or probabilistic) $p$, and the model prediction $\\hat{p}$, we can write the log-likelihood for a single instance as:\n",
            "- \\[\\ell = \\sum_{c=1}^C p_c \\log \\hat{p}_c\\]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Gradient-of-log-likelihood\n",
            "Heading: Gradient of log likelihood\n",
            "URL: #Gradient-of-log-likelihood\n",
            "- To compute the gradient for log likelihood, we need to make the normalization of $\\hat{p}$ explicit:\n",
            "- \\[\\begin{aligned}\n",
            "\\ell &= \\sum_c p_c \\log \\frac{\\hat{p}_c}{\\sum_k\\hat{p}_k} \\\\\n",
            "&= (\\sum_c p_c \\log{\\hat{p}_c}) - (\\sum_c p_c \\log \\sum_k\\hat{p}_k) \\\\\n",
            "&= (\\sum_c p_c \\log{\\hat{p}_c}) - (\\log \\sum_k\\hat{p}_k) \\\\\n",
            "\\frac{\\partial \\ell}{\\partial \\hat{p}_i} &=\n",
            "\\frac{p_i}{\\hat{p}_i} - \\frac{1}{\\sum_k\\hat{p}_k}\n",
            "= \\frac{p_i}{\\hat{p}_i} - 1\n",
            "\\end{aligned}\\]\n",
            "- The gradient with respect to unnormalized y scores takes a particularly simple form:\n",
            "- \\[\\begin{aligned}\n",
            "\\frac{\\partial\\ell}{\\partial y_j}\n",
            "&= \\sum_i \\frac{\\partial\\ell}{\\partial \\hat{p}_i}\n",
            "\\frac{\\partial \\hat{p}_i}{\\partial y_j} \\\\\n",
            "&= \\sum_i (\\frac{p_i}{\\hat{p}_i} - 1)(\\,[i=j]\\, \\hat{p}_i - \\hat{p}_i \\hat{p}_j) \\\\\n",
            "&= \\, p_j - \\hat{p}_j \\\\\n",
            "\\nabla_y \\ell &= \\, p - \\hat{p}\n",
            "\\end{aligned}\\]\n",
            "- The gradient with respect to $\\hat{p}$ causes numerical overflow when some components of $\\hat{p}$ get very small. In practice we usually skip that and directly compute the gradient with respect to $y$ which is numerically stable.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#MNIST-example\n",
            "Heading: MNIST example\n",
            "URL: #MNIST-example\n",
            "- Let's try our softmax classifier on the MNIST handwritten digit classification dataset. Here are the first 8 images from MNIST, the goal is to look at the pixels and classify each image as one of the digits 0-9:\n",
            "- \n",
            "- Load and minibatch the data. dtrn and dtst consist of xy pairs [ (x1,y1), (x2,y2), ... ] where xi,yi are minibatches of 100 instances:\n",
            " - Code Block: \n",
            " [ using Knet\n",
            "include(Knet.dir(\"data\",\"mnist.jl\"))\n",
            "xtrn,ytrn,xtst,ytst = mnist()\n",
            "dtst = minibatch(xtst,ytst,100)\n",
            "dtrn = minibatch(xtrn,ytrn,100) ]\n",
            "- Here is the softmax classifier in Knet:\n",
            " - Code Block: \n",
            " [ predict(w,x) = w[1]*mat(x) .+ w[2]\t  # mat converts x to 2D\n",
            "loss(w,x,ygold) = nll(predict(w,x),ygold) # nll computes negative log likelihood\n",
            "lossgradient = grad(loss)                 # grad returns gradient function\n",
            "wsoft=[ 0.1*randn(10,784), zeros(10,1) ]  # initial weights and bias ]\n",
            "- Here is the SGD training loop (see the full example in the Knet tutorial):\n",
            " - Code Block: \n",
            " [ function train!(w, data; lr=.1)\n",
            "    for (x,y) in data\n",
            "        dw = lossgradient(w, x, y)\n",
            "        for i in 1:length(w)\n",
            "            w[i] -= lr * dw[i]\n",
            "        end\n",
            "    end\n",
            "    return w\n",
            "end ]\n",
            "- Here are the plots of the negative log likelihood and misclassification error vs training epochs:\n",
            "- \n",
            "- We can observe a few things. First the training losses are better than the test losses. This means there is some overfitting, i.e. the model is learning spurious regularities in the training data that do not generalize to test data. Second, it does not look like the training loss is going down to zero. This means there is also underfitting, i.e. the softmax model is not flexible enough to fit the training data exactly.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Representational-power\n",
            "Heading: Representational power\n",
            "URL: #Representational-power\n",
            "- So far we have seen how to create a machine learning model as a differentiable program (linear regression, softmax classification) whose parameters can be adjusted to hopefully imitate whatever process generated our training data. A natural question to ask is whether a particular model can behave like any system we want (given the right parameters) or whether there is a limit to what it can represent.\n",
            "- It turns out the softmax classifier is quite limited in its representational power: it can only represent linear classification boundaries. To show this, remember the form of the softmax classifier which gives the probability of the i'th class as:\n",
            "- \\[p_i = \\frac{\\exp y_i}{\\sum_{c=1}^C \\exp y_c}\\]\n",
            "- where $y_i$ is a linear function of the input $x$. Note that $p_i$ is a monotonically increasing function of $y_i$, so for two classes $i$ and $j$, $p_i > p_j$ iff $y_i > y_j$. The boundary between two classes $i$ and $j$ is the set of inputs for which the probability of the two classes are equal:\n",
            "- \\[\\begin{aligned}\n",
            "p_i &= p_j \\\\\n",
            "y_i &= y_j \\\\\n",
            "w_i x + b_i &= w_j x + b_j \\\\\n",
            "(w_i - w_j) x + (b_i - b_j) &= 0\n",
            "\\end{aligned}\\]\n",
            "- where $w_i, b_i$ refer to the i'th row of $w$ and $b$. This is a linear equation, i.e. the border between two classes will always be linear in the input space with the softmax classifier:\n",
            "- \n",
            "- In the MNIST example, the relation between the pixels and the digit classes is unlikely to be this simple. That is why we are stuck at 6-7% training error. To get better results we need more powerful models.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#References\n",
            "Heading: References\n",
            "URL: #References\n",
            "- UFLDL Tutorial, Softmax Regression\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[<h2 id=\"Stacking-linear-classifiers-is-useless\"><a class=\"docs-heading-anchor\" href=\"#Stacking-linear-classifiers-is-useless\">Stacking linear classifiers is useless</a><a id=\"Stacking-linear-classifiers-is-useless-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Stacking-linear-classifiers-is-useless\" title=\"Permalink\"></a></h2>, <h2 id=\"Introducing-nonlinearities\"><a class=\"docs-heading-anchor\" href=\"#Introducing-nonlinearities\">Introducing nonlinearities</a><a id=\"Introducing-nonlinearities-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Introducing-nonlinearities\" title=\"Permalink\"></a></h2>, <h2 id=\"Types-of-nonlinearities-(activation-functions)\"><a class=\"docs-heading-anchor\" href=\"#Types-of-nonlinearities-(activation-functions)\">Types of nonlinearities (activation functions)</a><a id=\"Types-of-nonlinearities-(activation-functions)-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Types-of-nonlinearities-(activation-functions)\" title=\"Permalink\"></a></h2>, <h2 id=\"Representational-power\"><a class=\"docs-heading-anchor\" href=\"#Representational-power\">Representational power</a><a id=\"Representational-power-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Representational-power\" title=\"Permalink\"></a></h2>, <h2 id=\"Matrix-vs-Neuron-Pictures\"><a class=\"docs-heading-anchor\" href=\"#Matrix-vs-Neuron-Pictures\">Matrix vs Neuron Pictures</a><a id=\"Matrix-vs-Neuron-Pictures-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Matrix-vs-Neuron-Pictures\" title=\"Permalink\"></a></h2>, <h2 id=\"Programming-Example\"><a class=\"docs-heading-anchor\" href=\"#Programming-Example\">Programming Example</a><a id=\"Programming-Example-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Programming-Example\" title=\"Permalink\"></a></h2>, <h2 id=\"References\"><a class=\"docs-heading-anchor\" href=\"#References\">References</a><a id=\"References-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#References\" title=\"Permalink\"></a></h2>]\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Stacking-linear-classifiers-is-useless\n",
            "Heading: Stacking linear classifiers is useless\n",
            "URL: #Stacking-linear-classifiers-is-useless\n",
            "- We could try stacking multiple linear classifiers together. Here is a two layer model:\n",
            " - Code Block: \n",
            " [ function multilinear(w, x, ygold)\n",
            "    y1 = w[1] * x  .+ w[2]\n",
            "    y2 = w[3] * y1 .+ w[4]\n",
            "    return softloss(ygold, y2)\n",
            "end ]\n",
            "- Note that instead of using y1 as our prediction, we used it as input to another linear classifier. Intermediate arrays like y1 are known as hidden layers because their contents are not directly visible outside the model.\n",
            "- If you experiment with this model (I suggest using a smaller learning rate, e.g. 0.01), you will see that it performs similarly to the original softmax model. The reason is simple to see if we write the function computed in mathematical notation and do some algebra:\n",
            "- \\[\\begin{aligned}\n",
            "\\hat{p} &= \\operatorname{softmax}(W_2 (W_1 x + b_1) + b_2) \\\\\n",
            "&= \\operatorname{softmax}((W_2 W_1)\\, x + W_2 b_1 + b_2) \\\\\n",
            "&= \\operatorname{softmax}(W x + b)\n",
            "\\end{aligned}\\]\n",
            "- where $W=W_2 W_1$ and $b=W_2 b_1 + b_2$. In other words, we still have a linear classifier! No matter how many linear functions you put on top of each other, what you get at the end is still a linear function. So this model has exactly the same representation power as the softmax model. Unless, we add a simple instruction...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Introducing-nonlinearities\n",
            "Heading: Introducing nonlinearities\n",
            "URL: #Introducing-nonlinearities\n",
            "- Here is a slightly modified version of the two layer model:\n",
            " - Code Block: \n",
            " [ function mlp(w, x, ygold)\n",
            "    y1 = relu(w[1] * x .+ w[2])\n",
            "    y2 = w[3] * y1 .+ w[4]\n",
            "    return softloss(ygold, y2)\n",
            "end ]\n",
            "- MLP in mlp stands for multilayer perceptron which is one name for this type of model. The only difference with the previous example is the relu() function we introduced in the first line. This is known as the rectified linear unit (or rectifier), and is a simple function defined by relu(x)=max(x,0) applied elementwise to the input array. So mathematically what we are computing is:\n",
            "- \\[\\hat{p} = \\operatorname{softmax}(W_2\\, \\operatorname{relu}(W_1 x + b_1) + b_2) \\\\\\]\n",
            "- This cannot be reduced to a linear function, which may not seem like a big difference but what a difference it makes to the model! Here are the learning curves for mlp using a hidden layer of size 64:\n",
            "- Here are the learning curves for the linear model softmax plotted at the same scale for comparison:\n",
            "- We can observe a few things: using MLP instead of a linear model brings the training error from 6.7% to 0 and the test error from 7.5% to 2.0%. There is still overfitting: the test error is not as good as the training error, but the model has no problem classifying the training data (all 60,000 examples) perfectly!\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Types-of-nonlinearities-(activation-functions)\n",
            "Heading: Types of nonlinearities (activation functions)\n",
            "URL: #Types-of-nonlinearities-(activation-functions)\n",
            "- The functions we throw between linear layers to break the linearity are called nonlinearities or activation functions. Here are some activation functions that have been used as nonlinearities:\n",
            "- The step functions were the earliest activation functions used in the perceptrons of 1950s. Unfortunately they do not give a useful derivative that can be used for training a multilayer model. Sigmoid and tanh (sigm and tanh in Knet) became popular in 1980s as smooth approximations to the step functions and allowed the application of the backpropagation algorithm. Modern activation functions like relu and maxout are piecewise linear. They are computationally inexpensive (no exponentials), and perform well in practice. We are going to use relu in most of our models. Here is the backward passes for sigmoid, tanh, and relu:\n",
            "- See (Karpathy, 2016, Ch 1) for more on activation functions and MLP architecture.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Representational-power\n",
            "Heading: Representational power\n",
            "URL: #Representational-power\n",
            "- You might be wondering whether relu had any special properties or would any of the other nonlinearities be sufficient. Another question is whether there are functions multilayer perceptrons cannot represent and if so whether adding more layers or different types of functions would increase their representational power. The short answer is that a two layer model can approximate any function if the hidden layer is large enough, and can do so with any of the nonlinearities introduced in the last section. Multilayer perceptrons are universal function approximators!\n",
            "- We said that a two-layer MLP is a universal function approximator given enough hidden units. This brings up the questions of efficiency: how many hidden units / parameters does one need to approximate a given function and whether the number of units depends on the number of hidden layers. The efficiency is important both computationally and statistically: models with fewer parameters can be evaluated faster, and can learn from fewer examples (ref?). It turns out there are functions whose representations are exponentially more expensive in a shallow network compared to a deeper network (see (Nielsen, 2016, Ch 5) for a discussion). Recent winners of image recognition contests use networks with dozens of convolutional layers. The advantage of deeper MLPs is empirically less clear, but you should experiment with the number of units and layers using a development set when starting a new problem.\n",
            "- Please see (Nielsen, 2016, Ch 4) for an intuitive explanation of the universality result and (Bengio et al. 2016, Ch 6.4) for a more in depth discussion and references.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Matrix-vs-Neuron-Pictures\n",
            "Heading: Matrix vs Neuron Pictures\n",
            "URL: #Matrix-vs-Neuron-Pictures\n",
            "- So far we have introduced multilayer perceptrons (aka artificial neural networks) using matrix operations. You may be wondering why people call them neural networks and be confused by terms like layers and units. In this section we will give the correspondence between the matrix view and the neuron view. Here is a schematic of a biological neuron (figures from (Karpathy, 2016, Ch 1)):\n",
            "- A biological neuron is a complex organism supporting thousands of chemical reactions simultaneously under the regulation of thousands of genes, communicating with other neurons through electrical and chemical pathways involving dozens of different types of neurotransmitter molecules. We assume (do not know for sure) that the main mechanism of communication between neurons is electrical spike trains that travel from the axon of the source neuron, through connections called synapses, into dendrites of target neurons. We simplify this picture further representing the strength of the spikes and the connections with simple numbers to arrive at this cartoon model:\n",
            "- \n",
            "- This model is called an artificial neuron, a perceptron, or simply a unit in neural network literature. We know it as the softmax classifier.\n",
            "- When a number of these units are connected in layers, we get a multilayer perceptron. When counting layers, we ignore the input layer. So the softmax classifier can be considered a one layer neural network. Here is a neural network picture and the corresponding matrix picture for a two layer model:\n",
            "- Here is a neural network picture and the corresponding matrix picture for a three layer model:\n",
            "- We can use the following elementwise notation for the neural network picture (e.g. similar to the one used in UFLDL):\n",
            "- \\[x_i^{(l)} = f(b_i^{(l)} + \\sum_j w_{ij}^{(l)} x_j^{(l-1)})\\]\n",
            "- Here $x_i^{(l)}$ refers to the activation of the $i$ th unit in $l$ th layer. We are counting the input as the 0'th layer. $f$ is the activation function, $b_i^{(l)}$ is the bias term. $w_{ij}^{(l)}$ is the weight connecting unit $j$ from layer $l-1$ to unit $i$ from layer $l$. The corresponding matrix notation is:\n",
            "- \\[x^{(l)} = f(W^{(l)} x^{(l-1)} + b^{(l)})\\]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Programming-Example\n",
            "Heading: Programming Example\n",
            "URL: #Programming-Example\n",
            "- In this section we introduce several Knet features that make it easier to define complex models. As our working example, we will go through several attempts to define a 3-layer MLP. Here is our first attempt:\n",
            " - Code Block: \n",
            " [ function mlp3a(w, x0)\n",
            "    x1 = relu(w[1] * x0 .+ w[2])\n",
            "    x2 = relu(w[3] * x1 .+ w[4])\n",
            "    return w[5] * x2 .+ w[6]\n",
            "end ]\n",
            "- We can identify bad software engineering practices in this definition in that it contains a lot of repetition.\n",
            "- The key to controlling complexity in computer languages is abstraction. Abstraction is the ability to name compound structures built from primitive parts, so they too can be used as primitives.\n",
            "- Defining new operators\n",
            "- We could make the definition of mlp3 more compact by defining separate functions for its layers:\n",
            " - Code Block: \n",
            " [ function mlp3b(w, x0)\n",
            "    x1 = relu_layer1(w, x0)\n",
            "    x2 = relu_layer2(w, x1)\n",
            "    return pred_layer3(w, x2)\n",
            "end\n",
            "\n",
            "function relu_layer1(w, x)\n",
            "    return relu(w[1] * x .+ w[2])\n",
            "end\n",
            "\n",
            "function relu_layer2(w, x)\n",
            "    return relu(w[3] * x .+ w[4])\n",
            "end\n",
            "\n",
            "function pred_layer3(x)\n",
            "    return w[5] * x .+ w[6]\n",
            "end ]\n",
            "- This may make the definition of mlp3b a bit more readable. But it does not reduce the overall length of the program. The helper functions like relu_layer1 and relu_layer2 are too similar except for the weights they use and can be reduced to a single function.\n",
            "- Increasing the number of layers\n",
            "- We can define a more general mlp model of arbitrary length. With weights of length 2n, the following model will have n layers, n-1 layers having the relu non-linearity:\n",
            " - Code Block: \n",
            " [ function mlp_nlayer(w,x)\n",
            "    for i=1:2:length(w)-2\n",
            "        x = relu(w[i] * x .+ w[i+1]))\n",
            "    end\n",
            "    return w[end-1] * x .+ w[end]\n",
            "end ]\n",
            "- In this example stacking the layers in a loop saved us only two lines, but the difference can be more significant in deeper models.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#References\n",
            "Heading: References\n",
            "URL: #References\n",
            "- http://neuralnetworksanddeeplearning.com/chap4.html\n",
            "- http://www.deeplearningbook.org/contents/mlp.html\n",
            "- http://cs231n.github.io/neural-networks-1\n",
            "- http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetwork\n",
            "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[<h2 id=\"Motivation\"><a class=\"docs-heading-anchor\" href=\"#Motivation\">Motivation</a><a id=\"Motivation-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Motivation\" title=\"Permalink\"></a></h2>, <h2 id=\"Convolution\"><a class=\"docs-heading-anchor\" href=\"#Convolution\">Convolution</a><a id=\"Convolution-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Convolution\" title=\"Permalink\"></a></h2>, <h2 id=\"Pooling\"><a class=\"docs-heading-anchor\" href=\"#Pooling\">Pooling</a><a id=\"Pooling-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Pooling\" title=\"Permalink\"></a></h2>, <h2 id=\"Normalization\"><a class=\"docs-heading-anchor\" href=\"#Normalization\">Normalization</a><a id=\"Normalization-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Normalization\" title=\"Permalink\"></a></h2>, <h2 id=\"Architectures\"><a class=\"docs-heading-anchor\" href=\"#Architectures\">Architectures</a><a id=\"Architectures-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Architectures\" title=\"Permalink\"></a></h2>, <h2 id=\"Exercises\"><a class=\"docs-heading-anchor\" href=\"#Exercises\">Exercises</a><a id=\"Exercises-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Exercises\" title=\"Permalink\"></a></h2>, <h2 id=\"References\"><a class=\"docs-heading-anchor\" href=\"#References\">References</a><a id=\"References-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#References\" title=\"Permalink\"></a></h2>]\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Motivation\n",
            "Heading: Motivation\n",
            "URL: #Motivation\n",
            "- Let's say we are trying to build a model that will detect cats in photographs. The average resolution of images in ILSVRC is 482x415, with three channels (RGB) this makes the typical input size 482x415x3=600,090. Each hidden unit connected to the input in a multilayer perceptron would have 600K parameters, a single hidden layer of size 1000 would have 600 million parameters. Too many parameters cause three types of problems: (1) runtime: large models are computationally costly to train and run. (2) memory: today's GPUs have limited amount of memory (4G-12G) and large networks fill them up quickly. (3) sample complexity: models with a large number of parameters are difficult to train without overfitting: we need a lot of data, strong regularization, and/or a good initialization to learn with large models.\n",
            "- One problem with the MLP is that it is fully connected: every hidden unit is connected to every other in adjacent layers. The model does not assume any spatial relationships between pixels, in fact we can permute all the pixels in an image and the performance of the MLP would be the same!  We could instead have an architecture where each hidden unit is connected to a small patch of the image, say 40x40. Each such locally connected hidden unit would have 40x40x3=4800 parameters instead of 600K. For the price (in memory) of one fully connected hidden unit (600K), we could have 125 of these locally connected mini-hidden-units (4800 each) with receptive fields spread around the image.\n",
            "- The second problem with the MLP is that it does not take advantage of the symmetry in the problem: a cat in the lower right corner of the image may be similar to a cat in the upper left corner. This means the local hidden units looking at these two patches can share the same weights. We can take one 40x40 cat filter and apply it to each 40x40 patch in the image taking up only 4800 parameters.\n",
            "- A convolutional neural network (aka CNN or ConvNet) combines these two ideas and uses operations that are local and that share weights. CNNs commonly use three types of operations: convolution, pooling, and normalization which we describe next.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Convolution\n",
            "Heading: Convolution\n",
            "URL: #Convolution\n",
            "- Convolution is the main operation that provides sparse connectivity and weight sharing.  For simplicity we start describing convolution in 1-D using the conv4 primitive from Knet. We next look at three keyword options that provide variations on the convolution operation: padding, stride, and mode.  We then describe how conv4 handles multiple dimensions, filters, and instances in parallel.\n",
            "- The relationship between convolution and matrix multiplication allows the use of efficient algorithms developed for matrix multiplication in convolution implementations.  The fact that convolution and matrix multiplication can be implemented in terms of each other clarifies the distinction between CNNs and MLPs as one of efficiency, not representative power.  We end this section by describing backpropagation for convolution.\n",
            "- Let $w, x$ be two 1-D vectors with $W, X$ elements respectively. In our examples, we will assume $x$ is the input (consider it a 1-D image) and $w$ is a filter (aka kernel) with $W<X$. The 1-D convolution operation $y=w\\ast x$ results in a vector with $Y=X-W+1$ elements defined as:\n",
            "- \\[y_k \\equiv \\sum_{i+j=k+W} x_i w_j\\]\n",
            "- or equivalently\n",
            "- \\[y_k \\equiv \\sum_{i=k}^{k+W-1} x_i w_{k+W-i}\\]\n",
            "- where $i\\in[1,X], j\\in[1,W], k\\in[1,Y]$. We get each entry in y by multiplying pairs of matching entries in x and w and summing the results. Matching entries in x and w are the ones whose indices add up to a constant. This can be visualized as flipping w, sliding it over x, and at each step writing their dot product into a single entry in y. Here is an example in Julia you should be able to calculate by hand:\n",
            " - Code Block: \n",
            " [ julia> using Knet\n",
            "julia> w = reshape([1.0,2.0,3.0], (3,1,1,1))\n",
            "3×1×1×1 Array{Float64,4}: [1,2,3]\n",
            "julia> x = reshape([1.0:7.0...], (7,1,1,1))\n",
            "7×1×1×1 Array{Float64,4}: [1,2,3,4,5,6,7]\n",
            "julia> y = conv4(w, x)\n",
            "5×1×1×1 Array{Float64,4}: [10,16,22,28,34] ]\n",
            "- conv4 is the convolution operation in Knet (based on the CUDNN implementation). For reasons that will become clear it works with 4-D and 5-D arrays, so we reshape our 1-D input vectors by adding extra singleton dimensions at the end.  The convolution of w=[1,2,3] and x=[1,2,3,4,5,6,7] gives y=[10,16,22,28,34]. For example, the third element of y, 22, can be obtained by reversing w to [3,2,1] and taking its dot product starting with the third element of x, [3,4,5].\n",
            "- In the last example, the input x had 7 dimensions, the output y had 5. In image processing applications we typically want to keep x and y the same size. For this purpose we can provide a padding keyword argument to the conv4 operator. If padding=k, x will be assumed padded with k zeros on the left and right before the convolution, e.g. padding=1 means treat x as [0 1 2 3 4 5 6 7 0]. The default padding is 0. For inputs in D-dimensions we can specify padding with a D-tuple, e.g. padding=(1,2) for 2D, or a single number, e.g. padding=1 which is shorthand for padding=(1,1). The result will have $Y=X+2P-W+1$ elements where $P$ is the padding size. Therefore to preserve the size of x when W=3 we should use padding=1.\n",
            " - Code Block: \n",
            " [ julia> y = conv4(w, x; padding=(1,0))\n",
            "7×1×1×1 Array{Float64,4}: [4,10,16,22,28,34,32] ]\n",
            "- For example, to calculate the first entry of y, take the dot product of the inverted w, [3,2,1] with the first three elements of the padded x, [0 1 2]. You can see that in order to preserve the input size, $Y=X$, given a filter size $W$, the padding should be set to $P=(W-1)/2$. This will work if $W$ is odd.\n",
            "- In the preceding examples we shift the inverted w by one position after each dot product. In some cases you may want to skip two or more positions. This will effectively reduce the size of the output.  The amount of skip is set by the stride keyword argument of the conv4 operation (the default stride is 1). In the following example we set stride to $W$ such that the consecutive filter applications are non-overlapping:\n",
            " - Code Block: \n",
            " [ julia> y = conv4(w, x; padding=(1,0), stride=3)\n",
            "3×1×1×1 Array{Float64,4}: [4,22,32] ]\n",
            "- Note that the output has the first, middle, and last values of the previous example, i.e. every third value is kept and the rest are skipped. In general if stride=S and padding=P, the size of the output will be:\n",
            "- \\[Y = 1 + \\left\\lfloor\\frac{X+2P-W}{S}\\right\\rfloor\\]\n",
            "- The convolution operation we have used so far flips the convolution kernel before multiplying it with the input. To take our first 1-D convolution example with\n",
            "- \\[y_1 = x_1 w_W + x_2 w_{W-1} + x_3 w_{W-2} + \\ldots \\\\\n",
            "y_2 = x_2 w_W + x_3 w_{W-1} + x_4 w_{W-2} + \\ldots \\\\\n",
            "\\ldots\\]\n",
            "- We could also perform a similar operation without kernel flipping:\n",
            "- \\[y_1 = x_1 w_1 + x_2 w_2 + x_3 w_3 + \\ldots \\\\\n",
            "y_2 = x_2 w_1 + x_3 w_2 + x_4 w_3 + \\ldots \\\\\n",
            "\\ldots\\]\n",
            "- This variation is called cross-correlation. The two modes are specified in Knet by choosing one of the following as the value of the mode keyword:\n",
            "- 0for convolution\n",
            "- 1for cross-correlation\n",
            "- This option would be important if we were hand designing our filters. However the mode does not matter for CNNs where the filters are learnt from data, the CNN will simply learn an inverted version of the filter if necessary.\n",
            "- When the input x has multiple dimensions convolution is defined similarly. In particular the filter w has the same number of dimensions but typically smaller size. The convolution operation flips w in each dimension and slides it over x, calculating the sum of elementwise products at every step. The formulas we have given above relating the output size to the input and filter sizes, padding and stride parameters apply independently for each dimension.\n",
            "- Knet supports 2D and 3D convolutions. The inputs and the filters have two extra dimensions at the end which means we use 4D and 5D arrays for 2D and 3D convolutions. Here is a 2D convolution example:\n",
            " - Code Block: \n",
            " [ julia> w = reshape([1.0:4.0...], (2,2,1,1))\n",
            "2×2×1×1 Array{Float64,4}:\n",
            "[:, :, 1, 1] =\n",
            " 1.0  3.0\n",
            " 2.0  4.0\n",
            "julia> x = reshape([1.0:9.0...], (3,3,1,1))\n",
            "3×3×1×1 Array{Float64,4}:\n",
            "[:, :, 1, 1] =\n",
            " 1.0  4.0  7.0\n",
            " 2.0  5.0  8.0\n",
            " 3.0  6.0  9.0\n",
            "julia> y = conv4(w, x)\n",
            "2×2×1×1 Array{Float64,4}:\n",
            "[:, :, 1, 1] =\n",
            " 23.0  53.0\n",
            " 33.0  63.0 ]\n",
            "- To see how this result comes about, note that when you flip w in both dimensions you get:\n",
            " - Code Block: \n",
            " [ 4 2\n",
            "3 1 ]\n",
            "- Multiplying this elementwise with the upper left corner of x:\n",
            " - Code Block: \n",
            " [ 1 4\n",
            "2 5 ]\n",
            "- and adding the results gives you the first entry 23.\n",
            "- The padding and stride options work similarly in multiple dimensions and can be specified as tuples: padding=(1,2) means a padding width of 1 along the first dimension and 2 along the second dimension for a 2D convolution. You can use padding=1 as a shorthand for padding=(1,1).\n",
            "- So far we have been ignoring the extra dimensions at the end of our convolution arrays. Now we are ready to put them to use. A D-dimensional input image is typically represented as a D+1 dimensional array with dimensions:\n",
            "- \\[[ X_1, \\ldots, X_D, C_x ]\\]\n",
            "- The first D dimensions $X_1\\ldots X_D$ determine the spatial extent of the image. The last dimension $C_x$ is the number of channels (aka slices, frames, maps, filters). The definition and number of channels is application dependent. We use $C_x=3$ for RGB images representing the intensity in three colors: red, green, and blue. For grayscale images we have a single channel, $C_x=1$. If you were developing a model for chess, we could have $C_x=12$, each channel representing the locations of a different piece type.\n",
            "- In an actual CNN we do not typically hand-code the filters. Instead we tell the network: \"here are 1000 randomly initialized filters, you go ahead and turn them into patterns useful for my task.\" This means we usually work with banks of multiple filters simultaneously and GPUs have optimized operations for such filter banks. The dimensions of a typical filter bank are:\n",
            "- \\[[ W_1, \\ldots, W_D, C_x, C_y ]\\]\n",
            "- The first D dimensions $W_1\\ldots W_D$ determine the spatial extent of the filters. The next dimension $C_x$ is the number of input channels, i.e. the number of filters from the previous layer, or the number of color channels of the input image. The last dimension $C_y$ is the number of output channels, i.e. the number of filters in this layer.\n",
            "- If we take an input of size $[X_1,\\ldots, X_D,C_x]$ and apply a filter bank of size $[W_1,\\ldots,W_D,C_x,C_y]$ using padding $[P_1,\\ldots,P_D]$ and stride $[S_1,\\ldots,S_D]$ the resulting array will have dimensions:\n",
            "- \\[[ W_1, \\ldots, W_D, C_x, C_y ] \\ast [ X_1, \\ldots, X_D, C_x ] \n",
            "\\Rightarrow [ Y_1, \\ldots, Y_D, C_y ] \\\\\n",
            "\\mathrm{where } Y_i = 1 + \\left\\lfloor\\frac{X_i+2P_i-W_i}{S_i}\\right\\rfloor\\]\n",
            "- As an example let's start with an input image of 256x256 pixels and 3 RGB channels. We'll first apply 25 filters of size 5x5 and padding=2, then 50 filters of size 3x3 and padding=1, and finally 75 filters of size 3x3 and padding=1. Here are the dimensions we will get:\n",
            "- \\[[ 256, 256, 3 ] \\ast [ 5, 5, 3, 25 ] \\Rightarrow [ 256, 256, 25 ] \\\\\n",
            "[ 256, 256, 25] \\ast [ 3, 3, 25,50 ] \\Rightarrow [ 256, 256, 50 ] \\\\\n",
            "[ 256, 256, 50] \\ast [ 3, 3, 50,75 ] \\Rightarrow [ 256, 256, 75 ]\\]\n",
            "- Note that the number of input channels of the input data and the filter bank always match. In other words, a filter covers only a small part of the spatial extent of the input but all of its channel depth.\n",
            "- In addition to processing multiple filters in parallel, we implement CNNs with minibatching, i.e. process multiple inputs in parallel to fully utilize GPUs. A minibatch of D-dimensional images is represented as a D+2 dimensional array:\n",
            "- \\[[ X_1, \\ldots, X_D, C_x, N ]\\]\n",
            "- where $C_x$ is the number of channels as before, and N is the number of images in a minibatch. The convolution implementation in Knet/CUDNN use $D+2$ dimensional arrays for both images and filters. We used 1 for the extra dimensions in our first examples, in effect using a single channel and a single image minibatch.\n",
            "- If we apply a filter bank of size $[W_1, \\ldots, W_D, C_x, C_y]$ to the minibatch given above the output size would be:\n",
            "- \\[[ W_1, \\ldots, W_D, C_x, C_y ] \\ast [ X_1, \\ldots, X_D, C_x, N ] \n",
            "\\Rightarrow [ Y_1, \\ldots, Y_D, C_y, N ] \\\\\n",
            "\\mathrm{where } Y_i = 1 + \\left\\lfloor\\frac{X_i+2P_i-W_i}{S_i}\\right\\rfloor\\]\n",
            "- If we used a minibatch size of 128 in the previous example with 256x256 images, the sizes would be:\n",
            "- \\[[ 256, 256, 3, 128 ] \\ast [ 5, 5, 3, 25 ] \\Rightarrow [ 256, 256, 25, 128 ] \\\\\n",
            "[ 256, 256, 25, 128] \\ast [ 3, 3, 25,50 ] \\Rightarrow [ 256, 256, 50, 128 ] \\\\\n",
            "[ 256, 256, 50, 128] \\ast [ 3, 3, 50,75 ] \\Rightarrow [ 256, 256, 75, 128 ]\\]\n",
            "- basically adding an extra dimension of 128 at the end of each data array.\n",
            "- By the way, the arrays in this particular example already exceed 5GB of storage, so you would want to use a smaller minibatch size if you had a K20 GPU with 4GB of RAM.\n",
            "- Note: All the dimensions given above are for column-major languages like Julia. CUDNN uses row-major notation, so all the dimensions would be reversed, e.g. $[N,C_x,X_D,\\ldots,X_1]$.\n",
            "- Convolution can be turned into a matrix multiplication, where certain entries in the matrix are constrained to be the same. The motivation is to be able to use efficient algorithms for matrix multiplication in order to perform convolution. The drawback is the large amount of memory needed due to repeated entries or sparse representations.\n",
            "- Here is a matrix implementation for our first convolution example $w=[1\\ldots 3],\\,\\,x=[1\\ldots 7],\\,\\,w\\ast x = [10,16,22,28,34]$:\n",
            "- In this example we repeated the entries of the filter on multiple rows of a sparse matrix with shifted positions. Alternatively we can repeat the entries of the input to place each local patch on a separate column of an input matrix:\n",
            "- The first approach turns w into a $Y\\times X$ sparse matrix, wheras the second turns x into a $W\\times Y$ dense matrix.\n",
            "- For 2-D images, typically the second approach is used: the local patches of the image used by convolution are stretched out to columns of an input matrix, an operation commonly called im2col. Each convolutional filter is stretched out to rows of a filter matrix. After the matrix multiplication the resulting array is reshaped into the proper output dimensions. The following figure illustrates these operations on a small example:\n",
            "- It is also possible to go in the other direction, i.e. implement matrix multiplication (i.e. a fully connected layer) in terms of convolution. This conversion is useful when we want to build a network that can be applied to inputs of different sizes: the matrix multiplication would fail, but the convolution will give us outputs of matching sizes. Consider a fully connected layer with a weight matrix W of size $K\\times D$ mapping a D-dimensional input vector x to a K-dimensional output vector y. We can consider each of the K rows of the W matrix a convolution filter. The following example shows how we can reshape the arrays and use convolution for matrix multiplication:\n",
            " - Code Block: \n",
            " [ julia> using Knet\n",
            "julia> x = reshape([1.0:3.0...], (3,1))\n",
            "3×1 Array{Float64,2}:\n",
            " 1.0\n",
            " 2.0\n",
            " 3.0\n",
            "julia> w = reshape([1.0:6.0...], (2,3))\n",
            "2×3 Array{Float64,2}:\n",
            " 1.0  3.0  5.0\n",
            " 2.0  4.0  6.0\n",
            "julia> y = w * x\n",
            "2×1 Array{Float64,2}:\n",
            " 22.0\n",
            " 28.0\n",
            "julia> x2 = reshape(x, (3,1,1,1))\n",
            "3×1×1×1 Array{Float64,4}:\n",
            "[:, :, 1, 1] =\n",
            " 1.0\n",
            " 2.0\n",
            " 3.0\n",
            "julia> w2 = reshape(Array(w)', (3,1,1,2))\n",
            "3×1×1×2 Array{Float64,4}:\n",
            "[:, :, 1, 1] =\n",
            " 1.0\n",
            " 3.0\n",
            " 5.0\n",
            "[:, :, 1, 2] =\n",
            " 2.0\n",
            " 4.0\n",
            " 6.0\n",
            "julia> y2 = conv4(w2, x2; mode=1)\n",
            "1×1×2×1 Array{Float64,4}:\n",
            "[:, :, 1, 1] =\n",
            " 22.0\n",
            "[:, :, 2, 1] =\n",
            " 28.0 ]\n",
            "- In addition to computational concerns, these examples also show that a fully connected layer can emulate a convolutional layer given the right weights and vice versa, i.e. convolution does not get us any extra representational power. However it does get us representational and statistical efficiency, i.e. the functions we would like to approximate are often expressed with significantly fewer parameters using convolutional layers and thus require fewer examples to train.\n",
            "- Convolution is a linear operation consisting of additions and multiplications, so its backward pass is not very complicated except for the indexing. Just like the backward pass for matrix multiplication can be expressed as another matrix multiplication, the backward pass for convolution (at least if we use stride=1) can be expressed as another convolution. We will derive the backward pass for a 1-D example using the cross-correlation mode (no kernel flipping) to keep things simple. We will denote the cross-correlation operation with $\\star$ to distinguish it from convolution denoted with $\\ast$. Here are the individual entries of $y=w\\star x$:\n",
            "- \\[y_1 = x_1 w_1 + x_2 w_2 + x_3 w_3 + \\ldots \\\\\n",
            "y_2 = x_2 w_1 + x_3 w_2 + x_4 w_3 + \\ldots \\\\\n",
            "y_3 = x_3 w_1 + x_4 w_2 + x_5 w_3 + \\ldots \\\\\n",
            "\\ldots\\]\n",
            "- As you can see, because of weight sharing the same w entry is used in computing multiple y entries. This means a single w entry effects the objective function through multiple paths and these effects need to be added. Denoting $\\partial J/\\partial y_i$ as $y_i'$ for brevity we have:\n",
            "- \\[w_1' = x_1 y_1' + x_2 y_2' + \\ldots \\\\\n",
            "w_2' = x_2 y_1' + x_3 y_2' + \\ldots \\\\\n",
            "w_3' = x_3 y_1' + x_4 y_2' + \\ldots \\\\\n",
            "\\ldots \\\\\\]\n",
            "- which can be recognized as another cross-correlation operation, this time between $x$ and $y'$. This allows us to write $w'=y'\\star x$.\n",
            "- Alternatively, we can use the equivalent matrix multiplication operation from the last section to derive the backward pass:\n",
            "- If $r$ is the matrix with repeated $x$ entries in this picture, we have $y=wr$. Remember that the backward pass for matrix multiplication $y=wr$ is $w'=y'r^T$:\n",
            "- which can be recognized as the matrix multiplication equivalent of the cross correlation operation $w'=y'\\star x$.\n",
            "- Here is the gradient for the input:\n",
            "- \\[\\begin{aligned}\n",
            "& x_1' = w_1 y_1' \\\\\n",
            "& x_2' = w_2 y_1' + w_1 y_2' \\\\\n",
            "& x_3' = w_3 y_1' + w_2 y_2' + w_1 y_3' \\\\\n",
            "& \\ldots\n",
            "\\end{aligned}\\]\n",
            "- You can recognize this as a regular convolution between $w$ and $y'$ with some zero padding.\n",
            "- The following resources provide more detailed derivations of the backward pass for convolution:\n",
            "- Goodfellow, I.   (2010).   Technical report: Multidimensional, downsampled convolution for   autoencoders. Technical report, Université de Montréal. 312.\n",
            "- Bouvrie, J.   (2006).   Notes on convolutional neural networks.\n",
            "- UFLDLtutorialandexerciseon CNNs.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Pooling\n",
            "Heading: Pooling\n",
            "URL: #Pooling\n",
            "- It is common practice to use pooling (aka subsampling) layers in between convolution operations in CNNs. Pooling looks at small windows of the input, and computes a single summary statistic, e.g. maximum or average, for each window. A pooling layer basically says: tell me whether this feature exists in a certain region of the image, I don't care exactly where. This makes the output of the layer invariant to small translations of the input. Pooling layers use large strides, typically as large as the window size, which reduces the size of their output.\n",
            "- Like convolution, pooling slides a small window of a given size over the input optionally padded with zeros skipping stride pixels every step. In Knet by default there is no padding, the window size is 2, stride is equal to the window size and the pooling operation is max. These default settings reduce each dimension of the input to half the size.\n",
            "- Here is a 1-D example:\n",
            " - Code Block: \n",
            " [ julia> x = reshape([1.0:6.0...], (6,1,1,1))\n",
            "6×1×1×1 Array{Float64,4}: [1,2,3,4,5,6]\n",
            "julia> pool(x)\n",
            "3×1×1×1 Array{Float64,4}: [2,4,6] ]\n",
            "- With window size and stride equal to 2, pooling considers the input windows $[1,2], [3,4], [5,6]$ and picks the maximum in each window.\n",
            "- The default and most commonly used window size is 2, however other window sizes can be specified using the window keyword. For D-dimensional inputs the size can be specified using a D-tuple, e.g. window=(2,3) for 2-D, or a single number, e.g. window=3 which is shorthand for window=(3,3) in 2-D. Here is an example using a window size of 3 instead of the default 2:\n",
            " - Code Block: \n",
            " [ julia> x = reshape([1.0:6.0...], (6,1,1,1))\n",
            "6×1×1×1 Array{Float64,4}: [1,2,3,4,5,6]\n",
            "julia> pool(x; window=3)\n",
            "2×1×1×1 Array{Float64,4}: [3, 6] ]\n",
            "- With a window and stride of 3 (the stride is equal to window size by default), pooling considers the input windows $[1,2,3],[4,5,6]$, and writes the maximum of each window to the output. If the input size is $X$, and stride is equal to the window size $W$, the output will have $Y=\\lfloor X/W\\rfloor$ elements.\n",
            "- The amount of zero padding is specified using the padding keyword argument just like convolution. Padding is 0 by default. For D-dimensional inputs padding can be specified as a tuple such as padding=(1,2), or a single number padding=1 which is shorthand for padding=(1,1) in 2-D. Here is a 1-D example:\n",
            " - Code Block: \n",
            " [ julia> x = reshape([1.0:6.0...], (6,1,1,1))\n",
            "6×1×1×1 Array{Float64,4}: [1,2,3,4,5,6]\n",
            "\n",
            "julia> pool(x; padding=(1,0))\n",
            "4×1×1×1 Array{Float64,4}: [1,3,5,6] ]\n",
            "- In this example, window=stride=2 by default and the padding size is 1, so the input is treated as $[0,1,2,3,4,5,6,0]$ and split into windows of $[0,1],[2,3],[4,5],[6,0]$ and the maximum of each window is written to the output.\n",
            "- With padding size $P$, if the input size is $X$, and stride is equal to the window size $W$, the output will have $Y=\\lfloor (X+2P)/W\\rfloor$ elements.\n",
            "- The pooling stride is equal to the window size by default (as opposed to the convolution case, where it is 1 by default). This is most common in practice but other strides can be specified using tuples e.g. stride=(1,2) or numbers e.g. stride=1. Here is a 1-D example with a stride of 4 instead of the default 2:\n",
            " - Code Block: \n",
            " [ julia> x = reshape([1.0:10.0...], (10,1,1,1))\n",
            "10×1×1×1 Array{Float64,4}: [1,2,3,4,5,6,7,8,9,10]\n",
            "\n",
            "julia> pool(x; stride=4)\n",
            "4×1×1×1 Array{Float64,4}: [2, 6, 10] ]\n",
            "- In general, when we have an input of size $X$ and pool with window size $W$, padding $P$, and stride $S$, the size of the output will be:\n",
            "- \\[Y = 1 + \\left\\lfloor\\frac{X+2P-W}{S}\\right\\rfloor\\]\n",
            "- There are three pooling operations defined by CUDNN used for summarizing each window:\n",
            "- CUDNN_POOLING_MAX\n",
            "- CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING\n",
            "- CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING\n",
            "- These options can be specified as the value of the mode keyword argument to the pool operation. The default is 0 (max pooling) which we have been using so far. The last two compute averages, and differ in whether to include or exclude the padding zeros in these averages. mode should be 1 for averaging including padding, and 2 for averaging excluding padding. For example, with input $x=[1,2,3,4,5,6]$, window=stride=2, and padding=1 we have the following outputs with the three options:\n",
            " - Code Block: \n",
            " [ mode=0 => [1,3,5,6]\n",
            "mode=1 => [0.5, 2.5, 4.5, 3.0]\n",
            "mode=2 => [1.0, 2.5, 4.5, 6.0] ]\n",
            "- D-dimensional inputs are pooled with D-dimensional windows, the size of each output dimension given by the 1-D formulas above. Here is a 2-D example with default options, i.e. window=stride=(2,2), padding=(0,0), mode=max:\n",
            " - Code Block: \n",
            " [ julia> x = reshape([1.0:16.0...], (4,4,1,1))\n",
            "4×4×1×1 Array{Float64,4}:\n",
            "[:, :, 1, 1] =\n",
            " 1.0  5.0   9.0  13.0\n",
            " 2.0  6.0  10.0  14.0\n",
            " 3.0  7.0  11.0  15.0\n",
            " 4.0  8.0  12.0  16.0\n",
            "\n",
            "julia> pool(x)\n",
            "2×2×1×1 Array{Float64,4}:\n",
            "[:, :, 1, 1] =\n",
            " 6.0  14.0\n",
            " 8.0  16.0 ]\n",
            "- As we saw in convolution, each data array has two extra dimensions in addition to the spatial dimensions: $[ X_1, \\ldots, X_D, C_x, N ]$ where $C_x$ is the number of channels and $N$ is the number of instances in a minibatch.\n",
            "- When the number of channels is greater than 1, the pooling operation is performed independently on each channel, e.g. for each patch, the maximum/average in each channel is computed independently and copied to the output. Here is an example with two channels:\n",
            " - Code Block: \n",
            " [ julia> x = rand(4,4,2,1)\n",
            "4×4×2×1 Array{Float64,4}:\n",
            "[:, :, 1, 1] =\n",
            " 0.880221  0.738729  0.317231   0.990521\n",
            " 0.626842  0.562692  0.339969   0.92469\n",
            " 0.416676  0.403625  0.352799   0.46624\n",
            " 0.566254  0.634703  0.0632812  0.0857779\n",
            "\n",
            "[:, :, 2, 1] =\n",
            " 0.300799  0.407623   0.26275   0.767884\n",
            " 0.217025  0.0055375  0.623168  0.957374\n",
            " 0.154975  0.246693   0.769524  0.628197\n",
            " 0.259161  0.648074   0.333324  0.46305\n",
            "\n",
            "julia> pool(x)\n",
            "2×2×2×1 Array{Float64,4}:\n",
            "[:, :, 1, 1] =\n",
            " 0.880221  0.990521\n",
            " 0.634703  0.46624\n",
            "\n",
            "[:, :, 2, 1] =\n",
            " 0.407623  0.957374\n",
            " 0.648074  0.769524 ]\n",
            "- When the number of instances is greater than 1, i.e. we are using minibatches, the pooling operation similarly runs in parallel on all the instances:\n",
            " - Code Block: \n",
            " [ julia> x = rand(4,4,1,2)\n",
            "4×4×1×2 Array{Float64,4}:\n",
            "[:, :, 1, 1] =\n",
            " 0.155228  0.848345  0.629651  0.262436\n",
            " 0.729994  0.320431  0.466628  0.0293943\n",
            " 0.374592  0.662795  0.819015  0.974298\n",
            " 0.421283  0.83866   0.385306  0.36081\n",
            "\n",
            "[:, :, 1, 2] =\n",
            " 0.0562608  0.598084  0.0231604  0.232413\n",
            " 0.71073    0.411324  0.28688    0.287947\n",
            " 0.997445   0.618981  0.471971   0.684064\n",
            " 0.902232   0.570232  0.190876   0.339076\n",
            "\n",
            "julia> pool(x)\n",
            "2×2×1×2 Array{Float64,4}:\n",
            "[:, :, 1, 1] =\n",
            " 0.848345  0.629651\n",
            " 0.83866   0.974298\n",
            "\n",
            "[:, :, 1, 2] =\n",
            " 0.71073   0.287947\n",
            " 0.997445  0.684064 ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Normalization\n",
            "Heading: Normalization\n",
            "URL: #Normalization\n",
            "- Draft...\n",
            "- Karpathy says: \"Many types of normalization layers have been proposed for use in ConvNet architectures, sometimes with the intentions of implementing inhibition schemes observed in the biological brain. However, these layers have recently fallen out of favor because in practice their contribution has been shown to be minimal, if any.\" (http://cs231n.github.io/convolutional-networks/#norm) Batch normalization may be an exception, as it is used in modern architectures.\n",
            "- Here are some references for normalization operations:\n",
            "- Implementations:\n",
            "- Alex Krizhevsky's cuda-convnet library API.   (https://code.google.com/archive/p/cuda-convnet/wikis/LayerParams.wiki#Localresponsenormalizationlayer(same_map))\n",
            "- http://caffe.berkeleyvision.org/tutorial/layers.html\n",
            "- http://lasagne.readthedocs.org/en/latest/modules/layers/normalization.html\n",
            "- Divisive normalisation (DivN):\n",
            "- S. Lyu and E. Simoncelli. Nonlinear image representation using   divisive normalization. In CVPR, pages 1–8, 2008.\n",
            "- Local contrast normalization (LCN):\n",
            "- N. Pinto, D. D. Cox, and J. J. DiCarlo. Why is real-world visual   object recognition hard? PLoS Computational Biology, 4(1), 2008.\n",
            "- Jarrett, Kevin, et al. \"What is the best multi-stage architecture   for object recognition?.\" Computer Vision, 2009 IEEE 12th   International Conference on. IEEE, 2009.   (http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf)\n",
            "- Local response normalization (LRN):\n",
            "- Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet   classification with deep convolutional neural networks.\" Advances in   neural information processing systems. 2012.   (http://machinelearning.wustl.edu/mlpapers/paperfiles/NIPS20120534.pdf)\n",
            "- Batch Normalization: This is more of an optimization topic.\n",
            "- Ioffe, Sergey, and Christian Szegedy. \"Batch normalization:   Accelerating deep network training by reducing internal covariate   shift.\" arXiv preprint arXiv:1502.03167 (2015).   (http://arxiv.org/abs/1502.03167/)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Architectures\n",
            "Heading: Architectures\n",
            "URL: #Architectures\n",
            "- We have seen a number of new operations: convolution, pooling, filters etc. How to best put these together to form a CNN is still an active area of research. In this section we summarize common patterns of usage in recent work based on (Karpathy, 2016).\n",
            "- The operations in convolutional networks are usually ordered into   several layers of convolution-bias-activation-pooling sequences.   Note that the convolution-bias-activation sequence is an efficient   way to implement the common neural net function$f(wx+b)$for a   locally connected and weight sharing hidden layer.\n",
            "- The convolutional layers are typically followed by a number of fully   connected layers that end with a softmax layer for prediction (if we   are training for a classification problem).\n",
            "- It is preferrable to have multiple convolution layers with small   filter sizes rather than a single layer with a large filter size.   Consider three convolutional layers with a filter size of   3x3. The units in the top layer have receptive fields of   size 7x7. Compare this with a single layer with a filter   size of 7x7. The three layer architecture has two   advantages: The units in the single layer network is restricted to   linear decision boundaries, whereas the three layer network can be   more expressive. Second, if we assume C channels, the parameter   tensor for the single layer network has size$[7,7,C,C]$whereas the   three layer network has three tensors of size$[3,3,C,C]$i.e. a   smaller number of parameters. The one disadvantage of the three   layer network is the extra storage required to store the   intermediate results for backpropagation.\n",
            "- Thus common settings for convolution use 3x3 filters withstride = padding = 1(which incidentally preserves the input   size). The one exception may be a larger filter size used in the   first layer which is applied to the image pixels. This will save   memory when the input is at its largest, and linear functions may be   sufficient to express the low level features at this stage.\n",
            "- The pooling operation may not be present in every layer. Keep in   mind that pooling destroys information and having several   convolutional layers without pooling may allow more complex features   to be learnt. When pooling is present it is best to keep the window   size small to minimize information loss. The common settings for   pooling arewindow = stride = 2, padding = 0, which halves the   input size in each dimension.\n",
            "- Beyond these general guidelines, you should look at the architectures used by successful models in the literature. Some examples are LeNet (LeCun et al. 1998), AlexNet (Krizhevsky et al. 2012), ZFNet (Zeiler and Fergus, 2013), GoogLeNet (Szegedy et al. 2014), VGGNet (Simonyan and Zisserman, 2014), and ResNet (He et al. 2015).\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Exercises\n",
            "Heading: Exercises\n",
            "URL: #Exercises\n",
            "- Design a filter that shifts a given image one pixel to right.\n",
            "- Design an image filter that has 0 output in regions of uniform   color, but nonzero output at edges where the color changes.\n",
            "- If your input consisted of two consecutive frames of video, how   would you detect motion using convolution?\n",
            "- Can you implement matrix-vector multiplication in terms of   convolution? How about matrix-matrix multiplication? Do you need   reshape operations?\n",
            "- Can you implement convolution in terms of matrix multiplication?\n",
            "- Can you implement elementwise broadcasting multiplication in terms   of convolution?\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#References\n",
            "Heading: References\n",
            "URL: #References\n",
            "- Some of this chapter was based on the excellent lecture notes from:http://cs231n.github.io/convolutional-networks\n",
            "- Christopher Olah's blog has very good visual explanations (thanks to   Melike Softa for the reference):http://colah.github.io/posts/2014-07-Conv-Nets-Modular\n",
            "- http://yosinski.com/deepvis\n",
            "- https://distill.pub/2017/feature-visualization/\n",
            "- https://distill.pub/2018/building-blocks/\n",
            "- UFLDL(or itsold   version)   is an online tutorial with programming examples and explicit   gradient derivations coveringconvolution,pooling,   andCNNs.\n",
            "- Hinton's video lecture and presentation at Coursera (Lec 5):https://d396qusza40orc.cloudfront.net/neuralnets/lecture_slides/lec5.pdf\n",
            "- For a derivation of gradients see:http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdforhttp://www.iro.umontreal.ca/~lisa/pointeurs/convolution.pdf\n",
            "- The CUDNN manual has more details about the convolution API:https://developer.nvidia.com/cudnn\n",
            "- http://deeplearning.net/tutorial/lenet.html\n",
            "- http://www.denizyuret.com/2014/04/on-emergence-of-visual-cortex-receptive.html\n",
            "- http://neuralnetworksanddeeplearning.com/chap6.html\n",
            "- http://www.deeplearningbook.org/contents/convnets.html\n",
            "- http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp\n",
            "- http://scs.ryerson.ca/~aharley/vis/conv/has a nice visualization   of an MNIST CNN. (Thanks to Fatih Ozhamaratli for the reference).\n",
            "- http://josephpcohen.com/w/visualizing-cnn-architectures-side-by-side-with-mxnetvisualizing popular CNN architectures side by side with mxnet.\n",
            "- http://cs231n.github.io/understanding-cnnvisualizing what   convnets learn.\n",
            "- https://arxiv.org/abs/1603.07285A guide to convolution arithmetic   for deep learning\n",
            "- Reading (architectures):cs231n Architecture Slides\n",
            "- Reading (visualization):cs231n Visualization Slides,cs231n Visualization Notes,Distillpub visualization article,Yosinski blog,video,paper,repo\n",
            "- A Simple Guide to the Versions of the Inception Network\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[<h2 id=\"Motivation\"><a class=\"docs-heading-anchor\" href=\"#Motivation\">Motivation</a><a id=\"Motivation-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Motivation\" title=\"Permalink\"></a></h2>, <h2 id=\"Architectures\"><a class=\"docs-heading-anchor\" href=\"#Architectures\">Architectures</a><a id=\"Architectures-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Architectures\" title=\"Permalink\"></a></h2>, <h2 id=\"RNN-vs-MLP\"><a class=\"docs-heading-anchor\" href=\"#RNN-vs-MLP\">RNN vs MLP</a><a id=\"RNN-vs-MLP-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#RNN-vs-MLP\" title=\"Permalink\"></a></h2>, <h2 id=\"Backpropagation-through-time\"><a class=\"docs-heading-anchor\" href=\"#Backpropagation-through-time\">Backpropagation through time</a><a id=\"Backpropagation-through-time-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Backpropagation-through-time\" title=\"Permalink\"></a></h2>, <h2 id=\"Vanishing-and-exploding-gradients\"><a class=\"docs-heading-anchor\" href=\"#Vanishing-and-exploding-gradients\">Vanishing and exploding gradients</a><a id=\"Vanishing-and-exploding-gradients-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Vanishing-and-exploding-gradients\" title=\"Permalink\"></a></h2>, <h2 id=\"LSTM-and-GRU\"><a class=\"docs-heading-anchor\" href=\"#LSTM-and-GRU\">LSTM and GRU</a><a id=\"LSTM-and-GRU-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#LSTM-and-GRU\" title=\"Permalink\"></a></h2>, <h2 id=\"Practical-issues\"><a class=\"docs-heading-anchor\" href=\"#Practical-issues\">Practical issues</a><a id=\"Practical-issues-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Practical-issues\" title=\"Permalink\"></a></h2>, <h2 id=\"Further-reading\"><a class=\"docs-heading-anchor\" href=\"#Further-reading\">Further reading</a><a id=\"Further-reading-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#Further-reading\" title=\"Permalink\"></a></h2>]\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Motivation\n",
            "Heading: Motivation\n",
            "URL: #Motivation\n",
            "- Recurrent neural networks (RNNs) are typically used in sequence processing applications such as natural language processing and generation. Some specific examples include:\n",
            "- Sequence classification:given a sequence input, produce a fixed sized output, e.g. determine the \"sentiment\" of a product review.\n",
            "- Sequence generation:given a fixed sized input, produce a sequence output, e.g. automatic image captioning.\n",
            "- Sequence tagging:given a sequence, produce a label for each token, e.g. part-of-speech tagging.\n",
            "- Sequence-to-sequence mapping:given a sequence, produce another, not necessarily parallel, sequence. e.g. machine translation, speech recognition.\n",
            "- All feed-forward models we have seen so far (Linear, MLP, CNN) have a common limitation: They are memoryless, i.e. they apply the same computational steps to each instance without any memory of previous instances. Each output is obtained from the current input and model parameters using a fixed number of common operations:\n",
            "- \\[\\hat{y}_t = f(x_t,w)\\]\n",
            "- A model with no memory is difficult to apply to variable sized inputs and outputs with nontrivial dependencies.  Let us take sequence tagging as an example problem.  To apply a feed-forward model to a sequence, one option is to treat each token of the sequence as an individual input:\n",
            "- \n",
            "- Applying the same computation to each input token makes sense only if the different input-output pairs are IID (independent and identically distributed).  However the IID assumption is violated in typical sequence processing applications like language modeling and speech recognition where the output of one time step may depend on the inputs and outputs from other time steps.\n",
            "- Another option is to treat the whole sequence as a single input:\n",
            "- \n",
            "- The first problem with this approach is that the inputs are of varying length.  We could potentially address this issue using a convolutional architecture, and this is a viable alternative for sequence classification problems.  However we have a more serious problem with variable length outputs: The space of possible outputs grow exponentially with length and output tokens have possible dependencies between them.  Problems of this type are known as \"structured prediction\", see (Smith 2011) for a good introduction. It is not clear how to generate and score variable sized outputs in a single shot with a single feed-forward model.\n",
            "- Finally we can generate each output token separately, but take a fixed sized window around the corresponding input token to take into account more context:\n",
            "- \n",
            "- This is the approach taken by, e.g. n-gram language models, and Bengio's MLP language model.  The problem with this approach is that we don't know how large the window needs to be.  In fact different tokens may require different sized windows, e.g. long range dependencies between words in a sentence.  RNNs provide a more elegant solution.\n",
            "- RNNs process the input sequence one token at a time.  However, each output is not only a function of the current input, but some internal state determined by previous time steps:\n",
            "- \\[\\langle\\hat{y}_t,h_t\\rangle = f(x_t,w,h_{t-1})\\]\n",
            "- \n",
            "- The state $h_t$ can be thought of as analogous to a memory device storing variables in a computer program.  In fact, RNNs have been proven to be Turing complete machines (however see this and this for a discussion).  At each time step, the RNN processes the current input $x_t$ using the \"program\" specified by parameters $w$ and the internal \"variables\" specified by $h_{t-1}$.  The program stores new values in its internal variables with $h_t$ and possibly produces an output $\\hat{y}_t$.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Architectures\n",
            "Heading: Architectures\n",
            "URL: #Architectures\n",
            "- Depending on the type of problem, we can deploy an RNNs with architectures other than the tagger architecture we saw above.  Some examples are:\n",
            "- Sequence classification\n",
            "- \n",
            "- Sequence generation\n",
            "- \n",
            "- Sequence to sequence mapping models which combine the previous two architectures. The input sequence is processed by an encoder RNN (E), and the output sequence is generated by a decoder RNN (D). Information is passed from the encoder to the decoder through the initial hidden state, or an extra input, or an attention mechanism.\n",
            "- \n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#RNN-vs-MLP\n",
            "Heading: RNN vs MLP\n",
            "URL: #RNN-vs-MLP\n",
            "- For comparison here is the code for MLP with one hidden layer vs. the code for a comparable RNN. \n",
            " - Code Block: \n",
            " [ function mlp1(w,x)\n",
            "    h = tanh(w[1]*x .+ w[2])\n",
            "    y = w[3]*h .+ w[4]\n",
            "    return y\n",
            "end\n",
            "\n",
            "function rnn1(w,x,h)\n",
            "    h = tanh(w[1]*vcat(x,h) .+ w[2])\n",
            "    y = w[3]*h .+ w[4]\n",
            "    return (y,h)\n",
            "end ]\n",
            "- Note two crucial differences: First, RNN takes h, the hidden state from the previous time step, in addition to the regular input x. Second, it returns the new value of h in addition to the regular output y.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Backpropagation-through-time\n",
            "Heading: Backpropagation through time\n",
            "URL: #Backpropagation-through-time\n",
            "- RNNs can be trained using the same gradient based optimization algorithms we use for feed-foward networks. This is best illustrated with a picture of an RNN unrolled in time:\n",
            "-  (image source)\n",
            "- The picture on the left depicts an RNN influencing its own hidden state A while computing its output h for a single time step.  The equivalent picture on the right shows each time step as a separate column with its own input, state and output.  We need to keep in mind that the function that goes from the input and the previous state to the output and the next state is identical at each time step.  Viewed this way, there are no cycles in the computation graph and we can treat the RNN as just a multi-layer feed-forward net which (i) has as many layers as time steps, (ii) has weights shared between different layers, and (iii) may have multiple inputs and outputs received and produced at individual layers.\n",
            "- Backpropagation through time (BPTT) is the SGD algorithm applied to RNNs unrolled in time.  First, the RNN is run and its outputs are collected for the whole sequence.  Then the losses for all outputs are calculated and summed.  Finally the backward pass goes over the computational graph for the whole sequence, accumulating the gradients of each parameter coming from different time steps.\n",
            "- In practice, with Knet, all we have to do is to write a loss function that computes the total loss for the whole sequence and use its grad(f) for training.  Here is an example for a sequence tagger:\n",
            " - Code Block: \n",
            " [ function rnnloss(param,state,inputs,outputs)\n",
            "    # inputs and outputs are sequences of the same length\n",
            "    sumloss = 0\n",
            "    for t in 1:length(inputs)\n",
            "        prediction,state = rnn1(param,inputs[t],state)\n",
            "        sumloss += cross_entropy_loss(prediction,outputs[t])\n",
            "    end\n",
            "    return sumloss\n",
            "end\n",
            "\n",
            "rnngrad = grad(rnnloss)\n",
            "\n",
            "# train with our usual SGD procedure ]\n",
            "-  \n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Vanishing-and-exploding-gradients\n",
            "Heading: Vanishing and exploding gradients\n",
            "URL: #Vanishing-and-exploding-gradients\n",
            "- RNNs can be difficult to train because gradients passed back through many layers may vanish or explode. To see why, let us first look at the evolution of the hidden state during the forward pass of an RNN. We will ignore the input and the bias for simplicity:\n",
            " - Code Block: \n",
            " [ h[t+1] = tanh(W*h[t]) = tanh(W*tanh(W*h[t-1])) = ... ]\n",
            "- No matter how many layers we go through, the forward h values will remain in the [-1,1] range because of the squashing tanh function, no problems here.  However, look at what happens in the backward pass:\n",
            " - Code Block: \n",
            " [ dh[t] = W' * (dh[t+1] .* f(h[t+1])) ]\n",
            "- where dh[t] is the gradient of the loss with respect to h[t] and f is some elementwise function whose outputs are in the [-1,1] range (in the case of tanh, f(x)=(1+x)*(1-x)). The important thing to notice is that the dh gradients keep getting multiplied by the same matrix W' over and over again as we move backward, and the backward pass is linear, i.e. there is no squashing function.\n",
            "- What happens if we keep multiplying a vector $u$ with the same matrix over and over again?  Suppose the matrix has an eigendecomposition $V\\Lambda V^{-1}$.  After n multiplications in effect we will have multiplied with $V\\Lambda^n V^{-1}$ where $\\Lambda$ is a diagonal matrix of eigenvalues. The components of the gradient corresponding to eigenvalues greater than 1 will grow without a bound and the components for eigenvalues less than 1 will shrink towards zero. The gradient entries that grow without a bound destabilize SGD, and the ones that shrink to zero pass no information about the error back to the parameters.\n",
            "- There are several possible solutions to these problems:\n",
            "- Initialize the weights to avoid eigenvalues that are too large or too small. Even initializing the weights from a model successfully trained on some other task may help start them in the right regime.\n",
            "- Use gradient clipping: this is the practice of downscaling gradients if their norm is above a certain threshold to help stabilize SGD.\n",
            "- Use better optimization algorithms: methods like Adam and Adagrad adjust the learning rate for each parameter based on the history of updates and may be less sensitive to vanishing and exploding gradients.\n",
            "- Use RNN modules designed to preserve long range information: modules such as LSTM and GRU are designed to help information flow better across time steps and are detailed in the next section.\n",
            "-     \n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#LSTM-and-GRU\n",
            "Heading: LSTM and GRU\n",
            "URL: #LSTM-and-GRU\n",
            "- \n",
            "- The Long Short Term Memory (LSTM) and the Gated Recurrent Unit (GRU) are two of the modules designed as building blocks for RNNs to address vanishing gradients and better learn long term dependencies. These units replace the simple tanh unit used in rnn1.\n",
            "- ... To be continued\n",
            " - Code Block: \n",
            " [ function lstm(weight,bias,hidden,cell,input)\n",
            "    gates   = hcat(input,hidden) * weight .+ bias\n",
            "    h       = size(hidden,2)\n",
            "    forget  = sigm(gates[:,1:h])\n",
            "    ingate  = sigm(gates[:,1+h:2h])\n",
            "    outgate = sigm(gates[:,1+2h:3h])\n",
            "    change  = tanh(gates[:,1+3h:end])\n",
            "    cell    = cell .* forget + ingate .* change\n",
            "    hidden  = outgate .* tanh(cell)\n",
            "    return (hidden,cell)\n",
            "end ]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Practical-issues\n",
            "Heading: Practical issues\n",
            "URL: #Practical-issues\n",
            "- input and output (word Embedding and prediction) layers\n",
            "- decoding and generating: greedy, beam, stochastic.\n",
            "- minibatching\n",
            "- (Advanced topics)\n",
            "- (multilayer DL 10.5)\n",
            "- (bidirectional)\n",
            "- (attention: http://distill.pub/2016/augmented-rnns/)\n",
            "- (speech, handwriting, mt)\n",
            "- (image captioning, vqa)\n",
            "- (ntm, memory networks: (DL 10.12) http://distill.pub/2016/augmented-rnns/)\n",
            "- (2D rnns: graves chap 8. DL end of 10.3.)\n",
            "- (recursive nets? DL 10.6)\n",
            "- (different length input/output sequences: graves a chapter 7 on ctc, chap 6 on hmm hybrids., olah and carter on adaptive computation time. DL 10.4 on s2s.)\n",
            "- (comparison to LDS and HMM Hinton)\n",
            "- (discussion of teacher forcing and its potential problems DL 10.2.1)\n",
            "- (echo state networks DL 10.8 just fix the h->h weights.)\n",
            "- (skip connections in time, leaky units DL 10.9)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Further-reading\n",
            "Heading: Further reading\n",
            "URL: #Further-reading\n",
            "- Karpathy 2015.The Unreasonable Effectiveness of Recurrent Neural Networks.\n",
            "- Olah 2015.Understanding LSTMs.\n",
            "- Hinton 2012.RNN lecture slides.\n",
            "- Olah and Carter 2016.Attention and Augmented Recurrent Neural Networks.\n",
            "- Goodfellow 2016.Deep Learning, Chapter 10. Sequence modeling: recurrent and recursive nets.\n",
            "- Graves 2012., Supervised Sequence Labelling with Recurrent Neural Networks (textbook)\n",
            "- Britz 2015.Recurrent neural networks tutorial.\n",
            "- Manning and Socher 2017.CS224n: Natural Language Processing with Deep Learning.\n",
            "- Wikipedia.Recurrent neural network.\n",
            "- Orr 1999.RNN lecture notes.\n",
            "- Le et al. 2015.A simple way to initialize recurrent networks of rectified linear units\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[<h2 id=\"References\"><a class=\"docs-heading-anchor\" href=\"#References\">References</a><a id=\"References-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#References\" title=\"Permalink\"></a></h2>]\n",
            "https://denizyuret.github.io/Knet.jl/stable/rl/#Reinforcement-Learning#References\n",
            "Heading: References\n",
            "URL: #References\n",
            "- http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n",
            "- https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT\n",
            "- http://videolectures.net/rldm2015_silver_reinforcement_learning/?q=david%20silver\n",
            "- https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html\n",
            "- http://web.mit.edu/dimitrib/www/RLbook.html\n",
            "- https://sites.ualberta.ca/~szepesva/RLBook.html\n",
            "- http://banditalgs.com/print/\n",
            "- http://karpathy.github.io/2016/05/31/rl/\n",
            "- http://cs229.stanford.edu/notes/cs229-notes12.pdf\n",
            "- http://cs.stanford.edu/people/karpathy/reinforcejs/index.html\n",
            "- https://www.udacity.com/course/machine-learning-reinforcement-learning–ud820\n",
            "- http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html\n",
            "- http://people.csail.mit.edu/regina/my_papers/TG15.pdf\n",
            "- Inhttp://karpathy.github.io/2015/05/21/rnn-effectiveness: For   more about REINFORCE and more generally Reinforcement Learning and   policy gradient methods (which REINFORCE is a special case of) David   Silver's class, or one of Pieter Abbeel's classes. This is very much   ongoing work but these hard attention models have been explored, for   example, in Inferring Algorithmic Patterns with Stack-Augmented   Recurrent Nets, Reinforcement Learning Neural Turing Machines, and   Show Attend and Tell.\n",
            "- Inhttp://www.deeplearningbook.org/contents/ml.html: Please see   Sutton and Barto (1998) or Bertsekasand Tsitsiklis (1996) for   information about reinforcement learning, and Mnih et al.(2013) for   the deep learning approach to reinforcement learning.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[<h2 id=\"References\"><a class=\"docs-heading-anchor\" href=\"#References\">References</a><a id=\"References-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#References\" title=\"Permalink\"></a></h2>]\n",
            "https://denizyuret.github.io/Knet.jl/stable/opt/#Optimization#References\n",
            "Heading: References\n",
            "URL: #References\n",
            "- http://videolectures.net/deeplearning2015_goodfellow_network_optimization/(Ian Goodfellow's tutorial on neural network optimization at Deep   Learning Summer School 2015).\n",
            "- http://int8.io/comparison-of-optimization-techniques-stochastic-gradient-descent-momentum-adagrad-and-adadelta(implementation and comparison of popular methods)\n",
            "- http://www.deeplearningbook.org/contents/numerical.html(basic   intro in 4.3)\n",
            "- http://www.deeplearningbook.org/contents/optimization.html(8.1   generalization, 8.2 problems, 8.3 algorithms, 8.4 init, 8.5 adaptive   lr, 8.6 approx 2nd order, 8.7 meta)\n",
            "- https://arxiv.org/abs/1412.6544\n",
            "- http://andrew.gibiansky.com/blog/machine-learning/gauss-newton-matrix/(great posts on optimization)\n",
            "- https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf(excellent tutorial on cg, gd, eigens etc)\n",
            "- http://arxiv.org/abs/1412.6544(Goodfellow paper)\n",
            "- https://d396qusza40orc.cloudfront.net/neuralnets/lecture_slides/lec6.pdf(hinton slides)\n",
            "- https://d396qusza40orc.cloudfront.net/neuralnets/lecture_slides/lec8.pdf(hinton slides)\n",
            "- http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html\n",
            "- http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_Martens10.pdf\n",
            "- http://arxiv.org/abs/1503.05671\n",
            "- http://arxiv.org/abs/1412.1193\n",
            "- http://www.springer.com/us/book/9780387303031(nocedal and wright)\n",
            "- http://www.nrbook.com(numerical recipes)\n",
            "- https://maths-people.anu.edu.au/~brent/pub/pub011.html(without   derivatives)\n",
            "- http://stanford.edu/~boyd/cvxbook/(only convex optimization)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[<h2 id=\"References\"><a class=\"docs-heading-anchor\" href=\"#References\">References</a><a id=\"References-1\"></a><a class=\"docs-heading-anchor-permalink\" href=\"#References\" title=\"Permalink\"></a></h2>]\n",
            "https://denizyuret.github.io/Knet.jl/stable/gen/#Generalization#References\n",
            "Heading: References\n",
            "URL: #References\n",
            "- http://www.deeplearningbook.org/contents/regularization.html\n",
            "- https://d396qusza40orc.cloudfront.net/neuralnets/lecture_slides/lec9.pdf\n",
            "- https://d396qusza40orc.cloudfront.net/neuralnets/lecture_slides/lec10.pdf\n",
            "- http://blog.cambridgecoding.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ROOT_SRC = \"https://denizyuret.github.io/Knet.jl/stable/\"\n",
        "# response = requests.get(ROOT_SRC)\n",
        "# bsoup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "html_content1 = \"https://denizyuret.github.io/Knet.jl/stable/reference/\"\n",
        "html_content2 = \"https://denizyuret.github.io/Knet.jl/stable/install/\"\n",
        "html_content3 = \"https://denizyuret.github.io/Knet.jl/stable/tutorial/\"\n",
        "\n",
        "html_content4 = ['backprop/#Backpropagation-and-SGD',\n",
        "                  'softmax/#Softmax-Classification',\n",
        "                  'mlp/#Multilayer-Perceptrons',\n",
        "                  'cnn/#Convolutional-Neural-Networks',\n",
        "                  'rnn/#Recurrent-Neural-Networks',\n",
        "                  'rl/#Reinforcement-Learning',\n",
        "                  'opt/#Optimization',\n",
        "                  'gen/#Generalization']\n",
        "\n",
        "html_content4 = [ROOT_SRC+d for d in html_content4]\n",
        "\n",
        "\n",
        "html_contents = [html_content1,\n",
        "                 html_content2,\n",
        "                 html_content3\n",
        "                 ]\n",
        "\n",
        "html_contents.extend(html_content4)\n",
        "\n",
        "texts_all = []\n",
        "\n",
        "for html_content in html_contents:\n",
        "    response_all = requests.get(html_content)\n",
        "\n",
        "    soup = BeautifulSoup(response_all.text, 'html.parser')\n",
        "\n",
        "    h2_elements = soup.find_all('h2')\n",
        "\n",
        "    print(h2_elements)\n",
        "\n",
        "    # Iterate through each h2 element\n",
        "    for h2 in h2_elements:\n",
        "        print(html_content + h2.find('a').get('href'))\n",
        "        # print(\"-\"*100)\n",
        "        print(\"Heading:\", h2.text.strip())\n",
        "        print(\"URL:\",h2.find('a').get('href'))\n",
        "        # print(\"-\"*50)\n",
        "\n",
        "        # find all elements that come after this h2 until the next h2\n",
        "        next_element = h2.find_next_sibling()\n",
        "\n",
        "        dict_sub_link = {}\n",
        "        sub_text = f\"\"\n",
        "\n",
        "        while next_element and next_element.name != 'h2':\n",
        "            if next_element.name == 'article' and 'docstring' in next_element.get('class', []):\n",
        "\n",
        "                # handle article elements with class name of \"docstring\"\n",
        "                pre_elements = next_element.find_all(['pre','p', 'ul','header'])\n",
        "                for pre in pre_elements:\n",
        "                    if pre.name == \"header\":\n",
        "                      # print()\n",
        "                      print(\"Reference: \", pre.get_text())\n",
        "                      sub_text += \"Reference: \"+ pre.get_text()\n",
        "\n",
        "                    if pre.name == \"pre\":\n",
        "                      print(\" - Code Block: \\n [\", pre.get_text() ,\"]\")\n",
        "                      sub_text += \"Code Block: \\n [ \"+ pre.get_text() +\" ]\"\n",
        "\n",
        "                    if pre.name == \"p\" and pre.find_parent('li') is None:\n",
        "                      print(\"-\", pre.get_text())\n",
        "                      sub_text += pre.get_text()\n",
        "\n",
        "                    if pre.name == \"ul\":\n",
        "                      sub_list_items = get_sub_list_items(pre)\n",
        "                      for item in sub_list_items:\n",
        "                          print(\"-\", item)\n",
        "                          sub_text += \"- \"+ item\n",
        "\n",
        "            elif next_element.name == 'div' and 'admonition' in next_element.get('class', []):\n",
        "                pre_elements = next_element.find_all('pre')\n",
        "                for pre in pre_elements:\n",
        "                    print(\" - Code Block: \\n [\", pre.get_text() ,\"]\")\n",
        "                    sub_text += \"Code Block: \\n [ \"+ pre.get_text() +\" ]\"\n",
        "\n",
        "                p_elements = next_element.find_all('p')\n",
        "                for p in p_elements:\n",
        "                    print(\" -\", p.get_text(strip=True))\n",
        "                    sub_text += p.get_text(strip=True)\n",
        "\n",
        "            if next_element.name in [\"p\",\"ol\",\"ul\",\"pre\"]:\n",
        "                if next_element.name == \"pre\":\n",
        "                      print(\" - Code Block: \\n [\", next_element.get_text() ,\"]\")\n",
        "                      sub_text += \"Code Block: \\n [ \"+ next_element.get_text() +\" ]\"\n",
        "\n",
        "                elif next_element.name == \"p\":\n",
        "                      print(\"-\", next_element.get_text())\n",
        "                      sub_text += next_element.get_text()\n",
        "\n",
        "                elif next_element.name == \"ol\":\n",
        "                      sub_list_items = get_sub_list_items2(next_element)\n",
        "                      for item in sub_list_items:\n",
        "                          print(\"-\", item)\n",
        "                          sub_text += \"- \"+ item\n",
        "\n",
        "                elif next_element.name == \"ul\":\n",
        "                      sub_list_items = get_sub_list_items(next_element)\n",
        "                      for item in sub_list_items:\n",
        "                          print(\"-\", item)\n",
        "                          sub_text += \"- \"+ item\n",
        "\n",
        "\n",
        "            next_element = next_element.find_next_sibling()\n",
        "\n",
        "        dict_sub_link = {\"title\": h2.text.strip(),\n",
        "                          \"url\": html_content + h2.find('a').get('href'),\n",
        "                          \"p_content\": sub_text\n",
        "                          }\n",
        "        texts_all.append(dict_sub_link)\n",
        "        print(\"-\"*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY6xhm2dWukz",
        "outputId": "52553f4d-caa7-4ef8-ac64-598024d8323a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "execution_count": 238,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(texts_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPMhVnWt0Pz4",
        "outputId": "652c80e9-e8ae-477d-bd1c-e404fc9dcb45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': 'Tips for developers',\n",
              " 'url': 'https://denizyuret.github.io/Knet.jl/stable/install/#Tips-for-developers',\n",
              " 'p_content': 'Knet is an open-source project and we are always open to new contributions: bug fixes, new machine learning models and operators, inspiring examples, benchmarking results are all welcome. If you\\'d like to contribute to the code base, please sign up at the knet-dev mailing list and follow these tips:- Please get an account atgithub.com.- ForktheKnet   repository.- Point Julia to your fork withusing Pkg; pkg\"dev git@github.com:your-username/Knet.jl.git\".- Make sure yourfork is   up-to-date.- Retrieve the latest version of the master branch usinggit pullin the Knet directory.- Implement your contribution.  This typically involves:Creating a git branch.Writing your code.Adding documentation under doc/src and a summary in NEWS.md.Adding unit tests in the test directory and usingPkg.test(\"Knet\").- Creating a git branch.- Writing your code.- Adding documentation under doc/src and a summary in NEWS.md.- Adding unit tests in the test directory and usingPkg.test(\"Knet\").- Please submit your contribution using apull   request.'}"
            ]
          },
          "execution_count": 239,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts_all[16]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tqMPq8DXUu4",
        "outputId": "2a12a752-3383-4906-9713-40e48b1d41ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://denizyuret.github.io/Knet.jl/stable/reference/#AutoGrad\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#KnetArray\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#File-I/O\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Parameter-initialization\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Activation-functions\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Loss-functions\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Convolution-and-Pooling\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Recurrent-neural-networks\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Batch-Normalization\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Model-optimization\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Hyperparameter-optimization\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Utilities\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#AutoGrad-(advanced)\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Per-parameter-optimization-(advanced)\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Function-Index\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Installation\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Tips-for-developers\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Using-Amazon-AWS\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Using-Microsoft-Azure\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Using-Ubuntu18.04\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Summary\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Philosophy\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Tutorial\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Benchmarks\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Under-the-hood\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Supervised-learning\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Partial-derivatives\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Chain-rule-and-backpropagation\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Multiple-dimensions\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Multiple-instances\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Stochastic-Gradient-Descent\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Housing-Example\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Problems-with-SGD\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#References\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Notes\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Classification\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Likelihood\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Softmax\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#One-hot-vectors\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Gradient-of-log-likelihood\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#MNIST-example\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Representational-power\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#References\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Stacking-linear-classifiers-is-useless\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Introducing-nonlinearities\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Types-of-nonlinearities-(activation-functions)\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Representational-power\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Matrix-vs-Neuron-Pictures\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Programming-Example\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#References\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Motivation\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Convolution\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Pooling\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Normalization\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Architectures\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Exercises\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#References\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Motivation\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Architectures\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#RNN-vs-MLP\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Backpropagation-through-time\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Vanishing-and-exploding-gradients\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#LSTM-and-GRU\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Practical-issues\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Further-reading\n",
            "https://denizyuret.github.io/Knet.jl/stable/rl/#Reinforcement-Learning#References\n",
            "https://denizyuret.github.io/Knet.jl/stable/opt/#Optimization#References\n",
            "https://denizyuret.github.io/Knet.jl/stable/gen/#Generalization#References\n"
          ]
        }
      ],
      "source": [
        "for i in texts_all:\n",
        "    print(i.get(\"url\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n6_O5Opj7PS",
        "outputId": "277f2dc2-52de-4e2e-85e4-d661102894e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://denizyuret.github.io/Knet.jl/stable/reference/#AutoGrad\n",
            "--------------------\n",
            "Reference: Knet.AutoGrad — ModuleUsage:Code Block: \n",
            " [ x = Param([1,2,3])          # The user declar\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#KnetArray\n",
            "--------------------\n",
            "Reference: Knet.KnetArrays.KnetArray — TypeCode Block: \n",
            " [ KnetArray{T}(undef,dims)\n",
            "KnetArray(a::Abs\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#File-I/O\n",
            "--------------------\n",
            "Missing docstring forKnet.save. Check Documenter's build log for details.Missing docstring forKnet.l\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Parameter-initialization\n",
            "--------------------\n",
            "Reference: Knet.Train20.param — FunctionCode Block: \n",
            " [ param(array; atype)\n",
            "param(dims...; init, aty\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Activation-functions\n",
            "--------------------\n",
            "Reference: Knet.Ops20.elu — FunctionCode Block: \n",
            " [ elu(x) ]Return (x > 0 ? x : exp(x)-1).Reference:\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Loss-functions\n",
            "--------------------\n",
            "Reference: Knet.Ops20.accuracy — FunctionCode Block: \n",
            " [ accuracy(scores, labels; dims=1, average=tr\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Convolution-and-Pooling\n",
            "--------------------\n",
            "Reference: Knet.Ops20.conv4 — FunctionCode Block: \n",
            " [ conv4(w, x; kwargs...) ]Execute convolutions o\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Recurrent-neural-networks\n",
            "--------------------\n",
            "Reference: Knet.Ops20.RNN — TypeCode Block: \n",
            " [ rnn = RNN(inputSize, hiddenSize; opts...)\n",
            "rnn(x; bat\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Batch-Normalization\n",
            "--------------------\n",
            "Reference: Knet.Ops20.batchnorm — FunctionCode Block: \n",
            " [ batchnorm(x[, moments, params]; kwargs...)\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Model-optimization\n",
            "--------------------\n",
            "Reference: Knet.Train20.minimize — FunctionCode Block: \n",
            " [ minimize(func, data, optimizer=Adam(); pa\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Hyperparameter-optimization\n",
            "--------------------\n",
            "Reference: Knet.Train20.goldensection — FunctionCode Block: \n",
            " [ goldensection(f,n;kwargs) => (fmin,x\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Utilities\n",
            "--------------------\n",
            "Reference: Knet.Ops20.bmm — FunctionCode Block: \n",
            " [ bmm(A, B ; transA=false, transB=false) ]Perform \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#AutoGrad-(advanced)\n",
            "--------------------\n",
            "Reference: AutoGrad.@gcheck — MacroCode Block: \n",
            " [ gcheck(f, x...; kw, o...)\n",
            "@gcheck f(x...; kw...) \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Per-parameter-optimization-(advanced)\n",
            "--------------------\n",
            "The model optimization methods apply the same algorithm with the same configuration to every paramet\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/reference/#Function-Index\n",
            "--------------------\n",
            "- Knet.AutoGrad- Knet.KnetArrays.KnetArray- Knet.Ops20.RNN- Knet.Train20.Adadelta- Knet.Train20.Adag\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Installation\n",
            "--------------------\n",
            "For best results install (1) Julia, (2) CUDA.jl, (3) Knet.jl in that order. Step (2) can be skipped \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Tips-for-developers\n",
            "--------------------\n",
            "Knet is an open-source project and we are always open to new contributions: bug fixes, new machine l\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Using-Amazon-AWS\n",
            "--------------------\n",
            "If you don't have access to a GPU machine, but would like to experiment with one, Amazon Web Service\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Using-Microsoft-Azure\n",
            "--------------------\n",
            "Knet can be used with Azure. For GPU support, you need to create a virtual machine with GPU, for ins\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/install/#Using-Ubuntu18.04\n",
            "--------------------\n",
            "The CUDA stack can be installed using the following instructions:Code Block: \n",
            " [ ###################\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Summary\n",
            "--------------------\n",
            "Knet (pronounced \"kay-net\") is the Koç University deep learning framework implemented in Julia by De\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Philosophy\n",
            "--------------------\n",
            "Knet uses dynamic computational graphs generated at runtime for automatic differentiation of (almost\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Tutorial\n",
            "--------------------\n",
            "The Knet tutorial consists of Jupyter notebooks that introduce the programming language Julia and th\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Benchmarks\n",
            "--------------------\n",
            "Each of the examples above was used as a benchmark to compare Knet with other frameworks. The table \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/tutorial/#Under-the-hood\n",
            "--------------------\n",
            "Knet relies on the AutoGrad package and the KnetArray data type for its functionality and performanc\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Supervised-learning\n",
            "--------------------\n",
            "Arthur Samuel, the author of the first self-learning checkers program, defined machine learning as a\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Partial-derivatives\n",
            "--------------------\n",
            "When we have a function with a scalar output, we can look at how its value changes in response to a \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Chain-rule-and-backpropagation\n",
            "--------------------\n",
            "The chain rule allows us to calculate partial derivatives in terms of other partial derivatives, sim\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Multiple-dimensions\n",
            "--------------------\n",
            "Let's look at the case where the input and output are not scalars but vectors. In particular assume \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Multiple-instances\n",
            "--------------------\n",
            "We will typically process data multiple instances at a time for efficiency. Thus, the input $x$ will\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Stochastic-Gradient-Descent\n",
            "--------------------\n",
            "The gradients calculated by backprop, $\\nabla_w J$ and $\\nabla_b J$, tell us how much small changes \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Housing-Example\n",
            "--------------------\n",
            "We will use the Boston Housing dataset from the UCI Machine Learning Repository to train a linear re\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Problems-with-SGD\n",
            "--------------------\n",
            "Over the years, people have noted many subtle problems with the SGD algorithm and suggested improvem\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#References\n",
            "--------------------\n",
            "- UFLDL Tutorial, Linear Regression- cs231n Optimization Notes- cs229 Convex optimization overview,P\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/backprop/#Backpropagation-and-SGD#Notes\n",
            "--------------------\n",
            "- Supervised learning is also known asregressionif the outputs are numeric andclassificationif they \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Classification\n",
            "--------------------\n",
            "Classification problems are supervised machine learning problems where the task is to predict a disc\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Likelihood\n",
            "--------------------\n",
            "A natural objective in classification could be to minimize the number of misclassified examples in t\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Softmax\n",
            "--------------------\n",
            "A classification model for a problem with $C$ classes typically generates $y\\in\\mathbb{R}^C$, a vect\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#One-hot-vectors\n",
            "--------------------\n",
            "When using a probabilistic classifier, it is convenient to represent the desired output as a one-hot\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Gradient-of-log-likelihood\n",
            "--------------------\n",
            "To compute the gradient for log likelihood, we need to make the normalization of $\\hat{p}$ explicit:\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#MNIST-example\n",
            "--------------------\n",
            "Let's try our softmax classifier on the MNIST handwritten digit classification dataset. Here are the\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#Representational-power\n",
            "--------------------\n",
            "So far we have seen how to create a machine learning model as a differentiable program (linear regre\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/softmax/#Softmax-Classification#References\n",
            "--------------------\n",
            "- UFLDL Tutorial, Softmax Regression\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Stacking-linear-classifiers-is-useless\n",
            "--------------------\n",
            "We could try stacking multiple linear classifiers together. Here is a two layer model:Code Block: \n",
            " \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Introducing-nonlinearities\n",
            "--------------------\n",
            "Here is a slightly modified version of the two layer model:Code Block: \n",
            " [ function mlp(w, x, ygold)\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Types-of-nonlinearities-(activation-functions)\n",
            "--------------------\n",
            "The functions we throw between linear layers to break the linearity are called nonlinearities or act\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Representational-power\n",
            "--------------------\n",
            "You might be wondering whether relu had any special properties or would any of the other nonlinearit\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Matrix-vs-Neuron-Pictures\n",
            "--------------------\n",
            "So far we have introduced multilayer perceptrons (aka artificial neural networks) using matrix opera\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Programming-Example\n",
            "--------------------\n",
            "In this section we introduce several Knet features that make it easier to define complex models. As \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#References\n",
            "--------------------\n",
            "- http://neuralnetworksanddeeplearning.com/chap4.html- http://www.deeplearningbook.org/contents/mlp.\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Motivation\n",
            "--------------------\n",
            "Let's say we are trying to build a model that will detect cats in photographs. The average resolutio\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Convolution\n",
            "--------------------\n",
            "Convolution is the main operation that provides sparse connectivity and weight sharing.  For simplic\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Pooling\n",
            "--------------------\n",
            "It is common practice to use pooling (aka subsampling) layers in between convolution operations in C\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Normalization\n",
            "--------------------\n",
            "Draft...Karpathy says: \"Many types of normalization layers have been proposed for use in ConvNet arc\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Architectures\n",
            "--------------------\n",
            "We have seen a number of new operations: convolution, pooling, filters etc. How to best put these to\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#Exercises\n",
            "--------------------\n",
            "- Design a filter that shifts a given image one pixel to right.- Design an image filter that has 0 o\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/cnn/#Convolutional-Neural-Networks#References\n",
            "--------------------\n",
            "- Some of this chapter was based on the excellent lecture notes from:http://cs231n.github.io/convolu\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Motivation\n",
            "--------------------\n",
            "Recurrent neural networks (RNNs) are typically used in sequence processing applications such as natu\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Architectures\n",
            "--------------------\n",
            "Depending on the type of problem, we can deploy an RNNs with architectures other than the tagger arc\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#RNN-vs-MLP\n",
            "--------------------\n",
            "For comparison here is the code for MLP with one hidden layer vs. the code for a comparable RNN. Cod\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Backpropagation-through-time\n",
            "--------------------\n",
            "RNNs can be trained using the same gradient based optimization algorithms we use for feed-foward net\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Vanishing-and-exploding-gradients\n",
            "--------------------\n",
            "RNNs can be difficult to train because gradients passed back through many layers may vanish or explo\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#LSTM-and-GRU\n",
            "--------------------\n",
            "The Long Short Term Memory (LSTM) and the Gated Recurrent Unit (GRU) are two of the modules designed\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Practical-issues\n",
            "--------------------\n",
            "- input and output (word Embedding and prediction) layers- decoding and generating: greedy, beam, st\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rnn/#Recurrent-Neural-Networks#Further-reading\n",
            "--------------------\n",
            "- Karpathy 2015.The Unreasonable Effectiveness of Recurrent Neural Networks.- Olah 2015.Understandin\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/rl/#Reinforcement-Learning#References\n",
            "--------------------\n",
            "- http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html- https://www.youtube.com/watch?v=2pWv7GO\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/opt/#Optimization#References\n",
            "--------------------\n",
            "- http://videolectures.net/deeplearning2015_goodfellow_network_optimization/(Ian Goodfellow's tutori\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "https://denizyuret.github.io/Knet.jl/stable/gen/#Generalization#References\n",
            "--------------------\n",
            "- http://www.deeplearningbook.org/contents/regularization.html- https://d396qusza40orc.cloudfront.ne\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for i in texts_all:\n",
        "    print(i.get(\"url\"))\n",
        "    print(\"-\"*20)\n",
        "    print(i.get(\"p_content\")[:100])\n",
        "    print(\"-\"*150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_jgzUFu8VMa"
      },
      "source": [
        "## Save as CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Eh1pclOy4iYi",
        "outputId": "6e3d88c4-d05c-4ebd-bfb8-0d790961f729"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 68,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 59,\n        \"samples\": [\n          \"AutoGrad\",\n          \"Loss functions\",\n          \"Notes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 68,\n        \"samples\": [\n          \"https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Representational-power\",\n          \"https://denizyuret.github.io/Knet.jl/stable/install/#Tips-for-developers\",\n          \"https://denizyuret.github.io/Knet.jl/stable/reference/#Activation-functions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p_content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 68,\n        \"samples\": [\n          \"You might be wondering whether relu had any special properties or would any of the other nonlinearities be sufficient. Another question is whether there are functions multilayer perceptrons cannot represent and if so whether adding more layers or different types of functions would increase their representational power. The short answer is that a two layer model can approximate any function if the hidden layer is large enough, and can do so with any of the nonlinearities introduced in the last section. Multilayer perceptrons are universal function approximators!We said that a two-layer MLP is a universal function approximator given enough hidden units. This brings up the questions of efficiency: how many hidden units / parameters does one need to approximate a given function and whether the number of units depends on the number of hidden layers. The efficiency is important both computationally and statistically: models with fewer parameters can be evaluated faster, and can learn from fewer examples (ref?). It turns out there are functions whose representations are exponentially more expensive in a shallow network compared to a deeper network (see (Nielsen, 2016, Ch 5) for a discussion). Recent winners of image recognition contests use networks with dozens of convolutional layers. The advantage of deeper MLPs is empirically less clear, but you should experiment with the number of units and layers using a development set when starting a new problem.Please see (Nielsen, 2016, Ch 4) for an intuitive explanation of the universality result and (Bengio et al. 2016, Ch 6.4) for a more in depth discussion and references.\",\n          \"Knet is an open-source project and we are always open to new contributions: bug fixes, new machine learning models and operators, inspiring examples, benchmarking results are all welcome. If you'd like to contribute to the code base, please sign up at the knet-dev mailing list and follow these tips:- Please get an account atgithub.com.- ForktheKnet   repository.- Point Julia to your fork withusing Pkg; pkg\\\"dev git@github.com:your-username/Knet.jl.git\\\".- Make sure yourfork is   up-to-date.- Retrieve the latest version of the master branch usinggit pullin the Knet directory.- Implement your contribution.  This typically involves:Creating a git branch.Writing your code.Adding documentation under doc/src and a summary in NEWS.md.Adding unit tests in the test directory and usingPkg.test(\\\"Knet\\\").- Creating a git branch.- Writing your code.- Adding documentation under doc/src and a summary in NEWS.md.- Adding unit tests in the test directory and usingPkg.test(\\\"Knet\\\").- Please submit your contribution using apull   request.\",\n          \"Reference: Knet.Ops20.elu \\u2014 FunctionCode Block: \\n [ elu(x) ]Return (x > 0 ? x : exp(x)-1).Reference: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) (https://arxiv.org/abs/1511.07289).Reference: Knet.Ops20.relu \\u2014 FunctionCode Block: \\n [ relu(x) ]Return max(0,x).References: - Nair and Hinton, 2010. Rectified Linear Units Improve Restricted Boltzmann Machines. ICML.- Glorot, Bordes and Bengio, 2011. Deep Sparse Rectifier Neural Networks. AISTATS.Reference: Knet.Ops20.selu \\u2014 FunctionCode Block: \\n [ selu(x) ]Return \\u03bb01 * (x > 0 ? x : \\u03b101 * (exp(x)-1)) where \\u03bb01=1.0507009873554805 and \\u03b101=1.6732632423543778.Reference: Self-Normalizing Neural Networks (https://arxiv.org/abs/1706.02515).Reference: Knet.Ops20.sigm \\u2014 FunctionCode Block: \\n [ sigm(x) ]Return 1/(1+exp(-x)).Reference: Numerically stable sigm implementation from http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-99eec705-9c64-418f-b468-95f00b66c370\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>p_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AutoGrad</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Reference: Knet.AutoGrad — ModuleUsage:Code Bl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KnetArray</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Reference: Knet.KnetArrays.KnetArray — TypeCod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>File I/O</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Missing docstring forKnet.save. Check Document...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter initialization</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Reference: Knet.Train20.param — FunctionCode B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Activation functions</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Reference: Knet.Ops20.elu — FunctionCode Block...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99eec705-9c64-418f-b468-95f00b66c370')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-99eec705-9c64-418f-b468-95f00b66c370 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-99eec705-9c64-418f-b468-95f00b66c370');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-713ae253-590d-41e0-a660-db4bd9bfe24a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-713ae253-590d-41e0-a660-db4bd9bfe24a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-713ae253-590d-41e0-a660-db4bd9bfe24a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                      title  \\\n",
              "0                  AutoGrad   \n",
              "1                 KnetArray   \n",
              "2                  File I/O   \n",
              "3  Parameter initialization   \n",
              "4      Activation functions   \n",
              "\n",
              "                                                 url  \\\n",
              "0  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "1  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "2  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "3  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "4  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "\n",
              "                                           p_content  \n",
              "0  Reference: Knet.AutoGrad — ModuleUsage:Code Bl...  \n",
              "1  Reference: Knet.KnetArrays.KnetArray — TypeCod...  \n",
              "2  Missing docstring forKnet.save. Check Document...  \n",
              "3  Reference: Knet.Train20.param — FunctionCode B...  \n",
              "4  Reference: Knet.Ops20.elu — FunctionCode Block...  "
            ]
          },
          "execution_count": 242,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame.from_records(texts_all)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erCh6KWb6kUV",
        "outputId": "99b51e11-73d6-4322-ceae-40f9f14c5437"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "execution_count": 244,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "368O7WdT7ioG"
      },
      "outputs": [],
      "source": [
        "df.to_csv('knet_v1_indFalse.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzowEvkQ6okE"
      },
      "outputs": [],
      "source": [
        "for index, row in df.iterrows():\n",
        "    print(\"TITLE: \", row['title'])\n",
        "    print(\"URL: \",row['url'])\n",
        "    print(row[\"p_content\"])\n",
        "    print(\"-\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "id": "GPwUnPHr7WPo",
        "outputId": "e440b019-3064-4f3e-b9c3-7146687bb70c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Reference: Knet.Ops20.accuracy — FunctionCode Block: \\n [ accuracy(scores, labels; dims=1, average=true) ]Given an unnormalized scores matrix and an Integer array of correct labels, return the ratio of instances where the correct label has the maximum score. dims=1 means instances are in columns, dims=2 means instances are in rows. Use average=false to return the pair (ncorrect,count) instead of the ratio (ncorrect/count). The valid labels should be integers in the range 1:numclasses, if labels[i] == 0, instance i is skipped.Code Block: \\n [ accuracy(model; data, dims=1, average=true, o...) ]Compute the number of correct predictions of a model over a dataset:Code Block: \\n [ accuracy(model(inputs; kwargs...), labels; dims) for (inputs,labels) in data ]and return (ncorrect/count) if average=true or (ncorrect,count) if average=false where count is the number instances not skipped (instances with label==0 are skipped) and ncorrect is the number of them correctly labeled by the model.The model should be a function returning scores given inputs, and data should be an iterable of (inputs,labels) pairs. The valid labels should be integers in the range 1:numclasses, if labels[i] == 0, instance i is skipped.Reference: Knet.Ops20.bce — FunctionCode Block: \\n [ bce(scores, labels; average=true) ]Computes binary cross entropy loss given predicted unnormalized scores and answer labels for a binary prediction task. Label values should be in {0,1}. Scores are unrestricted and will be converted to probabilities usingCode Block: \\n [ probs = 1 ./ (1 .+ exp.(-scores)) ]The loss calculated isCode Block: \\n [ -(labels .* log.(probs) .+ (1 .- labels) .* log.(1 .- probs)) ]The return value is (total/count) if average=true and (total,count) if average=false where count is the number of instances and total is their total loss.See also logistic which computes the same loss with {-1,1} labels.Reference: https://towardsdatascience.com/nothing-but-numpy-understanding-creating-binary-classification-neural-networks-with-e746423c8d5cReference: Knet.Ops20.logistic — FunctionCode Block: \\n [ logistic(scores, labels; average=true) ]Computes logistic loss given predicted unnormalized scores and answer labels for a binary prediction task.Code Block: \\n [ log.(1 .+ exp.(-labels .* scores)) ]Label values should be {-1,1}. Scores are unrestricted.  The return value is (total/count) if average=true and (total,count) if average=false where count is the number of instances and total is their total loss.See also bce which computes the same loss with {0,1} labels.Reference: https://towardsdatascience.com/nothing-but-numpy-understanding-creating-binary-classification-neural-networks-with-e746423c8d5cReference: Knet.Ops20.logp — FunctionCode Block: \\n [ softmax(x; dims=:)\\nlogsoftmax(x; dims=:) ]Treat entries in x as as unnormalized log probabilities and return normalized (log) probabilities, i.e. Code Block: \\n [ softmax(x; dims) = exp.(x) ./ sum(exp.(x); dims=dims)\\nlogsoftmax(x; dims) = x .- log.(sum(exp.(x); dims=dims)) ]For numerical stability x = x .- maximum(x,dims=dims) is performed before exponentiation.dims is an optional argument, if not specified the normalization is over the whole x, otherwise the normalization is performed over the given dimensions.  In particular, if x is a matrix, dims=1 normalizes columns of x and dims=2 normalizes rows of x.Reference: Knet.Ops20.logsoftmax — FunctionCode Block: \\n [ softmax(x; dims=:)\\nlogsoftmax(x; dims=:) ]Treat entries in x as as unnormalized log probabilities and return normalized (log) probabilities, i.e. Code Block: \\n [ softmax(x; dims) = exp.(x) ./ sum(exp.(x); dims=dims)\\nlogsoftmax(x; dims) = x .- log.(sum(exp.(x); dims=dims)) ]For numerical stability x = x .- maximum(x,dims=dims) is performed before exponentiation.dims is an optional argument, if not specified the normalization is over the whole x, otherwise the normalization is performed over the given dimensions.  In particular, if x is a matrix, dims=1 normalizes columns of x and dims=2 normalizes rows of x.Reference: Knet.Ops20.logsumexp — FunctionCode Block: \\n [ logsumexp(x;dims=:) ]Compute log(sum(exp(x);dims)) in a numerically stable manner.dims is an optional argument, if not specified the summation is over the whole x, otherwise the summation is performed over the given dimensions.  In particular if x is a matrix, dims=1 sums columns of x and dims=2 sums rows of x.Reference: Knet.Ops20.nll — FunctionCode Block: \\n [ nll(scores, labels; dims=1, average=true) ]Return the negative log likelihood for a single batch of data given an unnormalized scores matrix and an Integer array of correct labels. The scores matrix should have size (classes,instances) if dims=1 or (instances,classes) if dims=2. labels[i] should be in 1:classes to indicate the correct class for instance i, or 0 to skip instance i.The return value is (total/count) if average=true and (total,count) if average=false where count is the number of instances not skipped (i.e. label != 0) and total is their total negative log likelihood.ExampleLet's assume that there are three classes (cat, dog, ostrich) and just 2 instances with the unnormalized score scores[:,1] and scores[:,2] respectively. The first instance is actually a cat and the second instance a dog:Code Block: \\n [ scores = [12.2    0.3;\\n           2.0   21.5;\\n           0.0  -21.0]\\nlabels = [1, 2]\\nnll(scores,labels)\\n# returns 2.1657e-5 ]The probabilites are derived from the scores and the negative log-probabilities corresponding to the labels are averaged:Code Block: \\n [ probabilites = exp.(scores) ./ sum(exp.(scores),dims=1)\\n-(log(probabilites[labels[1],1]) + log(probabilites[labels[2],2]))/2\\n# returns 2.1657e-5 ]Code Block: \\n [ nll(model; data, dims=1, average=true, o...) ]Compute the negative log likelihood for a model over a dataset:Code Block: \\n [ nll(model(inputs; kwargs...), labels; dims) for (inputs,labels) in data ]and return (total/count) if average=true or (total,count) if average=false where count is the number of instances not skipped (instances with label==0 are skipped) and total is their total negative log likelihood.The model should be a function returning scores given inputs, and data should be an iterable of (inputs,labels) pairs. The valid labels should be integers in the range 1:numclasses, if labels[i] == 0, instance i is skipped.Reference: Knet.Ops20.softmax — FunctionCode Block: \\n [ softmax(x; dims=:)\\nlogsoftmax(x; dims=:) ]Treat entries in x as as unnormalized log probabilities and return normalized (log) probabilities, i.e. Code Block: \\n [ softmax(x; dims) = exp.(x) ./ sum(exp.(x); dims=dims)\\nlogsoftmax(x; dims) = x .- log.(sum(exp.(x); dims=dims)) ]For numerical stability x = x .- maximum(x,dims=dims) is performed before exponentiation.dims is an optional argument, if not specified the normalization is over the whole x, otherwise the normalization is performed over the given dimensions.  In particular, if x is a matrix, dims=1 normalizes columns of x and dims=2 normalizes rows of x.Reference: Knet.Ops20.zeroone — Functionzeroone loss is equal to 1 - accuracy\""
            ]
          },
          "execution_count": 250,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df.loc[5][\"p_content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ya3SU3eXpplv",
        "outputId": "64d99966-df1c-4795-c0d1-d9342429cdb7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df2\",\n  \"rows\": 68,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 59,\n        \"samples\": [\n          \"AutoGrad\",\n          \"Loss functions\",\n          \"Notes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 68,\n        \"samples\": [\n          \"https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Representational-power\",\n          \"https://denizyuret.github.io/Knet.jl/stable/install/#Tips-for-developers\",\n          \"https://denizyuret.github.io/Knet.jl/stable/reference/#Activation-functions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p_content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 68,\n        \"samples\": [\n          \"You might be wondering whether relu had any special properties or would any of the other nonlinearities be sufficient. Another question is whether there are functions multilayer perceptrons cannot represent and if so whether adding more layers or different types of functions would increase their representational power. The short answer is that a two layer model can approximate any function if the hidden layer is large enough, and can do so with any of the nonlinearities introduced in the last section. Multilayer perceptrons are universal function approximators!We said that a two-layer MLP is a universal function approximator given enough hidden units. This brings up the questions of efficiency: how many hidden units / parameters does one need to approximate a given function and whether the number of units depends on the number of hidden layers. The efficiency is important both computationally and statistically: models with fewer parameters can be evaluated faster, and can learn from fewer examples (ref?). It turns out there are functions whose representations are exponentially more expensive in a shallow network compared to a deeper network (see (Nielsen, 2016, Ch 5) for a discussion). Recent winners of image recognition contests use networks with dozens of convolutional layers. The advantage of deeper MLPs is empirically less clear, but you should experiment with the number of units and layers using a development set when starting a new problem.Please see (Nielsen, 2016, Ch 4) for an intuitive explanation of the universality result and (Bengio et al. 2016, Ch 6.4) for a more in depth discussion and references.\",\n          \"Knet is an open-source project and we are always open to new contributions: bug fixes, new machine learning models and operators, inspiring examples, benchmarking results are all welcome. If you'd like to contribute to the code base, please sign up at the knet-dev mailing list and follow these tips:- Please get an account atgithub.com.- ForktheKnet   repository.- Point Julia to your fork withusing Pkg; pkg\\\"dev git@github.com:your-username/Knet.jl.git\\\".- Make sure yourfork is   up-to-date.- Retrieve the latest version of the master branch usinggit pullin the Knet directory.- Implement your contribution.  This typically involves:Creating a git branch.Writing your code.Adding documentation under doc/src and a summary in NEWS.md.Adding unit tests in the test directory and usingPkg.test(\\\"Knet\\\").- Creating a git branch.- Writing your code.- Adding documentation under doc/src and a summary in NEWS.md.- Adding unit tests in the test directory and usingPkg.test(\\\"Knet\\\").- Please submit your contribution using apull   request.\",\n          \"Reference: Knet.Ops20.elu \\u2014 FunctionCode Block: \\n [ elu(x) ]Return (x > 0 ? x : exp(x)-1).Reference: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) (https://arxiv.org/abs/1511.07289).Reference: Knet.Ops20.relu \\u2014 FunctionCode Block: \\n [ relu(x) ]Return max(0,x).References: - Nair and Hinton, 2010. Rectified Linear Units Improve Restricted Boltzmann Machines. ICML.- Glorot, Bordes and Bengio, 2011. Deep Sparse Rectifier Neural Networks. AISTATS.Reference: Knet.Ops20.selu \\u2014 FunctionCode Block: \\n [ selu(x) ]Return \\u03bb01 * (x > 0 ? x : \\u03b101 * (exp(x)-1)) where \\u03bb01=1.0507009873554805 and \\u03b101=1.6732632423543778.Reference: Self-Normalizing Neural Networks (https://arxiv.org/abs/1706.02515).Reference: Knet.Ops20.sigm \\u2014 FunctionCode Block: \\n [ sigm(x) ]Return 1/(1+exp(-x)).Reference: Numerically stable sigm implementation from http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df2"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-dc12bc8e-a73f-483c-bf39-0f14865f4b15\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>p_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AutoGrad</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Reference: Knet.AutoGrad — ModuleUsage:Code Bl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KnetArray</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Reference: Knet.KnetArrays.KnetArray — TypeCod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>File I/O</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Missing docstring forKnet.save. Check Document...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter initialization</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Reference: Knet.Train20.param — FunctionCode B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Activation functions</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Reference: Knet.Ops20.elu — FunctionCode Block...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc12bc8e-a73f-483c-bf39-0f14865f4b15')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dc12bc8e-a73f-483c-bf39-0f14865f4b15 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dc12bc8e-a73f-483c-bf39-0f14865f4b15');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e551f4b5-adec-4526-b8aa-f3b7d9c96947\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e551f4b5-adec-4526-b8aa-f3b7d9c96947')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e551f4b5-adec-4526-b8aa-f3b7d9c96947 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                      title  \\\n",
              "0                  AutoGrad   \n",
              "1                 KnetArray   \n",
              "2                  File I/O   \n",
              "3  Parameter initialization   \n",
              "4      Activation functions   \n",
              "\n",
              "                                                 url  \\\n",
              "0  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "1  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "2  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "3  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "4  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "\n",
              "                                           p_content  \n",
              "0  Reference: Knet.AutoGrad — ModuleUsage:Code Bl...  \n",
              "1  Reference: Knet.KnetArrays.KnetArray — TypeCod...  \n",
              "2  Missing docstring forKnet.save. Check Document...  \n",
              "3  Reference: Knet.Train20.param — FunctionCode B...  \n",
              "4  Reference: Knet.Ops20.elu — FunctionCode Block...  "
            ]
          },
          "execution_count": 252,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2 = pd.read_csv(\"knet_v1_indFalse.csv\")\n",
        "\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7WijSTH371_q",
        "outputId": "0092ee54-5f89-44ea-b522-7c8cd4fdc5d6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 68,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 59,\n        \"samples\": [\n          \"AutoGrad\",\n          \"Loss functions\",\n          \"Notes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 68,\n        \"samples\": [\n          \"https://denizyuret.github.io/Knet.jl/stable/mlp/#Multilayer-Perceptrons#Representational-power\",\n          \"https://denizyuret.github.io/Knet.jl/stable/install/#Tips-for-developers\",\n          \"https://denizyuret.github.io/Knet.jl/stable/reference/#Activation-functions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p_content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 68,\n        \"samples\": [\n          \"You might be wondering whether relu had any special properties or would any of the other nonlinearities be sufficient. Another question is whether there are functions multilayer perceptrons cannot represent and if so whether adding more layers or different types of functions would increase their representational power. The short answer is that a two layer model can approximate any function if the hidden layer is large enough, and can do so with any of the nonlinearities introduced in the last section. Multilayer perceptrons are universal function approximators!We said that a two-layer MLP is a universal function approximator given enough hidden units. This brings up the questions of efficiency: how many hidden units / parameters does one need to approximate a given function and whether the number of units depends on the number of hidden layers. The efficiency is important both computationally and statistically: models with fewer parameters can be evaluated faster, and can learn from fewer examples (ref?). It turns out there are functions whose representations are exponentially more expensive in a shallow network compared to a deeper network (see (Nielsen, 2016, Ch 5) for a discussion). Recent winners of image recognition contests use networks with dozens of convolutional layers. The advantage of deeper MLPs is empirically less clear, but you should experiment with the number of units and layers using a development set when starting a new problem.Please see (Nielsen, 2016, Ch 4) for an intuitive explanation of the universality result and (Bengio et al. 2016, Ch 6.4) for a more in depth discussion and references.\",\n          \"Knet is an open-source project and we are always open to new contributions: bug fixes, new machine learning models and operators, inspiring examples, benchmarking results are all welcome. If you'd like to contribute to the code base, please sign up at the knet-dev mailing list and follow these tips:- Please get an account atgithub.com.- ForktheKnet   repository.- Point Julia to your fork withusing Pkg; pkg\\\"dev git@github.com:your-username/Knet.jl.git\\\".- Make sure yourfork is   up-to-date.- Retrieve the latest version of the master branch usinggit pullin the Knet directory.- Implement your contribution.  This typically involves:Creating a git branch.Writing your code.Adding documentation under doc/src and a summary in NEWS.md.Adding unit tests in the test directory and usingPkg.test(\\\"Knet\\\").- Creating a git branch.- Writing your code.- Adding documentation under doc/src and a summary in NEWS.md.- Adding unit tests in the test directory and usingPkg.test(\\\"Knet\\\").- Please submit your contribution using apull   request.\",\n          \"Reference: Knet.Ops20.elu \\u2014 FunctionCode Block: \\n [ elu(x) ]Return (x > 0 ? x : exp(x)-1).Reference: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) (https://arxiv.org/abs/1511.07289).Reference: Knet.Ops20.relu \\u2014 FunctionCode Block: \\n [ relu(x) ]Return max(0,x).References: - Nair and Hinton, 2010. Rectified Linear Units Improve Restricted Boltzmann Machines. ICML.- Glorot, Bordes and Bengio, 2011. Deep Sparse Rectifier Neural Networks. AISTATS.Reference: Knet.Ops20.selu \\u2014 FunctionCode Block: \\n [ selu(x) ]Return \\u03bb01 * (x > 0 ? x : \\u03b101 * (exp(x)-1)) where \\u03bb01=1.0507009873554805 and \\u03b101=1.6732632423543778.Reference: Self-Normalizing Neural Networks (https://arxiv.org/abs/1706.02515).Reference: Knet.Ops20.sigm \\u2014 FunctionCode Block: \\n [ sigm(x) ]Return 1/(1+exp(-x)).Reference: Numerically stable sigm implementation from http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e8cd0447-b352-4531-aff6-216fadc38cec\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>p_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AutoGrad</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Reference: Knet.AutoGrad — ModuleUsage:Code Bl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KnetArray</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Reference: Knet.KnetArrays.KnetArray — TypeCod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>File I/O</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Missing docstring forKnet.save. Check Document...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter initialization</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Reference: Knet.Train20.param — FunctionCode B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Activation functions</td>\n",
              "      <td>https://denizyuret.github.io/Knet.jl/stable/re...</td>\n",
              "      <td>Reference: Knet.Ops20.elu — FunctionCode Block...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8cd0447-b352-4531-aff6-216fadc38cec')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e8cd0447-b352-4531-aff6-216fadc38cec button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e8cd0447-b352-4531-aff6-216fadc38cec');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-15624840-c639-4188-93dd-dc6791b7b278\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-15624840-c639-4188-93dd-dc6791b7b278')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-15624840-c639-4188-93dd-dc6791b7b278 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                      title  \\\n",
              "0                  AutoGrad   \n",
              "1                 KnetArray   \n",
              "2                  File I/O   \n",
              "3  Parameter initialization   \n",
              "4      Activation functions   \n",
              "\n",
              "                                                 url  \\\n",
              "0  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "1  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "2  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "3  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "4  https://denizyuret.github.io/Knet.jl/stable/re...   \n",
              "\n",
              "                                           p_content  \n",
              "0  Reference: Knet.AutoGrad — ModuleUsage:Code Bl...  \n",
              "1  Reference: Knet.KnetArrays.KnetArray — TypeCod...  \n",
              "2  Missing docstring forKnet.save. Check Document...  \n",
              "3  Reference: Knet.Train20.param — FunctionCode B...  \n",
              "4  Reference: Knet.Ops20.elu — FunctionCode Block...  "
            ]
          },
          "execution_count": 253,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1lpHMAJ75Ah",
        "outputId": "8ed5962f-827c-4d39-affa-87b403538761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "68\n",
            "68\n"
          ]
        }
      ],
      "source": [
        "print(len(df))\n",
        "print(len(df2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w-0q8sV8FVa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
